\chapter{Conclusion}
In which we review the work that has been covered and consider future directions of 
investigation.
\section{Summary of Results}
In Chapter 2 we reversed the normal development of stochastic matrices, which usually starts 
with characterizing matrices as having non-negative entries with fixed row sums in a
standard orthonormal basis $\hat{e}_i$. The line of typical development then notices that
the vector $\vec{\mathbbm{1}} = \sum_{i=1}^n \hat{e}_i$ is an eigenvector. Instead we began
with characterizing all invertible matrices $A$ such that $A \hat{\mathbbm{1}} = \hat{\mathbbm{1}}$
for a fixed unit vector $\hat{\mathbbm{1}}$. We showed that there is always a basis $\hat{e}_i$ 
such that $\vec{\mathbbm{1}} = \sqrt{n} \hat{\mathbbm{1}}$ can be interpreted as the row sum
vector, and found that this allowed us to characterize both the Lie group in which the 
matrices $A \hat{\mathbbm{1}} = \hat{\mathbbm{1}}$ reside and the Lie algebra of tangents to
the Lie group. We denoted $St(\hat{\mathbbm{1}})$ the stochastic Lie group with respect to $\hat{\mathbbm{1}}$ 
and $\mathfrak{st}(\hat{\mathbbm{1}})$ the stochastic Lie algebra with respect to $\hat{\mathbbm{1}}$. 
We further characterized the doubly stochastic Lie group and algebra with respect to $\hat{\mathbbm{1}}$, 
denoted by $St(\hat{\mathbbm{1}}, \hat{\mathbbm{1}})$ and $\mathfrak{st}(\hat{\mathbbm{1}}, \hat{\mathbbm{1}})$ 
respectively, of invertible matrices of fixed row and column sums with respect to $\hat{\mathbbm{1}}$. 
This was accomplished by generalizing the stochastic Lie group and algebra to the dual 
stochastic Lie group and algebra, denoted by $St^\dagger(\hat{\mathbbm{1}})$ and $\mathfrak{st}^\dagger(\hat{\mathbbm{1}})$ 
respectively, of invertible matrices such that $A^\dagger \hat{\mathbbm{1}} = \hat{\mathbbm{1}}$.

In Chapter 3 we used analytic and closure properties of the stochastic Lie algebras to argue 
that the algorithms developed for calculating the gradient and Hessian of the matrix
exponential will result in matrices that belong to the stochastic Lie algebra. An initial
computation of the coefficients for the Pad\'{e} approximations was presented and a sketch
of algorithms to calculate the gradient and the Hessian of the matrix exponential were 
outlined.

In Chapter 4 we combined the results of Chapters 2 and 3, which were illustrated through the 
example of aging processes. The algorithms to calculate the log-likelihood, its gradient, 
and Hessian were developed. Finally Newton-Raphson maximization of the log-likelihood was 
briefly introduced.
\section{Discussion}
Any attempt to estimate the generator of a Markov process from observations is in essence an 
exercise of calculating the logarithm of a matrix. As in the scalar complex case, the matrix 
logarithm is not unique and has many branches. To demonstrate this, in Chapter 2, a 
calculation of one of the branches of the logarithm of permutations was developed in the
context of stochastic Lie algebra. However, much as there is a unique real logarithm of a 
positive real number, working within the normal subgroup and the real sub-algebra of the 
stochastic Lie group does provide certain guarantees of algebraic and analytic closure. 
Furthermore no time homogeneous Markov process can ever escape from the normal subgroup 
because the identity element is a member of the normal subgroup of matrices with positive 
determinant.

The fitting of generators of Markov processes to observations is bedeviled by a second
source of degeneracy, beyond the branches of the matrix logarithm. As reviewed in the 
beginning of Chapter 4, unless the experiment can be designed to observe every possible 
transition between states there will always be indeterminacy in the model. In particular, 
when an incomplete number of stopping statistics are used, one can construct many different Markov models with 
the exact distributions of the stopping statistics. For example, one could always add 
ghost states and transitions between them that are never observed. This is when the 
intuition of scientist is of critical importance, wherein they apply Occam's razor to 
select the most parsimonious explanation for the observations. Even an application of the 
likelihood ratio test or the Akiake information criteria \cite{akaike_new_1974} will not be 
of assistance as the extra parameters wash out when the distribution of specific stopped
statistics are formulated. A full resolution of this indeterminacy awaits a comprehensive
classification of the generators of continuous time homogeneous Markov processes on discrete
state spaces by the distributions of their stopping statistics. This undertaking should be
well within the reach of contemporary mathematical technology.

This should be of no discouragement as there is much fertile ground that still needs
covering. The Pad\'{e} approximations presented are only the first pass derivation, and
much work remains to be done in numerically optimizing the choice of approximation orders
for various implementations, and architectures. Furthermore, the calculation of the adjoint
through the Kronecker products will present a serious bottleneck to scaling the current
algorithms up to terabytes and pentabytes of data. As well, the branch of the bilinear 
non-commutative perturbation that is calculated using a recursive Taylor series 
approximation needs deep investigation to determine if there are additional factorizations 
that can numerically stabilize the loop, and speed up the computation. On top of that, the 
algorithms presented are merely sketches for the actual implementation, which when done will 
need careful consideration of memory management, assignment, logic branching, and in-lining 
of subroutines. Finally the propositions in Chapter 2 deserve to be given proper treatment 
and have the proofs of the claims completed. Finally, the algorithms presented herein 
deserve thorough generalizations to handle incomplete data using the standard expectation 
maximization techniques.