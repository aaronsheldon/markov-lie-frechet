\chapter{The Lie Algebra of the Generators of Continuous Time Markov Processes on Finite State Spaces}
\section{Stochastic Matrices}
The classical Lie algebras of physics, like the infinitesimal symmetries
of the special unitary algebra $\mathfrak{su}(n)$, are defined with respect to
invariants of a Banach algebra, such as the matrix invariants of the 
determinant, trace, or norm. In contrast stochastic matrices are always 
characterized with respect to a specific unit vector, which we will denote 
$\hat{\mathbbm{1}}$. In the next two sections we provide an explicit 
construction and characterization of the Lie algebra of stochastic matrices, 
building on the original the work of ??.

The common approach to stochastic matrices begins with the restriction that the 
matrices have non-negative entries with respect to the standard orthonormal 
basis for the vector space on which it acts; namely $\left\langle\hat{e}_i,A \hat{e}_j\right\rangle \ge 0$
for all $i,j$. In addition to allowing for singular matrices, this poses an 
immediate obstacle to the necessary closure with respect to matrix inversion 
required for matrix groups. As the inverse of a stochastic matrix need not have 
non-negative entries with respect to the standard orthonormal basis. 

For the moment we will set aside the restriction that the entries be 
non-negative, and instead begin with a generalization of the concept of fixed 
row sums. We will show that this generalization is preserved by matrix 
inversion, and then develop an orthonormal basis from which specific matrices 
with non-negative entries, with respect to the basis, can be constructed. In 
essence tackling the problem from the reverse direction, starting with the more 
general idea of fixed row sums, and then specifying to matrices with 
non-negative entries with respect to a constructed orthonormal basis.

\begin{definition}
	A matrix $A$ is stochastic with respect to the unit vector $\hat{\mathbbm{1}}$ 
	if $A \hat{\mathbbm{1}} = \hat{\mathbbm{1}}$
\end{definition}

Note that this definition does not stipulate any conditions on non-singularity,
and thus includes all the matrices in the convex polytope of stochastic 
matrices. For an $n$ dimensional vector space the vector $\vec{\mathbbm{1}} = \sqrt{n} \hat{\mathbbm{1}}$ 
acts as the row sum operator on matrices stochastic with respect to $\hat{\mathbbm{1}}$. 
We will make this claim more precise after we dispense with a few more 
foundational definitions.

\begin{definition}
	Let $St(\hat{\mathbbm{1}})$ denote the stochastic Lie group of invertible 
	matrices stochastic with respect to $\hat{\mathbbm{1}}$
\end{definition}

It is tempting to view the name stochastic Lie group as a bait and switch, or 
at least an abuse of the terminology, given we have removed the usual convex
polytope of stochastic matrices and replaced it with a group of invertible 
matrices with a common eigenvector $\hat{\mathbbm{1}}$. Previous authors ?? have 
denoted the convex polytope of stochastic matrices as the stochastic semi-group, 
and the group of invertible matrices as the pseudo-stochastic Lie group. One 
could even consider incorporating Markov into the name, in reference to the 
fact that the transition matrices of a continuous Markov process on a finite 
state space are by definition invertible and have common eigenvector $\hat{\mathbbm{1}}$. 
However the suffix of Lie group in the name connotes both sufficient additional 
restrictions to make the name distinct, and still allows for an indication of a 
relationship with the original concept. Of course, this definition immediately 
necessitates proof of the claim embedded in the definition.

\begin{lemma}
	$St(\hat{\mathbbm{1}})$ is a Lie group
\end{lemma}

\begin{proof}
	We proceed by working mechanistically through the Lie group axioms.
	\begin{enumerate}
		\item The identity element $I$ is in $St(\hat{\mathbbm{1}})$. Clearly $I$ is
		invertible and $I \hat{\mathbbm{1}} = \hat{\mathbbm{1}}$.
		\item If $A,B \in St(\hat{\mathbbm{1}})$ then $AB \in St(\hat{\mathbbm{1}})$. 
		This follows from the computation $AB \hat{\mathbbm{1}} = A \hat{\mathbbm{1}} = \hat{\mathbbm{1}}$.
		\item If $A \in St(\hat{\mathbbm{1}})$ then $A^{-1} \in St(\hat{\mathbbm{1}})$.
		Recognize that $A \hat{\mathbbm{1}} = \hat{\mathbbm{1}}$ implies $\hat{\mathbbm{1}} = A^{-1} A \hat{\mathbbm{1}} = A^{-1} \hat{\mathbbm{1}}$.
		\item Associativity follows from $St(\hat{\mathbbm{1}})$ being a subgroup of $GL\left(n\right)$.
		\item Finally, that the matrix product $A^{-1}B$ is smooth for all $A,B \in St(\hat{\mathbbm{1}})$
		likewise follows from $St(\hat{\mathbbm{1}}) < GL\left(n\right)$.
	\end{enumerate}
\end{proof}

That $St(\hat{\mathbbm{1}})$ is a proper matrix Lie group implies that it must be
infinitesimal generated by elements of a Lie algebra.

\begin{definition}
	Let $\mathfrak{st}(\hat{\mathbbm{1}})$ denote the stochastic Lie algebra of $St(\hat{\mathbbm{1}})$
\end{definition}

By infinitesimally generated we mean that every element of $St(\hat{\mathbbm{1}})$
is a matrix exponential of at least one element in $\mathfrak{st}(\hat{\mathbbm{1}})$. 
We can fully characterize this algebra as the set of matrices such that their 
row sums are zero with respect to $\hat{\mathbbm{1}}$.

\begin{lemma}
	The algebra $\mathfrak{st}(\hat{\mathbbm{1}})$ is exactly the set of all 
	matrices with $\hat{\mathbbm{1}}$ in their kernel.
\end{lemma}

\begin{proof}
	Working through the forward and backward inclusions we have
	\begin{enumerate}
		\item Suppose $A \hat{\mathbbm{1}} = 0$ then from the definition of the 
		matrix exponential we have:
		\begin{IEEEeqnarray*}{rCl}
			\exp\left(A\right) \hat{\mathbbm{1}}
				& = & \sum_{n=0}^{\infty} \frac{1}{n!} A^n \hat{\mathbbm{1}}\\
				& = & \hat{\mathbbm{1}} + \sum_{n=1}^{\infty} \frac{1}{n!} 0\\
				& = & \hat{\mathbbm{1}}
		\end{IEEEeqnarray*}
		Thus $\exp\left(A\right) \in St(\hat{\mathbbm{1}})$ implying that $A \in \mathfrak{st}(\hat{\mathbbm{1}})$
		\item Now begin with the reverse assumption, that $A \in \mathfrak{st}(\hat{\mathbbm{1}})$.
		For all $t \in \mathbb{R}$ we have $\exp\left(tA\right) \hat{\mathbbm{1}} = \hat{\mathbbm{1}}$.
		Differentiation with respect to $t$ and evaluation at $t = 0$ yields
		\begin{IEEEeqnarray*}{rCl}
			0 & = & \left. \frac{d}{dt} \hat{\mathbbm{1}} \right|_{t=0}\\
				& = & \left. \frac{d}{dt} \exp\left(tA\right) \hat{\mathbbm{1}} \right|_{t=0}\\
				& = & \left. \exp\left(tA\right) A \hat{\mathbbm{1}} \right|_{t=0}\\
				& = & A \hat{\mathbbm{1}}
		\end{IEEEeqnarray*}
	\end{enumerate}
\end{proof}

Over an $n$ dimensional vector space, the condition on a matrix $A$ that $A \hat{\mathbbm{1}} = 0$
places $n$ constraints on the $n^2$ dimensions of $A$. This leaves $n^2 - n$ 
free dimensions on $\mathfrak{st}(\hat{\mathbbm{1}})$, when considered as a
vector space. This hints that we can construct a generator of $\mathfrak{st}(\hat{\mathbbm{1}})$
from order pairs of basis elements $\hat{e}_i$ for the vector space of $\hat{\mathbbm{1}}$.
To see how this is done we first construct a useful basis for the vector space
to which $\hat{\mathbbm{1}}$ is a member.

\begin{lemma}
	There exists an orthonormal basis $\hat{e}_i$ such that $\left\langle \hat{e}_i, \hat{\mathbbm{1}} \right\rangle = \frac{1}{\sqrt{n}}$
	for all $i$
\end{lemma}

\begin{proof}
	While a basis with the stipulated properties can be constructed through the
	Gram-Schmidt process, the proof of the existence proceeds by induction.
	\begin{enumerate}
		\item For $n=1$ the desired basis is precisely the trivial set $\left\lbrace \hat{\mathbbm{1}} \right\rbrace$ 
		which satisfies the condition that $\left\langle \hat{\mathbbm{1}}, \hat{\mathbbm{1}} \right\rangle = 1$.
		\item Assume the claim is true for $n$. For $n+1$ pick a unit vector $\hat{e}_{\perp}$
		that is orthogonal to $\hat{\mathbbm{1}}$ and construct the unit vector
		$\hat{e}_{n+1} = \frac{1}{\sqrt{n+1}} \hat{\mathbbm{1}} + \sqrt{\frac{n}{n+1}} \hat{e}_{\perp}$.
		Clearly $\hat{e}_{n+1}$ satisfies the condition $\left\langle \hat{e}_{n+1}, \hat{\mathbbm{1}} \right\rangle = \frac{1}{\sqrt{n+1}}$.
		\item To use the the induction assumption we construct a new row sum unit 
		vector $\hat{\mathbbm{1}}_n = \sqrt{\frac{n+1}{n}}\hat{\mathbbm{1}} - \frac{1}{\sqrt{n}}\hat{e}_{n+1}$ 
		in one dimension lower by projecting onto the subspace orthogonal to $\hat{e}_{n+1}$.
		\item By the induction assumption there exists a basis $\hat{e}_i$ with $i \le n$, 
		such that $\left\langle \hat{e}_i, \hat{\mathbbm{1}}_n \right\rangle = \frac{1}{\sqrt{n}}$.
		\item Because $\hat{e}_i$ with $i \le n$ was constructed in the space 
		orthogonal to $\hat{e}_{n+1}$ if follows that $\left\langle \hat{e}_i, \hat{e}_j \right\rangle = \delta_{ij}$
		for all $i,j \le n+1 $.
		\item Then using the definitions of $\hat{e}_{n+1}$ and $\hat{\mathbbm{1}}_n$
		we can calculate the inner product $\left\langle \hat{e}_i, \hat{\mathbbm{1}}_n \right\rangle$
		for $i \le n$
		\begin{IEEEeqnarray*}{rCl}
			\frac{1}{\sqrt{n}}
				& = & \left\langle \hat{e}_i, \hat{\mathbbm{1}}_n \right\rangle\\
				& = & \sqrt{\frac{n+1}{n}} \left\langle \hat{e}_j, \hat{\mathbbm{1}} \right\rangle - \frac{1}{\sqrt{n}} \left\langle \hat{e}_i, \hat{e}_{n+1} \right\rangle\\
				& = & \sqrt{\frac{n+1}{n}} \left\langle \hat{e}_j, \hat{\mathbbm{1}} \right\rangle
		\end{IEEEeqnarray*}
		Inverting the fraction in the equality yields $\left\langle \hat{e}_i, \hat{\mathbbm{1}} \right\rangle = \frac{1}{\sqrt{n+1}}$
		for all $i \le n+1$.
	\end{enumerate}
\end{proof}

As a direct result of the construction of the basis vectors $\hat{e}_i$ we see 
that $\vec{\mathbbm{1}} = \sum_{i=1}^n \hat{e}_i$. Thus $\vec{\mathbbm{1}}$ can
be interpreted as the row sum vector in basis $\hat{e}_i$.

The constructed basis leads naturally to considering the minimal non-trivial 
matrices $C_{ij} = \hat{e}_i \otimes \left( \hat{e}_j - \hat{e}_i \right)$ as 
holding significance in the structure of $\mathfrak{st}(\hat{\mathbbm{1}})$.
In fact this will be the central result of this chapter: that the algebraic 
closure of the matrices $C_{ij}$ is the stochastic Lie algebra $\mathfrak{st}(\hat{\mathbbm{1}})$. 
To establish this result we need a preliminary result that proves the 
commutators $\left[C_{ij},C_{kl}\right]$ are linear combinations of matrices $C_{ij}$.

\begin{lemma}
	\begin{IEEEeqnarray*}{rCl}
		C_{ij}C_{kl} & = &
		\begin{cases}
			- C_{il} & i=k,\\
			C_{il} - C_{ij} & j=k,\\
			0 & \text{otherwise}.
		\end{cases}
	\end{IEEEeqnarray*}
\end{lemma}

\begin{proof}
	We proceed in two steps; calculating the terms of the products, then 
	simplifying the cases, always assuming $i \neq j$ and $k \neq l$.
	\begin{enumerate}
		\item Term wise computation of the Kronecker products yields
		\begin{IEEEeqnarray*}{rCl}
			C_{ij}C_{kl}
				& = & \hat{e}_i \otimes \left( \hat{e}_j - \hat{e}_i \right) \hat{e}_k \otimes \left( \hat{e}_l - \hat{e}_k \right)\\
				& = & \hat{e}_i \otimes \hat{e}_j \hat{e}_k \otimes \hat{e}_l + \hat{e}_i \otimes \hat{e}_i \hat{e}_k \otimes \hat{e}_k - \hat{e}_i \otimes \hat{e}_j \hat{e}_k \otimes \hat{e}_k - \hat{e}_i \otimes \hat{e}_i \hat{e}_k \otimes \hat{e}_l\\
				& = & \delta_{jk} \hat{e}_i \otimes \hat{e}_l + \delta_{ik} \hat{e}_i \otimes \hat{e}_k - \delta_{jk} \hat{e}_i \otimes \hat{e}_k - \delta_{ik} \hat{e}_i \otimes \hat{e}_l\\
				& = & \left(\delta_{jk} - \delta_{ik} \right) \hat{e}_i \otimes \left( \hat{e}_l - \hat{e}_k \right)
		\end{IEEEeqnarray*}
		\item We work through each case of $\delta_{jk} - \delta_{ik}$, starting 
		with the case $i=k$ 
		\begin{IEEEeqnarray*}{rCl}
			C_{ij}C_{il}
				& = & \left(\delta_{jk} - \delta_{ii} \right) \hat{e}_i \otimes \left( \hat{e}_l - \hat{e}_i \right)\\
				& = & - \hat{e}_i \otimes \left( \hat{e}_l - \hat{e}_i \right)\\
				& = & - C_{il}
		\end{IEEEeqnarray*}
		\item When $j=k$ we have
		\begin{IEEEeqnarray*}{rCl}
			C_{ij}C_{jl}
				& = & \left(\delta_{jj} - \delta_{ij} \right) \hat{e}_i \otimes \left( \hat{e}_l - \hat{e}_j \right)\\
				& = & \hat{e}_i \otimes \left( \hat{e}_l - \hat{e}_j \right)\\
				& = & \hat{e}_i \otimes \left( \hat{e_l} - \hat{e}_i + \hat{e}_i - \hat{e}_j \right)\\
				& = & C_{il} - C_{ij}
		\end{IEEEeqnarray*}
		\item Finally when none of the previous conditions apply
		\begin{IEEEeqnarray*}{rCl}
			C_{ij}C_{kl}
				& = & \left(\delta_{jk} - \delta_{ik} \right) \hat{e}_i \otimes \left( \hat{e}_l - \hat{e}_k \right)\\
				& = & 0 \cdot \hat{e}_i \otimes \left( \hat{e}_l - \hat{e}_k \right)\\
				& = & 0
		\end{IEEEeqnarray*}
	\end{enumerate}
\end{proof}

While this result is sufficient to accomplish the central result, it is worth
carrying through with the computation of the structure constants of the generators.

\begin{corollary}
	\begin{IEEEeqnarray*}{rCl}
		\left[C_{ij},C_{kl}\right] & = &
		\begin{cases}
			C_{ij} - C_{il} & i=k,\\
			C_{kj} - C_{ki} & i=l,\\
			C_{il} - C_{ij} & j=k,\\
			0 & \text{otherwise}.
		\end{cases}
	\end{IEEEeqnarray*}
\end{corollary}

\begin{proof}
	As in the previous lemma we work case wise through the equalities.
	\begin{enumerate}
		\item Starting with $i=k$
		\begin{IEEEeqnarray*}{rCl}
			\left[C_{ij},C_{il}\right]
				& = & C_{ij}C_{il} - C_{il}C_{ij}\\
				& = & C_{ij} - C_{il}
		\end{IEEEeqnarray*}
		\item For $i=l$
		\begin{IEEEeqnarray*}{rCl}
			\left[C_{ij},C_{ki}\right]
				& = & C_{ij}C_{ki} - C_{ki}C_{ij}\\
				& = & C_{kj} - C_{ki}
		\end{IEEEeqnarray*}
		\item For $j=k$
		\begin{IEEEeqnarray*}{rCl}
			\left[C_{ij},C_{jl}\right]
				& = & C_{ij}C_{jl} - C_{jl}C_{ij}\\
				& = & C_{il} - C_{ij}
		\end{IEEEeqnarray*}
		\item When none of the conditions apply
		\begin{IEEEeqnarray*}{rCl}
			\left[C_{ij},C_{kl}\right]
				& = & C_{ij}C_{kl} - C_{kl}C_{ij}\\
				& = & 0
		\end{IEEEeqnarray*}
	\end{enumerate}
\end{proof}

We can now proceed with the central result that motivates this chapter.

\begin{theorem}
	The canonical generators of $\mathfrak{st}(\hat{\mathbbm{1}})$ are $C_{ij}$
\end{theorem}

\begin{proof}
	The previous lemma has established that the products, and thus the 
	commutators, of $C_{ij}$ are linear in $C_{ij}$. We then have to prove that 
	the smallest algebra that contains $C_{ij}$ is $\mathfrak{st}(\hat{\mathbbm{1}})$. 
	As thus, it is sufficient to prove that matrices $C_{ij}$ from a basis for $\mathfrak{st}(\hat{\mathbbm{1}})$.
	This is because a necessary condition for an algebra to contain the matrices
	$C_{ij}$ is that it must contain all sums of the matrices $C_{ij}$. If one
	could sum their way out of the algebra then it would not be an algebra.
	\begin{enumerate}
		\item That $\mathfrak{st}(\hat{\mathbbm{1}})$ is an $n^2-n$ dimensional 
		vector space should be clear from the previous discussion. A full formal
		proof of this claim is found through induction on the dimension $n$.
		\item The matrices $C_{ij}$ are in $\mathfrak{st}(\hat{\mathbbm{1}})$. From
		the definition of the canonical generators
		\begin{IEEEeqnarray*}{rCl}
			C_{ij} \hat{\mathbbm{1}}
				& = & \hat{e}_i \otimes \left( \hat{e}_j - \hat{e}_i \right) \hat{\mathbbm{1}}\\
				& = & \hat{e}_i \left( \left\langle \hat{e}_j, \hat{\mathbbm{1}} \right\rangle - \left\langle \hat{e}_i, \hat{\mathbbm{1}} \right\rangle \right)\\
				& = & \hat{e}_i \left(\frac{1}{\sqrt{n}} - \frac{1}{\sqrt{n}}\right)\\
				& = & 0
		\end{IEEEeqnarray*}
		\item $C_{ij}$ is a set of $n^2-n$ linear independent matrices and so must
		form a basis for all of $\mathfrak{st}(\hat{\mathbbm{1}})$. That there are
		only $n^2-n$ matrices is clear from the fact that $C_{ii} = 0$. While the 
		formal proof of linear independence is again found through induction on the 
		dimension $n$.
	\end{enumerate}
\end{proof}

The previous theorem serves as the definition of a set of canonical generators 
of $\mathfrak{st}(\hat{\mathbbm{1}})$. It is  important to note that neither the 
basis $\hat{e}_i$ nor the canonical generators $C_{ij}$ are unique. They are 
uniquely defined only up to rotations orthogonal to the vector $\hat{\mathbbm{1}}$.

Since $C_{ij} \in \mathfrak{st}(\hat{\mathbbm{1}})$ its matrix exponential must
be in $St(\hat{\mathbbm{1}})$, which can be summarized in the following 
corollary.

\begin{corollary}
	$\exp\left(\alpha C_{ij}\right) = I + \left(1 - e^{-\alpha} \right)C_{ij}$
\end{corollary}

\begin{proof}
	From the definition of the matrix exponential
	\begin{IEEEeqnarray*}{rCl}
		\exp\left(\alpha C_{ij}\right)
			& = & \sum_{n=0}^{\infty} \frac{1}{n!}\alpha^n C_{ij}^n\\
			& = & I + \sum_{n=1}^{\infty} \frac{1}{n!} \left(-1\right)^{n+1} \alpha^n C_{ij}\\
			& = & I + \left(1 - e^{-\alpha} \right)C_{ij}
	\end{IEEEeqnarray*}	
\end{proof}

This last corollary admits a intuitive heuristic interpretation: that each 
canonical generator $C_{ij}$ can be thought of as measuring the infinitesimal 
transition rate, or flow of probability, from the state represented by basis 
element $\hat{e}_i$ to the state represented by the basis element $\hat{e}_j$. 
This can be seen by considering the matrix representation of $\exp\left(\alpha C_{ij}\right)$ 
in the basis spanned by $\hat{e}_i$ and $\hat{e}_j$.

\begin{IEEEeqnarray*}{rCl}
	I + \left(1 - e^{-\alpha} \right)C_{ij}
		& = &
		\begin{pmatrix}
			e^{-\alpha} & 1 - e^{-\alpha}\\
			0 & 1
		\end{pmatrix}
\end{IEEEeqnarray*}

Taking the limit as $\alpha \rightarrow \infty$ shows while the limits in the 
positive directions in the tangent space $\mathfrak{st}(\hat{\mathbbm{1}})$
maybe finite, $St(\hat{\mathbbm{1}})$ is not closed.

\begin{IEEEeqnarray*}{rCl}
	\lim_{\alpha \rightarrow \infty} \exp\left(\alpha C_{ij} \right)
		& = & I + C_{ij}\\
		& = &
		\begin{pmatrix}
			0 & 1\\
			0 & 1
		\end{pmatrix}
\end{IEEEeqnarray*}

The interpretation of $C_ij$ as measuring a real flow is more nuanced, as $e^{-\alpha} > 0$ 
for all $\alpha \in \mathbb{R}$. To realize $e^{-\alpha} > 0$ requires that $\mathbb{I}m\left(\alpha\right) \mod 2 = \pi$. 
The role of imaginary numbers becomes clear when considering the generator $\alpha \left(C_{ij} + C_{ji}\right)$

In general, a matrix Lie group $M$ is it's Lie algebra offset by the identity 
element $M = I + \mathfrak{m}$; which can be seen from the Taylor series
expansion of the matrix exponential. It follows that for a $G = \sum_{ij} g_{ij} C_{ij} \in \mathfrak{st}(\hat{\mathbbm{1}})$
the matrix exponential is $\exp\left(G\right) = I + \sum_{ij} h_{ij} C_{ij}$.

Unfortunately, because polynomials of degree greater than four are generally 
unsolvable, the relationship between the coefficients $g_{ij}$ and $h_{ij}$ is 
highly non-trivial in most circumstances. An exception to this
difficulty can be found in the formulation of the of first exit
times from a fixed initial state. The generator of the first exit from $i$ to any
state $j \ne i$ is given by $G_i = \sum_{i \ne j} \alpha_j C_{ij}$, from which
we have

\begin{IEEEeqnarray*}{rCl}
	\exp\left(G_i\right)
		& = & I + \sum_{n=1}^{\infty} \left(-\sum_{i \ne j} \alpha_j\right)^{n-1} \sum_{i \ne j} \alpha_j C_{ij}\\
		& = & I +\frac{1 - e^{-\sum_{i \ne k} \alpha_k}}{\sum_{i \ne l} \alpha_l} \sum_{i \ne j} \alpha_j C_{ij}
\end{IEEEeqnarray*}

At this point it is worth briefly revisiting the distinction between the 
standard convex polytope of stochastic matrices and the stochastic Lie group, to
develop some physical intuition into the relationship between the two sets of
matrices which have a non-trivial and geometrical interesting intersection.
Consider a general stochastic matrix $A \hat{\mathbbm{1}} = \hat{\mathbbm{1}}$,
not necessarily invertible, on a $n=2$ dimensional vector space, in the standard
orthonormal basis

\begin{IEEEeqnarray*}{rCl}
	A & = & 
		\begin{pmatrix}
			1-a & a\\
			b & 1-b
		\end{pmatrix}
\end{IEEEeqnarray*}

% need a figure here displaying the parameter space

The matrix is invertible when $\det\left(A\right) = 1 - a - b \ne 0$, and has 
non-negative entries in the standard orthonormal basis when $a,b \in \left[0,1\right]$. 
The convex polytope of $2 \times 2$ stochastic matrices in the standard basis is
the convex hull of the vertexes 

\begin{IEEEeqnarray*}{rCl}
	V_0 & = & I + C_{21}\\
	V_1 & = & I + C_{12} + C_{21}\\
	V_2 & = & I\\
	V_3 & = & I + C_{12}
\end{IEEEeqnarray*}

Geometrically fixing a basis $\hat{e}_i$ defines a convex polytope in the form
of a unit hypercube $\left[0,1\right]^{n^2-n}$ in the parameter space isomorphic 
to $\mathfrak{st}(\hat{\mathbbm{1}})$. The vertexes of the convex polytope of 
stochastic matrices are the $n^n$ sums of the limiting elements 

\begin{IEEEeqnarray*}{rCl}
	V_k 
		& = & \sum_{i=1}^n \hat{e}_i \otimes \hat{e}_{j_n^i\left(k\right)}\\
		& = & I + \sum_{i=1}^n C_{i j_n^i\left(k\right)}\\
		& = & \left(1 - n\right) I + \sum_{i=1}^n I + C_{i j_n^i\left(k\right)}\\
		& = & \left(1 - n\right) I + \sum_{i=1}^n \lim_{\alpha \rightarrow \infty} \exp\left(\alpha C_{i j_n^i\left(k\right)}\right)
\end{IEEEeqnarray*}

where $j_n^i\left(k\right) = 1 + \left\lfloor \frac{k}{n^{i-1}} \right\rfloor \mod n$ 
is $1$ plus the $i$ digit of $0 \le k < n^n$ in base $n$. Analytically $St(\hat{\mathbbm{1}})$
is not closed with respect to some limits along the directions in the tangent space $\mathfrak{st}(\hat{\mathbbm{1}})$, 
yet its closure contains the all vertexes for the convex polytope of stochastic 
matrices.

The constructed basis elements $\hat{e}_i$ are a well defined enumeration of the 
states of a continuous Markov process on a finite state space; in that when the 
multipliers of the canonical generators are non-negative the matrix exponential
gives a proper transition matrix for the process. The reverse is also true, up
to a choice of branch of the matrix logarithm. Proofs of either direction of
inclusions can be found in a number of texts, for example ??, and typically
involve studying the resolvent of the matrices of $St(\hat{\mathbbm{1}})$, and
the derivatives of the exponentials of the matrices of $\mathfrak{st}(\hat{\mathbbm{1}})$.

% The resolvent as either the Laplace or Fourier transform of an element of st(1)
% extends St(1) to Gl(1) through contours that intersect with st(1) at the element
% being transformed L[exp(ut)exp(tG)] = 1/(u+G) = u exp(-G'). Gl(1) is the Lie 
% group of of invertible matrices with 1 as the eigenvector. gl(1) = st(1) + 1 

We have developed an interpretation of the Eigen equation $A \hat{\mathbbm{1}} = \hat{\mathbbm{1}}$
as a conservation of the row sums of $A$; likewise the Eigen equation $A^T \hat{\mathbbm{1}} = \hat{\mathbbm{1}}$
can be interpreted as the conservation of the column sums of $A$. The dual 
definitions for the Lie group and algebra follow natural.

\begin{definition}
	Let $St^T(\hat{\mathbbm{1}})$ denote the dual stochastic Lie group of 
	invertible matrices whose transpose is stochastic with respect to $\hat{\mathbbm{1}}$.
\end{definition}

\begin{definition}
	Let $\mathfrak{st}^T(\hat{\mathbbm{1}})$ denote the dual stochastic Lie 
	algebra of $St^T(\hat{\mathbbm{1}})$.
\end{definition}

Thus if $C_{ij}$ are generators of $\mathfrak{st}(\hat{\mathbbm{1}})$ then $C_{ij}^T = \left(\hat{e}_j - \hat{e}_i \right) \otimes \hat{e}_i$
are generators of $\mathfrak{st}^T(\hat{\mathbbm{1}})$. That $St(\hat{\mathbbm{1}}) \cap St^T(\hat{\mathbbm{1}})$ 
is a Lie group and $\mathfrak{st}(\hat{\mathbbm{1}}) \cap \mathfrak{st}^T(\hat{\mathbbm{1}})$ 
is a Lie algebra will be used extensively in the next section.

\section{Doubly Stochastic Matrices}
Doubly stochastic matrices require row and column conservation of the vector $\hat{\mathbbm{1}}$, 
in the sense that both $A \hat{\mathbbm{1}} = \hat{\mathbbm{1}}$ and $A^T \hat{\mathbbm{1}} = \hat{\mathbbm{1}}$ 
must hold. The group of invertible doubly stochastic matrices is then a subgroup
of the group of stochastic matrices. The two constraints of row and column 
conservation leaves only $\left(n - 1\right)^2$ linear degrees of freedom. This 
is an important clue in the construction of a canonical representation. In fact 
the representation can be found by choosing one additional vector $\hat{e}_n$, 
from the basis constructed in the previous section, to center the combinatorial 
construction of the generators of the algebra around. This vector plays a 
similar role to the diagonal in the previous construction and is used to balance 
the row and column sums back to zero. As in the previous section we start with 
a foundational definition.

\begin{definition}
	Let $St(\hat{\mathbbm{1}}, \hat{\mathbbm{1}})$ denote the doubly stochastic 
	Lie group of invertible matrices $A$ such that both $A$ and $A^T$ are 
	stochastic with respect to $\hat{\mathbbm{1}}$
\end{definition}

We can immediately observe with out proof that $St(\hat{\mathbbm{1}}, \hat{\mathbbm{1}}) = St(\hat{\mathbbm{1}}) \cap St^T(\hat{\mathbbm{1}})$;
leading to the next definition.

\begin{definition}
	Let $\mathfrak{st}(\hat{\mathbbm{1}}, \hat{\mathbbm{1}})$ denote the doubly 
	stochastic Lie algebra of $St(\hat{\mathbbm{1}})$.
\end{definition}

Again, it should be clear that $\mathfrak{st}(\hat{\mathbbm{1}}, \hat{\mathbbm{1}}) = \mathfrak{st}(\hat{\mathbbm{1}}) \cap \mathfrak{st}^T(\hat{\mathbbm{1}})$.
The implication being that $\mathfrak{st}(\hat{\mathbbm{1}}, \hat{\mathbbm{1}})$
is the algebra of all matrices $A$ such that $\hat{\mathbbm{1}}$ is in the 
kernel of both $A$ and $A^T$.

We can then find canonical generators of $\mathfrak{st}(\hat{\mathbbm{1}}, \hat{\mathbbm{1}})$
by similar methods as in the previous section. Given a constructed basis $\hat{e}_i$
such that $\left\langle \hat{e}_i, \hat{\mathbbm{1}} \right\rangle = \frac{1}{\sqrt{n}}$
we pick a single arbitrary element from the basis, say $\hat{e}_n$, the last
element for example. We then balance a transition rate from $i$ to $j$, with the
reverse rates from $i$ to $n$ and $n$ to $j$, and completing the circuit with
any entry for $n$ to $n$, yielding the matrix $C_{ijn} = C_{ij} + C_{ni} + C_{jn}$.

% Need a figure here of the circuit

The matrices $C_{ijn}$ are elements of $\mathfrak{st}(\hat{\mathbbm{1}})$. 
Furthermore they are a closed set with respect to matrix transposition, because 
$C_{ijn}^T = C_{jin}$. The algebra $\mathfrak{st}(\hat{\mathbbm{1}})$ is 
isomorphic to the space of $\left(n-1\right) \times \left(n-1\right)$ matrices,
which can be seen by the relationships, for any $i,j \le n-1$.

\begin{IEEEeqnarray*}{rCl}
	- C_{iin} = \hat{e}_i \otimes \hat{e_i} - \hat{e}_i \otimes \hat{e}_n - \hat{e}_n \otimes \hat{e}_i + \hat{e}_n \otimes \hat{e}_n\\
	C_{ijn} - C_{iin} - C_{jjn} = \hat{e}_i \otimes \hat{e_j} - \hat{e}_i \otimes \hat{e}_n - \hat{e}_n \otimes \hat{e}_j + \hat{e}_n \otimes \hat{e}_n
\end{IEEEeqnarray*}

The intuition being that any $n \times n$ matrix with fixed row and column sums
can be created by starting with any matrix and appending a compensating $n$ row
and $n$ column. These relationships are implicitly used extensively in proving 
the following lemma and corollary on the products, commutators, and structure 
constants of $C_{ijn}$.

\begin{lemma}
	\begin{IEEEeqnarray*}{rCl}
		C_{ijn}C_{kln} & = &
		\begin{cases}
			C_{jin} - 2C_{ijn} & i=k \text{ and } j=l,\\
			- \left(C_{ijn} + C_{jin}\right) & i=l \text{ and } j=k,\\
			C_{jin} - C_{iln} - C_{iin} - C_{jjn} + C_{lln} & i=k,\\
			C_{jkn} - C_{iin} - C_{jjn} - C_{kkn} & i=l,\\
			C_{iln} - C_{jln} - C_{ijn} & j=k,\\
			C_{iin} - C_{jjn} - C_{kkn} + C_{jkn} - C_{ijn} & j=l,\\
			C_{jjn} & \text{otherwise}.
		\end{cases}
	\end{IEEEeqnarray*}
\end{lemma}

\begin{proof}
	We proceed by calculating the terms of the products and then simplifying the 
	cases; assuming $i \neq j$, $k \neq l$, and $i,j,k,l \neq n$.
	\begin{enumerate}
		\item Term wise computation of the Kronecker products yields
		\begin{IEEEeqnarray*}{rCl}
			C_{ijn}C_{kln}
				& = & \left(\hat{e}_i \otimes \hat{e}_j - \hat{e}_i \otimes \hat{e}_i + \hat{e}_n \otimes \hat{e}_i - \hat{e}_n \otimes \hat{e}_n + \hat{e}_j \otimes \hat{e}_n - \hat{e}_j \otimes \hat{e}_j\right)\\
				&   & \cdot \left(\hat{e}_k \otimes \hat{e}_l - \hat{e}_k \otimes \hat{e}_k + \hat{e}_n \otimes \hat{e}_k - \hat{e}_n \otimes \hat{e}_n + \hat{e}_l \otimes \hat{e}_n - \hat{e}_l \otimes \hat{e}_l\right)\\
				& = & \delta_{jk} \hat{e}_i \otimes \hat{e}_l - \delta_{jk} \hat{e}_i \otimes \hat{e}_k + \delta_{jl} \hat{e}_i \otimes \hat{e}_n - \delta_{jl} \hat{e}_i \otimes \hat{e}_l\\
				&   & - \delta_{ik} \hat{e}_i \otimes \hat{e}_l + \delta_{ik} \hat{e}_i \otimes \hat{e}_k - \delta_{il} \hat{e}_i \otimes \hat{e}_n + \delta_{il} \hat{e}_i \otimes \hat{e}_l\\
				&   & \delta_{ik} \hat{e}_n \otimes \hat{e}_l - \delta_{ik} \hat{e}_n \otimes \hat{e}_k + \delta_{il} \hat{e}_n \otimes \hat{e}_n - \delta_{il} \hat{e}_n \otimes \hat{e}_l\\
				&   & - \hat{e}_n \otimes \hat{e}_k + \hat{e}_n \otimes \hat{e}_n + \hat{e}_j \otimes \hat{e}_k - \hat{e}_j \otimes \hat{e}_n\\
				&   & -\delta_{jk} \hat{e}_j \otimes \hat{e}_l + \delta_{jk} \hat{e}_j \otimes \hat{e}_k - \delta_{jl} \hat{e}_j \otimes \hat{e}_n + \delta_{jl} \hat{e}_j \otimes \hat{e}_l\\
				& = & C_{jk} - C_{jn} + C_{nk}\\
				&   & + \delta_{ik} \left(C_{nl} - C_{il} - C_{ni}\right) - \delta_{il} \left(C_{in} + C_{ni}\right)\\
				&   & + \delta_{jk} \left(C_{il} - C_{ij} - C_{jl}\right) + \delta_{jl} \left(C_{in} - C_{ij} - C_{jn}\right)
		\end{IEEEeqnarray*}
		\item The cases follow from simplifying the $\delta$ functions; starting
		with $i=k$ and $j=l$
		\begin{IEEEeqnarray*}{rCl}
			C_{ijn}C_{ijn}
				& = & C_{ji} - 2C_{jn} + C_{nj} - 2C_{ij} + C_{in}\\
				& = & C_{jin} - 2C_{ijn}
		\end{IEEEeqnarray*}
		\item When $i=l$ and $j=k$
		\begin{IEEEeqnarray*}{rCl}
			C_{ijn}C_{jin}
				& = & C_{nj} - C_{jn} - C_{in} - C_{ni} - C_{ij} - C_{ji}\\
				& = & - \left(C_{ijn} + C_{jin}\right)
		\end{IEEEeqnarray*}
		\item When $i=k$
		\begin{IEEEeqnarray*}{rCl}
			C_{ijn}C_{iln}
				& = & C_{ji} - C_{jn} + C_{ni} + C_{nl} - C_{il} -C_{ni}\\
				& = & C_{jin} - C_{iln} - C_{iin} - C_{jjn} + C_{lln}
		\end{IEEEeqnarray*}
		\item When $i=l$
		\begin{IEEEeqnarray*}{rCl}
			C_{ijn}C_{kin}
				& = & C_{jk} - C_{jn} + C_{nk} - C_{in} - C_{ni}\\
				& = & C_{jkn} - C_{iin} - C_{jjn} - C_{kkn}
		\end{IEEEeqnarray*}
		\item When $j=k$
		\begin{IEEEeqnarray*}{rCl}
			C_{ijn}C_{jln}
				& = & C_{nj} - C_{jn} + C_{il} - C_{ij} + C_{jl}\\
				& = & C_{iln} - C_{jln} - C_{ijn}
		\end{IEEEeqnarray*}
		\item When $j=l$
		\begin{IEEEeqnarray*}{rCl}
			C_{ijn}C_{kjn}
				& = & C_{ij} + 2C_{jk} + C_{nj} - C_{jn} - C_{ik}\\
				& = & C_{iin} - C_{jjn} - C_{kkn} + C_{jkn} - C_{ijn}
		\end{IEEEeqnarray*}
		\item When none of the conditions apply
		\begin{IEEEeqnarray*}{rCl}
			C_{ijn}C_{kln}
				& = & C_{jk} - C_{jn} + C_{nk}\\
				& = & C_{jjn}
		\end{IEEEeqnarray*}
	\end{enumerate}
\end{proof}

Moving immediately to the commutators we have

\begin{corollary}
	\begin{IEEEeqnarray*}{rCl}
		\left[C_{ijn},C_{kln}\right] & = &
		\begin{cases}
			0 & i=k \text{ and } j=l,\\
			0 & i=l \text{ and } j=k,\\
			C_{ijn} + C_{jin} - C_{iln} - C_{lin} - 2C_{jjn} - 2C_{lln} & i=k,\\
			C_{jkn} - C_{kjn} - C_{ijn} - C_{kin} - C_{iin} - C_{jjn} - C_{kkn} & i=l,\\
			C_{iln} - C_{lin} - C_{ijn} - C_{jln} - C_{iin} - C_{jjn} - C_{lln} & j=k,\\
			C_{jkn} + C_{kjn} - C_{ijn} - C_{jin} + 2C_{iin} - 2C_{kkn} & j=l,\\
			C_{jjn} - C_{lln} & \text{otherwise}.
		\end{cases}
	\end{IEEEeqnarray*}
\end{corollary}

\begin{proof}
	We work case wise through the equalities; assuming $i \neq j$, $k \neq l$, and 
	$i,j,k,l \neq n$.
	\begin{enumerate}
		\item Starting with $i=k$ and $j=l$
		\begin{IEEEeqnarray*}{rCl}
			\left[C_{ijn},C_{ijn}\right]
				& = & C_{ijn}C_{ijn} - C_{ijn}C_{ijn}\\
				& = & 0
		\end{IEEEeqnarray*}
		\item When $i=l$ and $j=k$
		\begin{IEEEeqnarray*}{rCl}
			\left[C_{ijn},C_{jin}\right]
				& = & C_{ijn}C_{jin} - C_{jin}C_{ijn}\\
				& = & - \left(C_{ijn} + C_{jin}\right) + \left(C_{jin} + C_{ijn}\right)\\
				& = & 0
		\end{IEEEeqnarray*}
		\item When $i=k$
		\begin{IEEEeqnarray*}{rCl}
			\left[C_{ijn},C_{iln}\right]
				& = & C_{ijn}C_{iln} - C_{iln}C_{ijn}\\
				& = & C_{ijn} + C_{jin} - C_{iln} - C_{lin} - 2C_{jjn} - 2C_{lln}
		\end{IEEEeqnarray*}
		\item When $i=l$
		\begin{IEEEeqnarray*}{rCl}
			\left[C_{ijn},C_{kin}\right]
				& = & C_{ijn}C_{kin} - C_{kin}C_{ijn}\\
				& = & C_{jkn} - C_{kjn} - C_{ijn} - C_{kin} - C_{iin} - C_{jjn} - C_{kkn}
		\end{IEEEeqnarray*}
		\item When $j=k$
		\begin{IEEEeqnarray*}{rCl}
			\left[C_{ijn},C_{jln}\right]
				& = & C_{ijn}C_{jln} - C_{jln}C_{ijn}\\
				& = & C_{iln} - C_{lin} - C_{ijn} - C_{jln} - C_{iin} - C_{jjn} - C_{lln}
		\end{IEEEeqnarray*}
		\item When $j=l$
		\begin{IEEEeqnarray*}{rCl}
			\left[C_{ijn},C_{kjn}\right]
				& = & C_{ijn}C_{kjn} - C_{kjn}C_{ijn}\\
				& = & C_{jkn} + C_{kjn} - C_{ijn} - C_{jin} + 2C_{iin} - 2C_{kkn}
		\end{IEEEeqnarray*}
		\item When none of the conditions apply
		\begin{IEEEeqnarray*}{rCl}
			\left[C_{ijn},C_{kln}\right]
				& = & C_{ijn}C_{kln} - C_{kln}C_{ijn}\\
				& = & C_{jjn} - C_{lln}
		\end{IEEEeqnarray*}
	\end{enumerate}
\end{proof}

Restricting to the matrices where both the row and column sums are zero, which
is equivalent to demanding the conservation of the transition rates, or 
infinitesimal flows of probability, introduces a significant degree of
complexity to the algebra. In particular demanding that all the transition rates
be balanced by transitions through $\hat{e}_n$ means that only the simplest two
and three state processes have easily calculable algebras. Nevertheless, the 
result makes the sibling theorem accessible.

\begin{theorem}
	$C_{ijn}$ are canonical generators of $\mathfrak{st}(\hat{\mathbbm{1}}, \hat{\mathbbm{1}})$.
\end{theorem}

\begin{proof}
	The proof proceeds in the same manner as the proof of the sibling theorem in
	the previous section.
	\begin{enumerate}
		\item As discussed before $\mathfrak{st}(\hat{\mathbbm{1}}, \hat{\mathbbm{1}})$
		is an $\left(n-1\right)^2$ dimensional vector space.
		\item By construction there are only $\left(n-1\right)^2$ matrices $C_{ijn}$
		for a fixed choice of $\hat{e}_n$.
		\item Through induction the matrices $C_{ijn}$ are linearly independent for
		a fixed choice of $\hat{e}_n$.
		\item Thus the matrices $C_{ijn}$, for a fixed choice of $\hat{e}_n$, are a 
		basis for $\mathfrak{st}(\hat{\mathbbm{1}}, \hat{\mathbbm{1}})$.
		\item By the previous lemma the commutators of matrices $C_{ijn}$ are linear
		combinations of themselves.
		\item It follows then that the smallest algebra that contains the matrices $C_{ijn}$
		is $\mathfrak{st}(\hat{\mathbbm{1}}, \hat{\mathbbm{1}})$.
	\end{enumerate}
\end{proof}

As with the stochastic Lie algebra the generators of the doubly stochastic Lie 
algebra are not unique, not only do they depend on the choice of the basis $\hat{e}_i$ 
but also on the choice of the basis element $\hat{e}_n$ used to sum the rows and 
columns to zero.

% Unlike stochastic Lie group there are no nice guareentees on properness with
% respect to postive sums. No matter what generators of the algebra are chosen
% there will always be Markov processes that require negative terms in the sum.
% the physical intuition is that to composing cicuits will require cancellation.

% Discuss Birkhoff polytope versus Lie group, construct vertexes vis-a-vi 
% Birkhoff-von Neumann theorem. Again vector space structure means every
% DS is linear sum.

% Basis choice unqiuely gives the Birkhoff polytope, it is independent of the
% choice of conservation unit vector. Requires proof the vertexes are 
% independent of conservation vector.