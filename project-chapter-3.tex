\chapter{Pad\'{e} Approximation of the Fr\'{e}chet Derivatives of the Exponential Map}
\section{The Gradient}
Moler and Van Loan seminally reviewed algorithms for calculating the matrix exponential in
1978, and revisited that review in 2003 \cite{moler_nineteen_1978,moler_nineteen_2003}. 
Building on the discussions of Moler and Van Loan, Higham established the standard 
implementation of the matrix exponential based on scaling and scaring, and Pad\'{e} 
approximation \cite{higham_scaling_2005,higham_functions_2008}. The Higham implementation was
further optimized for $64$ bit architectures by Al-Mohy \cite{al-mohy_new_2009}. In the same
work Al-Mohy developed an algorithm to approximate the derivative of the matrix exponential, 
formulated by taking the derivative of the Pad\'{e} approximation of the matrix exponential 
and then working out a recursive calculation for the derivatives of matrix 
powers \cite{al-mohy_computing_2009}.

While the derivative of the Pad\'{e} approximation of an analytic function will converge to 
the derivative of the analytic function, it is not true that the derivative of the Pad\'{e} 
approximation of an analytic function is the Pad\'{e} approximation of the derivative of an 
analytic function. In the sense that Pad\'{e} approximations of analytic functions are an 
optimal series of algebraic approximations, the method proposed by Al-Mohy in 2009 is not 
optimal.

We can further flush out the difficulties with using the derivative of the Pad\'{e} 
approximation of a function to approximate the derivative of the function. By definition
a Pad\'{e} approximation is the ratio of two polynomials, of order $m$ in the numerator
and order $n$ in the denominator, so that the approximation has $m$ roots and $n$ poles. The
$k^{th}$ derivative then has $m \times \left(n-1\right)^k$ roots, and $n$ poles. If any of 
the roots of the $\left(k+1\right)^{th}$ derivative occurs on a particular domain, then the 
$k^{th}$ derivative can become arbitrarily large, even if the Pad\'{e} approximation is 
within $\pm\epsilon$ of the actual function, on the domain. This follows from the mean value
theorem, which asserts that if the function has a particular average slope on a domain, the
upper limit of the slope is inversely proportional to the lower limit of the slope. In 
essence, the Pad\'{e} approximation can oscillate on a particular domain within the 
$\pm\epsilon$ error bounds.

In this chapter we will develop an approximation for the first, and second order Fr\'{e}chet
derivatives of the matrix exponential, by decomposing the derivatives into components that
hold for the commutative condition and components containing the perturbation due to 
non-commutativity. We will then derive the Pad\'{e} approximation for the non-commutative 
perturbation.

We begin by listing the following eight forms of the Fr\'{e}chet derivative of exponential 
map, in the tangent direction $\frac{\partial X}{\partial x}$ at the point $X$ in the Lie 
algebra:
{\setlength{\IEEEnormaljot}{18pt}
\begin{IEEEeqnarray*}{rClls}
	\frac{\partial e^X}{\partial x}
		& = & e^X \left[\int_0^1 e^{- s\operatorname{ad}_{X} \cdotp} ds \right] \left(\frac{\partial X}{\partial x}\right) & & \\
		& = & e^X \left[\frac{1 - e^{-\operatorname{ad}_X \cdotp}}{\operatorname{ad}_X \cdotp}\right]\left(\frac{\partial X}{\partial x}\right) & \smash{\left. \IEEEstrut[4\jot] \right\rbrace} & \text{left recursive} \\
		& = & e^X \left[\sum_{n=0}^{\infty} \frac{\left(-1\right)^n}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\frac{\partial X}{\partial x}\right) & & \\
		& = & \left[ \frac{\operatorname{ad}_{e^X} \cdotp}{\operatorname{ad}_X \cdotp} \right]\left(\frac{\partial X}{\partial x}\right) & & \text{adjoint ratio} \\
		& = & e^{\frac{1}{2}X} \left[ \frac{e^{\frac{1}{2}\operatorname{ad}_X \cdotp } - e^{-\frac{1}{2}\operatorname{ad}_X \cdotp}}{\operatorname{ad}_X \cdotp} \right]\left(\frac{\partial X}{\partial x}\right) e^{\frac{1}{2}X} & & \text{hyperbolic} \\
		& = & \left[ \int_0^1 e^{s \operatorname{ad}_{X} \cdotp} ds \right]\left(\frac{\partial X}{\partial x}\right) e^X & &\\
		& = & \left[ \frac{e^{\operatorname{ad}_X \cdotp} - 1}{\operatorname{ad}_X \cdotp} \right] \left(\frac{\partial X}{\partial x}\right) e^X & \smash{\left. \IEEEstrut[4\jot] \right\rbrace} & \text{right recursive}\\
		& = & \left[\sum_{n=0}^{\infty} \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\frac{\partial X}{\partial x} \right) e^X & &
\end{IEEEeqnarray*}}
With respect to the adjoint operator, we are using the currying partial application notation
of $\left[L f\left(\cdotp\right)\right]\left(y\right)$ to indicate the application of the 
operator $L$ to $f\left(x\right)$ followed by evaluation of the result at $y$. We have also
abused and confounded the notations for directional derivatives and partial derivatives here 
by assuming that $X$ is parameterized by $x$ so that $\frac{\partial e^X}{\partial x}$ is 
the derivative in the direction of change of $x$.

The last equality demonstrates the non-commutative perturbation term most clearly. The first
multiplicative factor in the derivative accounts for the lack of commutativity between $X$ 
and $\frac{\partial X}{\partial x}$, and the last term resembles the derivative in the
commutative case. This can be seen when considering the condition $\left[X,\frac{\partial X}{\partial x}\right]=0$,
in which case $\frac{\partial e^X}{\partial x} = \frac{\partial X}{\partial x} e^X$. 

Even though the multiplicative factorization provides a transparent representation of the 
computational terms, it is still far from optimal; because, when compared to matrix addition, 
matrix multiplication is both computationally more expensive and less numerically stable. 
The numerical stability and efficiency can be improved by decomposing the first 
multiplicative factor into a linear sum of a non-commutative perturbation term, which will 
reduce to $0$ when $\left[X,\frac{\partial X}{\partial x}\right]=0$, and an invariant term 
that contains the commutative relationship for all $X$:
\begin{IEEEeqnarray*}{rCl}
	\frac{\partial e^X}{\partial x}
		& = & \left[\frac{e^{\operatorname{ad}_X \cdotp} - 1 - \operatorname{ad}_X \cdotp }{\operatorname{ad}_X^2 \cdotp} \right] \left(\operatorname{ad}_X \frac{\partial X}{\partial x} \right) e^X + \frac{\partial X}{\partial x} e^X\\
		& = & \underbrace{\left[\sum_{n=0}^{\infty} \frac{1}{\left(n+2\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\operatorname{ad}_X \frac{\partial X}{\partial x} \right)}_{\text{non-commutative anomaly}} e^X + \underbrace{\frac{\partial X}{\partial x}}_{\text{invariant}} e^X
\end{IEEEeqnarray*}
Formally the infinite series in the non-commutative perturbation is related to the lower 
incomplete gamma function $\gamma\left(n,x\right)$. This can be seen by considering the 
general case when the offset of $2$ in the factorial is allowed to be any natural number $n$, 
and then restating the sum in terms of a truncated exponential series.
\begin{IEEEeqnarray*}{rCl}
	\sum_{m=0}^{\infty} \frac{x^m}{\left(m+n\right)!}
		& = & \frac{1}{x^n} \sum_{m=n}^{\infty} \frac{x^m}{m!}\\
		& = & \frac{1}{x^n} \left(e^x - \sum_{m=0}^{n-1}\frac{x^m}{m!} \right)\\
		& = & \frac{1}{x^n} \left(e^x - e^x \frac{\Gamma\left(n,x\right)}{\Gamma\left(n\right)}\right)\\
		& = & \frac{e^x}{\left(n-1\right)! x^n} \left(\int_0^{\infty} t^{n-1}e^{-t}dt - \int_x t^{n-1} e^{-t}dt \right)\\
		& = & \frac{e^x}{\left(n-1\right)! x^n} \int_0^x t^{n-1}e^{-t}dt\\
		& = & \frac{e^x}{\left(n-1\right)! x^n} \gamma\left(n,x\right)
\end{IEEEeqnarray*}
The non-commutative perturbation series is linear in $\frac{\partial X}{\partial x}$  and a 
Taylor series in the powers of $\operatorname{ad}_X \cdotp$. Thus any computation of an 
approximation will be in the powers of $\operatorname{ad}_X \cdotp$.

As was discussed in the Moler and Van Loan \cite{moler_nineteen_1978,moler_nineteen_2003}, 
naive computation of the Taylor series itself results in an approximation that will converge 
slowly, requiring a larger number of powers to be computed before the threshold of floating 
point error is reached. Pad\'{e} approximation by rational functions remedies this problem, 
by offering convergence to the threshold of floating point error in smaller powers and 
fewer computational steps.

Given that $\frac{e^{x} -1 - x}{x^2}$ is a rational perturbation of $e^x$, why not simply 
reuse the polynomials of the Pad\'{e} approximation of the exponential function to compute 
new polynomials for a rational approximation of the non-commutative perturbation Taylor 
series? This method has two shortcomings: first, the approximation found in this manner is 
not itself a Pad\'{e} approximation of the anomaly Taylor series, and so is not bounded by 
the same theoretical asymptotic results as Pad\'{e} approximations; second, computation by $\frac{e^{x} - 1 - x}{x^2}$ 
suffers from the same floating point errors near $0$ as naive computation of $e^x - 1$ by 
first computing the approximation of $e^x$ and then subtracting $1$
\footnote{Contemporary implementations of $e^x - 1$ approximate the function with $x$ in a
small neighborhood of zero.}.

In the case of a field such as $\mathbb{R}$ or $\mathbb{C}$ the function $\frac{e^{x} -1 - x}{x^2}$
is an analytic back into the field. It follows that Pad\'{e} approximation with coefficients 
in the field will yield values in the field, and approximates within $\pm\epsilon$ on an 
appropriate domain of convergence. However, in the case of a ring, such as bounded linear 
operators on a Lie algebra, it is not as clear that Pad\'{e} approximation is valid. In 
particular, the meaning of the polynomial denominator in the Pad\'{e} approximation needs 
careful consideration when working in a ring that may not have inverses for every element, 
such as the case with $\operatorname{ad}_X \cdotp$. Recall that $\operatorname{ad}_X \cdotp$ 
is not invertible because $\ker\left(\operatorname{ad}_X \cdotp\right) = \left\lbrace Y : \left[X,Y\right]=0 \right\rbrace$, 
and includes $aI+bX$, for any pair of scalars $a,b$ in the field of the Lie algebra. 

When $X \in \mathfrak{st}(\hat{\mathbbm{1}})$, the adjoint operator $\operatorname{ad}_X \cdotp$
is a linear endomorphism on the vector space $\mathfrak{st}(\hat{\mathbbm{1}})$ and belongs 
to the algebra of general linear operators $GL\left(\mathfrak{st}(\hat{\mathbbm{1}})\right)$. 
For a Pad\'{e} approximation with numerator polynomial $P\left(X\right)$ and denominator 
polynomial $Q\left(X\right)$, that $\operatorname{ad}_X \cdotp \in GL\left(\mathfrak{st}(\hat{\mathbbm{1}})\right)$
implies that $P\left(\operatorname{ad}_X \cdotp\right), Q\left(\operatorname{ad}_X \cdotp\right) \in GL\left(\mathfrak{st}(\hat{\mathbbm{1}})\right)$.
It follows that when a solution $Y$ to $\left[P\left(\operatorname{ad}_X \cdotp\right)\right] \left(\frac{\partial X}{\partial x}\right) = \left[Q\left(\operatorname{ad}_X \cdotp\right)\right] \left(Y\right) $
exists, it is guaranteed to belong to $\mathfrak{st}(\hat{\mathbbm{1}})$, because $\frac{\partial X}{\partial x} \in \mathfrak{st}(\hat{\mathbbm{1}})$.

The remaining question is whether or not $Y$, a solution to $\left[P\left(\operatorname{ad}_X \cdotp\right)\right] \left(\frac{\partial X}{\partial x}\right) = \left[Q\left(\operatorname{ad}_X \cdotp\right)\right] \left(Y\right) $,
is an approximation of $\left[\sum_{n=0}^{\infty} \frac{1}{\left(n+2\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\operatorname{ad}_X \frac{\partial X}{\partial x} \right)$.
Loosely, $\mathfrak{st}(\hat{\mathbbm{1}})$ is a normed vector space in the usual sense, so 
the convergence of the Pad\'{e} approximations that apply in scalar spaces carries over. 
Thus given a sequence of Pad\'{e} approximations $Y_{mn}$ that solve $\left[P_n\left(\operatorname{ad}_X \cdotp\right)\right] \left(\frac{\partial X}{\partial x}\right) = \left[Q_m\left(\operatorname{ad}_X \cdotp\right)\right] \left(Y_{mn}\right) $,
convergence $Y_{mn} \rightarrow \left[\sum_{n=0}^{\infty} \frac{1}{\left(n+2\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\operatorname{ad}_X \frac{\partial X}{\partial x} \right)$
is assured. 

Recapitulating the definition of the $\left[n/m\right]_f\left(x\right)$ Pad\'{e} approximations, 
we seek a rational polynomial approximation to the Taylor series
\begin{IEEEeqnarray*}{rCl}
	\frac{P_n\left(x\right)}{Q_m\left(x\right)}
		& =       & \frac{p_0 + p_1 x + \cdots + p_n x^n}{1 + q_1 x + \cdots + q_m x^m}\\
		& \approx & \sum_{n=0}^\infty \frac{1}{\left(n+2\right)!}x^n
\end{IEEEeqnarray*}
such that the first $n+m$ derivatives of the rational polynomial approximation evaluated 
at $x=0$ equal the first $n+m$ coefficients of the Taylor series, i.e.
\begin{IEEEeqnarray*}{rCl}
	\left.\frac{d^k}{d x^k}\frac{P_n\left(x\right)}{Q_m\left(x\right)}\right|_{x=0}
		& = & \frac{1}{\left(k+2\right)!} \text{ with } 0 \le k \le n+m.
\end{IEEEeqnarray*}
The symbolic computation of the exact rational coefficients of the $\left[11/12\right]_f\left(x\right)$ 
Pad\'{e} approximation was carried out in Julia programming language \cite{bezanson_julia:_2014}
using big integer methods and the Polynomials package. The results of the computation are 
displayed in table \ref{tab:perturbation}. The order of the Pad\'{e} approximation was 
chosen so that the largest denominator in the rational coefficients of the numerator
polynomial $\left(2698531355520000\right)$ and the largest denominator in the rational 
coefficients of the denominator polynomial $\left(2490952020480000\right)$ were the 
largest integers less than $2^{53}$ $\left(9007199254740992\right)$. The number $2^{53}$
is the largest $64$ bit signed float with no loss of integer precision. This choice of 
approximation will need further research to be better optimized.

To make use of the Pad\'{e} approximation we need to be able to compute powers of $\operatorname{ad}_X \cdotp$.
This can be accomplished through the Kronecker representation of $\operatorname{ad}_X \cdotp$
which requires representing the matrices of $\mathfrak{st}(\hat{\mathbbm{1}})$ as vectors. 
The vector representation of a matrix is achieved by the matrix reshaping  operator $\operatorname{vec}\left(Y\right) = \vec{y}$
which forms a vector $\vec{y}$ by concatenation of the columns of $Y$, called the 
vectorization of the matrix. We denote the inverse operator to vectorization $\operatorname{mat}\left(\vec{y}\right) = \operatorname{vec}^{-1}\left(\vec{y}\right) = Y$,
which reshapes a vector, of $n^2$ entries, into an $n \times n$ matrix.

After juggling the indexes of $\operatorname{vec}\left(\frac{\partial X}{\partial x}\right)$, 
the Kronecker representation of $\operatorname{ad}_X \frac{\partial X}{\partial x}$ follows
as
\begin{IEEEeqnarray*}{rCl}
	\operatorname{ad}_X \frac{\partial X}{\partial x} 
		& = & \operatorname{mat}\left( \left(I \otimes X - X^\dagger \otimes I \right) \operatorname{vec}\left(\frac{\partial X}{\partial x}\right)\right)
\end{IEEEeqnarray*}
Proceeding by induction we find that 
\begin{IEEEeqnarray*}{rCl}
	\operatorname{ad}_X^n \frac{\partial X}{\partial x} 
		& = & \operatorname{mat}\left(\left(I \otimes X - X^\dagger \otimes I \right)^n\operatorname{vec}\left(\frac{\partial X}{\partial x}\right)\right)
\end{IEEEeqnarray*}
It follows that $\frac{\partial e^X}{\partial x}$ can be computed by
\begin{IEEEeqnarray*}{rCl}
	\frac{\partial e^X}{\partial x}
		& = & \operatorname{mat}\left(\sum_{n=0}^{\infty} \frac{1}{\left(n+2\right)!} \left(I \otimes X - X^\dagger \otimes I \right)^{n+1} \operatorname{vec}\left(\frac{\partial X}{\partial x}\right)\right) e^X + \frac{\partial X}{\partial x} e^X
\end{IEEEeqnarray*}
and thus can be approximated by
\begin{IEEEeqnarray*}{rCl}
	\frac{\partial e^X}{\partial x}
		& \approx & \operatorname{mat}\left(\frac{P_{11}\left(I \otimes X - X^\dagger \otimes I\right)}{Q_{12} \left(I \otimes X - X^\dagger \otimes I\right)} \left(I \otimes X - X^\dagger \otimes I \right)\operatorname{vec}\left(\frac{\partial X}{\partial x}\right) \right) e^X + \frac{\partial X}{\partial x} e^X.
\end{IEEEeqnarray*}
By application of the chain rule this computational method is sufficient to calculate the
derivative of any parameterization of $X$ in the matrix exponential $e^X$.

We can summarize this work in an algorithm to compute the perturbation \ref{alg:perturbation}
and matrix exponential \ref{alg:gradient}. This algorithm serves as a sketch only
highlighting the novel elements developed in this section. Many additional optimizations 
could be implemented including minimizing memory assignments by carrying out in place 
computations, conditioning matrices to improve numerical stability and using recursive 
squaring and summing methods to efficiently compute the matrix polynomials. The first 
assignment of the Kronecker product is a point of concern about performance, because it 
results in a quadratic increase in the amount of memory used. 
\section{The Hessian}
The Hessian of $e^X$ exists when the parameterization of $X$ by $x,y \in \mathbb{R}$ is 
analytic, or at least twice differentiable. Furthermore the Hessian is a linear map of three
tangent matrices $\frac{\partial X}{\partial x}$, $\frac{\partial X}{\partial y}$, and $\frac{\partial^2 X}{\partial x \partial y}$.
In general, none of these tangent matrices need commute with each other. Even the second 
derivative $\frac{\partial^2 X}{\partial x^2}$ will not generally commute with the first 
derivate $\frac{\partial X}{\partial x}$. While the additional terms complicate the 
computations, they do not lead to intractable results in the same way that finding a closed
form for $e^X$ in dimensions greater than four becomes intractable. Unfortunately in the 
most general case when $X$ neither commutes with $\frac{\partial X}{\partial x}$ nor $\frac{\partial X}{\partial y}$,
we will find that we have to compute the Taylor series of a bilinear operator, which is not 
susceptible to Pad\'{e} approximation.

To proceed we need a pair of results focused on the binomial combinatorics of adjoints. We
will use these results in developing the bilinear non-commutative perturbation of the
Hessian.
\begin{lemma}
	For any differentiable matrix function $X$ parameterized by $x \in \mathbb{R}$, matrices $A$ 
	$B$, and integer $0 \le n$,
	\begin{IEEEeqnarray*}{lrCl}
		1\text{. } & \left[\frac{\partial}{\partial x}\operatorname{ad}_X^n \cdot\right]\left( A\right)
			& = & \sum_{k=1}^n \left(\operatorname{ad}_X^{k-1} A \right)\left(\operatorname{ad}_{\frac{\partial X}{\partial x}} A\right)  \left(\operatorname{ad}_X^{n-k} A \right)\\
		2\text{. } & \operatorname{ad}_X^n AB
			& = & \sum_{k=0}^n \binom{n}{k} \left(\operatorname{ad}_X^k A \right)\left(\operatorname{ad}_X^{n-k} B \right)\\
		3\text{. } & \operatorname{ad}_X^n \left[A,B\right]
			& = & \sum_{k=0}^n \binom{n}{k} \left[\operatorname{ad}_X^k A , \operatorname{ad}_X^{n-k} B \right]
	\end{IEEEeqnarray*}
\end{lemma}
\begin{IEEEproof}
	For each of the numbered equalities we have:
	\begin{enumerate}
		\item For first equality proceed by induction on $n$.
		\item For second equality proceed by induction on $n$.
		\item For third equality take the antisymmetric difference $\operatorname{ad}_X^n AB - \operatorname{ad}_X^n BA$ 
		of the second equality.\hfill\IEEEQEDhere
	\end{enumerate}
\end{IEEEproof}
We will also find use of the following corollary to the binomial theorem.
\begin{corollary}
	For any $n,m \ge 0$,
	\begin{IEEEeqnarray*}{rCl}
		\sum_{k=m}^{n+m} \binom{k}{m}
			& = & \binom{n+m+1}{n}
	\end{IEEEeqnarray*}
\end{corollary}
\begin{IEEEproof}
	Proceed by induction on $n$.\hfill\IEEEQEDhere
\end{IEEEproof}
In the next step of developing the Hessian we derive the Taylor series for the bilinear 
non-commutative perturbation. Only in two particular cases does the bilinear map admit the 
formulation of Pad\'{e} approximation.
\begin{corollary}
	For any differentiable matrix function $X$ parameterized by $x,y \in \mathbb{R}$,
	\begin{IEEEeqnarray*}{rCl}
		\IEEEeqnarraymulticol{3}{l}
		{
			\left[\frac{\partial}{\partial x} \sum_{n=1}^\infty \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right]\left(\frac{\partial X}{\partial y}\right)
			+ \left[\frac{\partial}{\partial y} \sum_{n=1}^\infty \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right]\left(\frac{\partial Y}{\partial x}\right)
		}\\\quad
			& = & \sum_{n \ge m \ge 0} \frac{1}{\left(n+2\right)!} \left(\binom{n+1}{m+1} - \binom{n+1}{m} \right) \left[\operatorname{ad}_X^{m} \frac{\partial X}{\partial x} ,\operatorname{ad}_X^{n-m} \frac{\partial X}{\partial y} \right]
	\end{IEEEeqnarray*}
\end{corollary}
\begin{IEEEproof}
	Applying the previous results in the order they were stated to the symmetric sum of the two
	terms, we have
	\begin{IEEEeqnarray*}{rCl}
		\IEEEeqnarraymulticol{3}{l}
		{
			\left[\frac{\partial}{\partial x} \sum_{n=1}^\infty \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right]\left(\frac{\partial X}{\partial y}\right)
			+ \left[\frac{\partial}{\partial y} \sum_{n=1}^\infty \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right]\left(\frac{\partial Y}{\partial x}\right)
		}\\\quad
			& = & \sum_{n=1}^\infty \frac{1}{\left(n+1\right)!} \sum_{k=1}^n \operatorname{ad}_X^{k-1} \operatorname{ad}_{\frac{\partial X}{\partial x}} \operatorname{ad}_X^{n-k} \frac{\partial X}{\partial y}\\
			&   & +\: \sum_{n=1}^\infty \frac{1}{\left(n+1\right)!} \sum_{k=1}^n \operatorname{ad}_X^{k-1} \operatorname{ad}_{\frac{\partial X}{\partial y}} \operatorname{ad}_X^{n-k} \frac{\partial X}{\partial x}\\
			& = & \sum_{n=1}^\infty \frac{1}{\left(n+1\right)!} \sum_{k=1}^n \operatorname{ad}_X^{k-1} \left[ \frac{\partial X}{\partial x} ,\operatorname{ad}_X^{n-k} \frac{\partial X}{\partial y} \right]\\
			&   & +\: \sum_{n=1}^\infty \frac{1}{\left(n+1\right)!} \sum_{k=1}^n \operatorname{ad}_X^{k-1} \left[ \frac{\partial X}{\partial y} ,\operatorname{ad}_X^{n-k} \frac{\partial X}{\partial x} \right]\\
			& = & \sum_{n=1}^\infty \frac{1}{\left(n+1\right)!} \sum_{k=1}^n \sum_{m=0}^{k-1} \binom{k-1}{m} \left[\operatorname{ad}_X^{m} \frac{\partial X}{\partial x} ,\operatorname{ad}_X^{n-m-1} \frac{\partial X}{\partial y} \right]\\
			&   & +\: \sum_{n=1}^\infty \frac{1}{\left(n+1\right)!} \sum_{k=1}^n \sum_{m=0}^{k-1} \binom{k-1}{m} \left[\operatorname{ad}_X^{m} \frac{\partial X}{\partial y} ,\operatorname{ad}_X^{n-m-1} \frac{\partial X}{\partial x} \right]\\
			& = & \sum_{n=0}^\infty \frac{1}{\left(n+2\right)!} \sum_{k=0}^n \sum_{m=0}^{k} \binom{k}{m} \left[\operatorname{ad}_X^{m} \frac{\partial X}{\partial x} ,\operatorname{ad}_X^{n-m} \frac{\partial X}{\partial y} \right]\\
			&   & +\: \sum_{n=0}^\infty \frac{1}{\left(n+2\right)!} \sum_{k=0}^n \sum_{m=0}^{k} \binom{k}{m} \left[\operatorname{ad}_X^{m} \frac{\partial X}{\partial y} ,\operatorname{ad}_X^{n-m} \frac{\partial X}{\partial x} \right]\\
			& = & \sum_{n,m \ge 0}^\infty \frac{1}{\left(n+m+2\right)!} \left[\operatorname{ad}_X^{m} \frac{\partial X}{\partial x} ,\operatorname{ad}_X^{n} \frac{\partial X}{\partial y} \right] \sum_{k=m}^{n+m} \binom{k}{m}\\
			&   & +\: \sum_{n,m \ge 0}^\infty \frac{1}{\left(n+m+2\right)!} \left[\operatorname{ad}_X^{n} \frac{\partial X}{\partial y} ,\operatorname{ad}_X^{m} \frac{\partial X}{\partial x} \right] \sum_{k=n}^{n+m} \binom{k}{n}\\
			& = & \sum_{n,m \ge 0}^\infty \frac{1}{\left(n+m+2\right)!} \left(\binom{n+m+1}{n} - \binom{n+m+1}{m} \right) \left[\operatorname{ad}_X^{m} \frac{\partial X}{\partial x} ,\operatorname{ad}_X^{n} \frac{\partial X}{\partial y} \right]\\
			& = & \sum_{n \ge m \ge 0} \frac{1}{\left(n+2\right)!} \left(\binom{n+1}{m+1} - \binom{n+1}{m} \right) \left[\operatorname{ad}_X^{m} \frac{\partial X}{\partial x} ,\operatorname{ad}_X^{n-m} \frac{\partial X}{\partial y} \right]\\
			& = & \sum_{n=0}^\infty \frac{1}{\left(n+2\right)!} F_n,
	\end{IEEEeqnarray*}
	here we define $F_n$ recursively as
	\begin{IEEEeqnarray*}{rCl}
		F_0 & = & 0\\
		F_{n+1} & = &  \left[\frac{\partial X}{\partial x},\operatorname{ad}_X^{n+1} \frac{\partial X}{\partial y}\right] + \operatorname{ad}_X F_n - \left[\operatorname{ad}_X^{n+1} \frac{\partial X}{\partial x},\frac{\partial X}{\partial y}\right]
	\end{IEEEeqnarray*}
	This recursive calculation is illustrate in figure \ref{fig:pascaldivergence} and the 
	proof can be given by carrying out induction on $n$.\hfill\IEEEQEDhere
\end{IEEEproof}
There are three special cases that simplify the calculation of Taylor series considerably, 
i.e.
\begin{IEEEeqnarray*}{rCl}
	\sum_{n=0}^\infty \frac{ F_n}{\left(n+2\right)!}
	& = & 
		\begin{cases}
			0
				& \operatorname{ad}_X \frac{\partial X}{\partial x} = 0 \text{ and } \operatorname{ad}_X \frac{\partial X}{\partial y} = 0\\[1ex]
			\left[\frac{\partial X}{\partial x}, \sum_{n=0}^\infty \frac{n}{\left(n+2\right)!} \operatorname{ad}_X^n \frac{\partial X}{\partial y}\right]
				& \operatorname{ad}_X  \frac{\partial X}{\partial x} = 0\\[1ex]
			\left[\frac{\partial X}{\partial y}, \sum_{n=0}^\infty \frac{n}{\left(n+2\right)!} \operatorname{ad}_X^n \frac{\partial X}{\partial x}\right]
				& \operatorname{ad}_X  \frac{\partial X}{\partial y} = 0
		\end{cases}
\end{IEEEeqnarray*}
The Taylor series in the last two cases does admit a Pad\'{e} approximation, by the same
reasoning as presented in the previous section. In particular we can further reduce the
Taylor series.
\begin{IEEEeqnarray*}{rCl}
	\sum_{n=0}^\infty \frac{n}{\left(n+2\right)!} x^n
		& = & \sum_{n=1}^\infty \frac{n}{\left(n+2\right)!} x^n\\
		& = & \left(\sum_{n=0}^\infty \frac{n+1}{\left(n+3\right)!} x^n\right)x
\end{IEEEeqnarray*}
Using the same criteria as in the previous section yields a $\left[10/12\right]_f\left(x\right)$
Pad\'{e} approximation for $f\left(x\right) = \sum_{n=0}^\infty \frac{n+1}{\left(n+3\right)!} x^n$.
The coefficients of the Pad\'{e} approximation are summarized in table \ref{tab:bilinear}. 
This approximation is then used in a pair of branches, one for the linear case \ref{alg:linear},
the other for the bilinear case \ref{alg:bilinear}, in the calculation of the perturbation 
to the Hessian of the matrix exponential \ref{alg:second}.

Assuming that $\frac{\partial^2 X}{\partial x \partial y},\frac{\partial^2 X}{\partial y \partial x}$ 
are continuous we have, by corollary to Clairaut's theorem, that $\frac{\partial^2 e^X}{\partial x \partial y} = \frac{\partial^2 e^X}{\partial y \partial x}$.
We can then compute the Hessian by symmetrizing the partial differential so that 
antisymmetric terms cancel out.
\begin{IEEEeqnarray*}{rCl}
	\frac{1}{2} \left(\frac{\partial^2 e^X}{\partial x \partial y} + \frac{\partial^2 e^X}{\partial y \partial x}\right)
		& = & \frac{1}{2} \frac{\partial}{\partial x} \left[\sum_{n=0}^{\infty} \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\frac{\partial X}{\partial y} \right) e^X\\
		&   & +\: \frac{1}{2} \frac{\partial}{\partial y} \left[\sum_{n=0}^{\infty} \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\frac{\partial X}{\partial x} \right) e^X\\
		& = & \frac{1}{2} \left[\sum_{n=0}^{\infty} \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\frac{\partial X}{\partial y} \right) \left[\sum_{n=0}^{\infty} \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\frac{\partial X}{\partial x} \right) e^X\\
		&   & +\: \frac{1}{2} \left[\sum_{n=0}^{\infty} \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\frac{\partial^2 X}{\partial x \partial y} \right) e^X\\
		&   & \quad +\: \frac{1}{2} \left[ \frac{\partial}{\partial x} \sum_{n=0}^{\infty} \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\frac{\partial X}{\partial y}\right) e^X\\
		&   & +\: \frac{1}{2} \left[\sum_{n=0}^{\infty} \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\frac{\partial X}{\partial x} \right) \left[\sum_{n=0}^{\infty} \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\frac{\partial X}{\partial y} \right) e^X\\
		&   & \quad +\: \frac{1}{2} \left[\sum_{n=0}^{\infty} \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\frac{\partial^2 X}{\partial y \partial x} \right) e^X\\
		&   & \quad\quad +\: \frac{1}{2} \left[\frac{\partial}{\partial y} \sum_{n=0}^{\infty} \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\frac{\partial X}{\partial x}\right) e^X\\
		& = & \left[\sum_{n=0}^{\infty} \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\frac{\partial^2 X}{\partial x \partial y} \right) e^X\\
		&   & +\: \frac{1}{2} \left[\sum_{n=0}^{\infty} \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\frac{\partial X}{\partial y} \right) \left[\sum_{n=0}^{\infty} \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\frac{\partial X}{\partial x} \right) e^X\\
		&   & +\: \frac{1}{2} \left[\sum_{n=0}^{\infty} \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\frac{\partial X}{\partial x} \right) \left[\sum_{n=0}^{\infty} \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\frac{\partial X}{\partial y} \right) e^X\\
		&   & +\: \frac{1}{2} \sum_{n \ge m \ge 0} \frac{1}{\left(n+2\right)!} \left(\binom{n+1}{m+1} - \binom{n+1}{m} \right) \left[\operatorname{ad}_X^{m} \frac{\partial X}{\partial x} ,\operatorname{ad}_X^{n-m} \frac{\partial X}{\partial y} \right]
\end{IEEEeqnarray*}
To flush out the final algorithm for the Hessian lets consider each summand in the last 
equality individually. The first term involving $\frac{\partial^2 X}{\partial x \partial y}$
can be calculated using the non-commutative perturbation algorithm developed in the 
preceding section
\begin{IEEEeqnarray*}{rCl}
	\left[\sum_{n=0}^{\infty} \frac{1}{\left(n+1\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\frac{\partial^2 X}{\partial x \partial y} \right)
		& = & \left[\sum_{n=0}^{\infty} \frac{1}{\left(n+2\right)!} \operatorname{ad}_X^n \cdotp \right] \left(\operatorname{ad}_X \frac{\partial^2 X}{\partial x \partial y} \right) + \frac{\partial^2 X}{\partial x \partial y}
\end{IEEEeqnarray*}
The next two summands together are the Poisson bracket of the non-commutative perturbations
of the gradients $\frac{\partial e^X}{\partial x}$, and $\frac{\partial e^X}{\partial y}$. 
The final summand is the bilinear non-commutative perturbation. Taken together the algorithm 
for the Hessian is then a sequence of calls to the non-commutative perturbation and the 
bilinear non-commutative perturbation; as outlined in algorithm \ref{alg:hessian}.

As in the previous section, the algorithm we have developed for the Hessian of the matrix
exponential is merely a starting point for further optimizations. In particular the bilinear
non-commutative perturbation needs attention to see if the Taylor series is susceptible to
further optimizations. As well, the Pad\'{e} approximation in the bilinear non-commutative 
perturbation needs further refinement. Nevertheless both of the algorithms for the 
non-commutative perturbation and the bilinear non-commutative perturbation are stable with 
respect to the the stochastic contraction Lie algebra $\mathfrak{st}^{+}(\hat{\mathbbm{1}})$; 
in the sense that if $X, \frac{\partial X}{\partial x}, \frac{\partial X}{\partial y}, \frac{\partial^2 X}{\partial x \partial y} \in \mathfrak{st}^{+}(\hat{\mathbbm{1}})$,
then the result of the algorithms will be in $\mathfrak{st}^{+}(\hat{\mathbbm{1}})$.
\clearpage
\section{Figures and Illustrations}
\begin{longtable}{r r r}
	\caption{Pad\'{e} Approximation of $\sum_{n=0}^\infty \frac{1}{\left(n+2\right)!} x^n$}
	\label{tab:perturbation}\\
	\multicolumn{1}{l}{Degree} & \multicolumn{1}{l}{Numerator} & \multicolumn{1}{l}{Denominator}\\
	\hline
	\endfirsthead
	\caption*{Continued from previous page.}\\
	\multicolumn{1}{l}{Degree} & \multicolumn{1}{l}{Numerator} & \multicolumn{1}{l}{Denominator}\\
	\hline
	\endhead
	\caption*{Continued on next page.}
	\endfoot
	\caption*{The exact rational coefficients of the numerator and denominator polynomials of the $\left[ 11/12 \right]_f\left(x\right)$ Pad\'{e} approximation of $f\left(x\right)=\sum_{n=0}^\infty \frac{1}{\left(n+2\right)!} x^n$; symbolically computed.}
	\endlastfoot
	$x^{0}$ & $\frac{1}{2}$ & $\frac{1}{1}$\\
	$x^{1}$ & $\frac{-11}{150}$ & $\frac{-12}{25}$\\
	$x^{2}$ & $\frac{1}{60}$ & $\frac{11}{100}$\\
	$x^{3}$ & $\frac{-3}{2300}$ & $\frac{-11}{690}$\\
	$x^{4}$ & $\frac{3}{23000}$ & $\frac{3}{1840}$\\
	$x^{5}$ & $\frac{-1}{161000}$ & $\frac{-1}{8050}$\\
	$x^{6}$ & $\frac{1}{2898000}$ & $\frac{1}{138000}$\\
	$x^{7}$ & $\frac{-1}{99111600}$ & $\frac{-1}{3059000}$\\
	$x^{8}$ & $\frac{1}{3171571200}$ & $\frac{1}{88099200}$\\
	$x^{9}$ & $\frac{-1}{197694604800}$ & $\frac{-1}{3369794400}$\\
	$x^{10}$ & $\frac{1}{13838622336000}$ & $\frac{1}{179722368000}$\\
	$x^{11}$ & $\frac{-1}{2698531355520000}$ & $\frac{-1}{14827095360000}$\\
	$x^{12}$ & & $\frac{1}{2490952020480000}$
\end{longtable}
\begin{algorithm}[!ht]
	\caption[Perturbation of $\frac{\partial e^X}{\partial x}$]{Numerical calculation of the perturbation of the first partial derivative of the matrix exponential, $\frac{\partial e^X}{\partial x}$, using the $\left[ 13/14 \right]_f\left(x\right)$ Pad\'{e} approximation of $f\left(x\right)=\sum_{n=0}^\infty \frac{1}{\left(n+2\right)!} x^n$}
	\label{alg:perturbation}
	\begin{algorithmic}[1]
		\Function{FRSTPRTB}{$X$, $\frac{\partial X}{\partial x}$}
			\State $A_x \gets \left[X, \frac{\partial X}{\partial x}\right]$\Comment{Allocates memory}
			\If{$A_x = 0$}
				\State \Return $\frac{\partial X}{\partial x}$
			\EndIf
			\State $A_X \gets I \otimes X - X^\dagger \otimes I$\Comment{if $X$ is $n \times n$ the result is $n^2 \times n^2$}
			\State $\vec{a_x} \gets \operatorname{VEC}\left(A_x\right)$\Comment{Change of indexing}
			\State $P \gets P_{10}\left(A_X\right)$\Comment{Pad\'{e} numerator by recursive summing and squaring}
			\State $Q \gets Q_{11}\left(A_X\right)$\Comment{Pad\'{e} denominator by recursive summing and squaring}
			\State Solve for $\vec{r}$: $P \vec{a_x} = Q \vec{r}$\Comment{Call to linear solver}
			\State \Return $\frac{\partial X}{\partial x} + \operatorname{MAT}\left(\vec{r}\right)$
		\EndFunction
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}[!ht]
	\caption[Gradient $\frac{\partial e^X}{\partial x}$]{Numerical calculation of the gradient of the matrix exponential, $\frac{\partial e^X}{\partial x}$.}
	\label{alg:gradient}
	\begin{algorithmic}[1]
		\Function{GRAD}{$X$, $\frac{\partial X}{\partial x}$}
			\State \Return $\operatorname{FRSTPRTB}\left(X, \frac{\partial X}{\partial x}\right) \operatorname{EXPM}\left(X\right)$
		\EndFunction
	\end{algorithmic}
\end{algorithm}
\begin{longtable}{r r r}
	\caption{Pad\'{e} Approximation of $\sum_{n=0}^\infty \frac{n+1}{\left(n+3\right)!} x^n$}
	\label{tab:bilinear}\\
	\multicolumn{1}{l}{Degree} & \multicolumn{1}{l}{Numerator} & \multicolumn{1}{l}{Denominator}\\
	\hline
	\endfirsthead
	\caption*{Continued from previous page.}\\
	\multicolumn{1}{l}{Degree} & \multicolumn{1}{l}{Numerator} & \multicolumn{1}{l}{Denominator}\\
	\hline
	\endhead
	\caption*{Continued on next page.}
	\endfoot
	\caption*{The exact rational coefficients of the numerator and denominator polynomials of the $\left[ 10/12 \right]_f\left(x\right)$ Pad\'{e} approximation of $f\left(x\right)=\sum_{n=0}^\infty \frac{n+1}{\left(n+3\right)!} x^n$; symbolically computed.}
	\endlastfoot
	$x^{0}$ & $\frac{1}{6}$ & $\frac{1}{1}$\\
	$x^{1}$ & $\frac{-987845443}{6830210462460}$ & $\frac{-285086025324}{569184205205}$\\
	$x^{2}$ & $\frac{74244818289}{22767368208200}$ & $\frac{3}{25}$\\
	$x^{3}$ & $\frac{-2885331297}{1047298937577200}$ & $\frac{-520403722879}{28562698297560}$\\
	$x^{4}$ & $\frac{180722213871}{10472989375772000}$ & $\frac{204914134377}{104729893757720}$\\
	$x^{5}$ & $\frac{-1993504343}{146621851260808000}$ & $\frac{-209171411061}{1332925920552800}$\\
	$x^{6}$ & $\frac{149844175309}{4750547980850179200}$ & $\frac{121443253013}{12567587250926400}$\\
	$x^{7}$ & $\frac{-347356579}{16410983933846073600}$ & $\frac{-52489277253}{113706741794096000}$\\
	$x^{8}$ & $\frac{1087980227}{60173607757435603200}$ & $\frac{37959419913}{2228652139164281600}$\\
	$x^{9}$ & $\frac{-1264009477}{157534505108966409177600}$ & $\frac{-416545133359}{876815427322633075200}$\\
	$x^{10}$ & $\frac{7919056259}{5119871416041408298272000}$ & $\frac{5918346737}{619970504167518336000}$\\
	$x^{11}$ & & $\frac{-2550355813}{20459026637528105088000}$\\
	$x^{12}$ & & $\frac{228053622637}{283562109196139536519680000}$
\end{longtable}
\begin{algorithm}[!ht]
	\caption[Linear perturbation of $\frac{\partial^2 e^X}{\partial x \partial y}$]{Numerical calculation of the linear perturbation of $\frac{\partial^2 e^X}{\partial x \partial y}$, using the $\left[ 12/14 \right]_f\left(x\right)$ Pad\'{e} approximation of $f\left(x\right)=\sum_{n=0}^\infty \frac{n+1}{\left(n+3\right)!} x^n$.}
	\label{alg:linear}
	\begin{algorithmic}[1]
		\Function{SCNDLINR}{$X$, $\frac{\partial X}{\partial x}$, $\frac{\partial X}{\partial y}$}
			\State $A_y \gets \left[X, \frac{\partial X}{\partial y}\right]$\Comment{Allocates memory}
			\State $A_X \gets I \otimes X - X^\dagger \otimes I$\Comment{if $X$ is $n \times n$ the result is $n^2 \times n^2$}
			\State $\vec{a_y} \gets \operatorname{VEC}\left(A_y\right)$\Comment{Change of indexing}
			\State $P \gets P_{10}\left(A_X\right)$\Comment{Pad\'{e} numerator by recursive summing and squaring}
			\State $Q \gets Q_{12}\left(A_X\right)$\Comment{Pad\'{e} denominator by recursive summing and squaring}
			\State Solve for $\vec{r}$: $P \vec{a_x} = Q \vec{r}$\Comment{Call to linear solver}
			\State \Return $\left[\frac{\partial X}{\partial x}, \operatorname{MAT}\left(\vec{r}\right) A_y\right]$
		\EndFunction
	\end{algorithmic}
\end{algorithm}
\begin{figure}[!ht]
	\centering
	\begin{tikzpicture}
		\node[text=gray] at (   0,   0) {$  0$};

		\node[text=gray] at (-1/2,  -1) {$  1$};
		\node[text=gray] at ( 1/2,  -1) {$ -1$};

		\node[text=gray] at (  -1,  -2) {$  1$};
		\node[font=\bf ] at (   0,  -2) {$  0$};
		\node[text=gray] at (   1,  -2) {$ -1$};

		\node[text=gray] at (-3/2,  -3) {$  1$};
		\node[font=\bf ] at (-1/2,  -3) {$  1$};
		\node[font=\bf ] at ( 1/2,  -3) {$ -1$};
		\node[text=gray] at ( 3/2,  -3) {$ -1$};

		\node[text=gray] at (  -2,  -4) {$  1$};
		\node[font=\bf ] at (  -1,  -4) {$  2$};
		\node[font=\bf ] at (   0,  -4) {$  0$};
		\node[font=\bf ] at (   1,  -4) {$ -2$};
		\node[text=gray] at (   2,  -4) {$ -1$};

		\node[text=gray] at (-5/2,  -5) {$  1$};
		\node[font=\bf ] at (-3/2,  -5) {$  3$};
		\node[font=\bf ] at (-1/2,  -5) {$  2$};
		\node[font=\bf ] at ( 1/2,  -5) {$ -2$};
		\node[font=\bf ] at ( 3/2,  -5) {$ -3$};
		\node[text=gray] at ( 5/2,  -5) {$ -1$};

		\node[text=gray] at (  -3,  -6) {$  1$};
		\node[font=\bf ] at (  -2,  -6) {$  4$};
		\node[font=\bf ] at (  -1,  -6) {$  5$};
		\node[font=\bf ] at (   0,  -6) {$  0$};
		\node[font=\bf ] at (   1,  -6) {$ -5$};
		\node[font=\bf ] at (   2,  -6) {$ -4$};
		\node[text=gray] at (   3,  -6) {$ -1$};

		\node[text=gray] at (-7/2,  -7) {$  1$};
		\node[font=\bf ] at (-5/2,  -7) {$  5$};
		\node[font=\bf ] at (-3/2,  -7) {$  9$};
		\node[font=\bf ] at (-1/2,  -7) {$  5$};
		\node[font=\bf ] at ( 1/2,  -7) {$ -5$};
		\node[font=\bf ] at ( 3/2,  -7) {$ -9$};
		\node[font=\bf ] at ( 5/2,  -7) {$ -5$};
		\node[text=gray] at ( 7/2,  -7) {$ -1$};

		\node[text=gray] at (  -4,  -8) {$  1$};
		\node[font=\bf ] at (  -3,  -8) {$  6$};
		\node[font=\bf ] at (  -2,  -8) {$ 14$};
		\node[font=\bf ] at (  -1,  -8) {$ 14$};
		\node[font=\bf ] at (   0,  -8) {$  0$};
		\node[font=\bf ] at (   1,  -8) {$-14$};
		\node[font=\bf ] at (   2,  -8) {$-14$};
		\node[font=\bf ] at (   3,  -8) {$ -6$};
		\node[text=gray] at (   4,  -8) {$ -1$};

		\node[text=gray] at (-9/2,  -9) {$  1$};
		\node[font=\bf ] at (-7/2,  -9) {$  7$};
		\node[font=\bf ] at (-5/2,  -9) {$ 20$};
		\node[font=\bf ] at (-3/2,  -9) {$ 28$};
		\node[font=\bf ] at (-1/2,  -9) {$ 14$};
		\node[font=\bf ] at ( 1/2,  -9) {$-14$};
		\node[font=\bf ] at ( 3/2,  -9) {$-28$};
		\node[font=\bf ] at ( 5/2,  -9) {$-20$};
		\node[font=\bf ] at ( 7/2,  -9) {$ -7$};
		\node[text=gray] at ( 9/2,  -9) {$ -1$};

		\node[text=gray] at (  -5, -10) {$\left[\frac{\partial X}{\partial x},\operatorname{ad}_X^{n+1} \frac{\partial X}{\partial y}\right]$};
		\node[text=gray] at (   5, -10) {$-\left[\operatorname{ad}_X^{n+1} \frac{\partial X}{\partial x},\frac{\partial X}{\partial y}\right]$};
	\end{tikzpicture}
	\caption[Pascal's triangle divergence]{Illustration of the calculation of the first 8 rows of coefficients of the divergence of Pascal's triangle; emphasizing the coefficients in $F_n$.}
	\label{fig:pascaldivergence}
\end{figure}
\begin{algorithm}[!ht]
	\caption[Bilinear perturbation of $\frac{\partial^2 e^X}{\partial x \partial y}$]{Numerical calculation of the bilinear perturbation of the $\frac{\partial^2 e^X}{\partial x \partial y}$, using the divergence of Pascal's triangle.}
	\label{alg:bilinear}
	\begin{algorithmic}[1]
		\Function{SCNDBILN}{$X$, $\frac{\partial X}{\partial x}$, $\frac{\partial X}{\partial y}$, $\epsilon = \text{machine float precision}$}
			\State $A_x \gets \left[X,\frac{\partial X}{\partial x}\right]$\Comment{Allocates memory}
			\State $A_y \gets \left[X,\frac{\partial X}{\partial y}\right]$\Comment{Allocates memory}
			\State $R \gets 0$\Comment{Allocates memory}
			\State $F \gets \left[\frac{\partial X}{\partial x}, A_y\right] + \left[\frac{\partial X}{\partial y},A_x\right]$\Comment{Allocates memory}
			\State $m \gets 3$\Comment{Factorial scalars}
			\State $n \gets 6$\Comment{Factorial scalars}
			\While{$ \left\| F \right\| > n\epsilon$}
				\State $R \gets R + \frac{F}{n}$\Comment{In place computation}
				\State $A_x \gets \left[X,A_x\right]$\Comment{In place computation}
				\State $A_y \gets \left[X,A_y\right]$\Comment{In place computation}
				\State $F \gets \left[\frac{\partial X}{\partial x}, A_y\right] + \left[X,F\right] + \left[\frac{\partial X}{\partial y},A_x\right]$\Comment{In place computation}
				\State $m \gets m+1$\Comment{Increment factorial}
				\State $n \gets m n$\Comment{Increment factorial}
			\EndWhile
			\State \Return $R$
		\EndFunction
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}[!ht]
	\caption[Perturbation of $\frac{\partial^2 e^X}{\partial x \partial y}$]{Numerical calculation of the perturbation of $\frac{\partial^2 e^X}{\partial x \partial y}$.}
	\label{alg:second}
	\begin{algorithmic}[1]
		\Function{SCNDPRTB}{$X$, $\frac{\partial X}{\partial x}$, $\frac{\partial X}{\partial y}$, $\frac{\partial^2 X}{\partial x \partial y}$}
			\If{$\left[X, \frac{\partial X}{\partial x}\right] = 0$ \textbf{and} $\left[X, \frac{\partial X}{\partial y}\right] = 0$}
				\State $B \gets 0$\Comment{No other perturbations}
			\ElsIf{$\left[X, \frac{\partial X}{\partial x}\right] = 0$}
				\State $B \gets \operatorname{SCNDLINR}\left(X, \frac{\partial X}{\partial x}, \frac{\partial X}{\partial y}\right)$\Comment{Linear second perturbation}
			\ElsIf{$\left[X, \frac{\partial X}{\partial y}\right] = 0$}
				\State $B \gets \operatorname{SCNDLINR}\left(X, \frac{\partial X}{\partial y}, \frac{\partial X}{\partial x}\right)$\Comment{Linear second perturbation}
			\Else
				\State $B \gets \operatorname{SCNDBILN}\left(X, \frac{\partial X}{\partial x}, \frac{\partial X}{\partial y}\right)$\Comment{Bilinear second perturbation}
			\EndIf
			\State $P_x \gets \operatorname{FIRSTPRTB}\left(X,\frac{\partial X}{\partial x}\right)$\Comment{Call to first perturbation}
			\State $P_y \gets \operatorname{FIRSTPRTB}\left(X,\frac{\partial X}{\partial y}\right)$\Comment{Call to first perturbation}
			\State \Return $\operatorname{FIRSTPRTB}\left(X,\frac{\partial^2 X}{\partial x \partial y}\right) + \frac{1}{2} B +  \frac{1}{2}\left(P_x P_y + P_y P_x\right)$
		\EndFunction
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}[!ht]
	\caption[Hessian of $e^X$]{Numerical calculation of the Hessian of the matrix exponential, $\frac{\partial^2 e^X}{\partial x \partial y}$.}
	\label{alg:hessian}
	\begin{algorithmic}[1]
		\Function{HESS}{$X$, $\frac{\partial X}{\partial x}$, $\frac{\partial X}{\partial y}$, $\frac{\partial^2 X}{\partial x \partial y}$}
			\State \Return $\operatorname{SCNDPRTB}\left(X, \frac{\partial X}{\partial x}, \frac{\partial X}{\partial y}, \frac{\partial^2 X}{\partial x \partial y}\right) \operatorname{EXPM}\left(X\right)$
		\EndFunction
	\end{algorithmic}
\end{algorithm}