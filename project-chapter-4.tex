\chapter{Maximum Likelihood Estimation from First Hitting Times}
\section{Distribution of First Hitting Times}
Contemporary methods for fitting time homogeneous Markov processes on a finite 
state space require directly parameterizing the transition probability matrix $\mathbb{P}\left[X_n = j \left\|X_0 = i \right.\right] = \hat{e}_i P^n \hat{e}_j$,
as they depend on realizing the process through discrete time steps $n$. While this 
formulation has many powerful applications, there are analyses where the parameterization of 
the generator of the time homogeneous Markov process, $\mathbb{P}\left[X_t = j \left\|X_0 = i \right.\right] = \hat{e}_i \exp\left({tG}\right) \hat{e}_j$, 
is of greater meaning, or importance. In particular when the observed process is, at least,
in principle continuous, or when the desired parameterization is in units of rates per time
parameterization of the generator is the more natural choice.

The natural experimental design for continuous time homogeneous Markov process on a finite
state space is the observation of a stopped process, such as the first hitting time to a
state statistic, or the first exit time from a state statistic. Surprisingly it is possible
to explicitly formulate the distribution of these two statistics in terms of the generator
of the process.

To start, recall the definition of the projection operator on a finite dimensional vector
space $P_i = \hat{e}_i \otimes \hat{e}_i$; which projects each vector in the space onto a
fixed unit vector $\hat{e}_i$. Projection operators hold a special purpose in analyzing
continuous time homogeneous Markov processes on a finite state spaces. The projection
operator $I-P_k$, when left multiplied to the generator $G = \sum_{ij}x_{ij}C_{ij}$, yields 
a new process $\left(I-P_k\right)G$, where state $k$ is an absorbing state.

The project operators $P_i$ are useful in formulating the distribution of first hitting 
times of a process generated by $G$ in terms of the transition probabilities of a process
generated by $\left(I-P_i\right)G$. We can apply this to restate the first hitting time 
results in the exercises of Rogers and Williams \cite{rogers_diffusions_2000}.
\begin{theorem}
	If $T_j = \inf\left\lbrace t: X_t=j\right\rbrace$ is the first 
	hitting time statistic of the transition to $j$ of a process generated by $G = \sum_{ij}x_{ij}C_{ij}$
	then
	\begin{IEEEeqnarray*}{rCl}
		\mathbb{P}_G\left[T_j \le t \left\| X_0=i \right.\right]
			& = & \hat{e}_i \exp\left(t\left(I - P_j\right) G\right) \hat{e}_j
	\end{IEEEeqnarray*}
\end{theorem}

\begin{IEEEproof}
	Let $\mathscr{F}_t$ be the standard filtration on $X_t$ a continuous time homogeneous 
	Markov process on a finite state space.
	\begin{enumerate}
		\item Expand the probability $\mathbb{P}_G\left[T_j \le t \left\| X_0=i \right.\right]$
		as an expectation over conditional outcomes of $X_{s_1}=j$ and $X_{s_2} \ne j$ for $s_1 \le s_2$
		\begin{IEEEeqnarray*}{rCl}
				\mathbb{P}_G\left[T_j \le t \left\| X_0=i \right.\right]
					& = & \mathbb{E}_G\left[ \mathbb{P}_G\left[X_{s_1}=j,X_{s_2} \enskip s_1 \le t \wedge s_2 \left\| X_0=i \right.\right] \left\| \right. \mathscr{F}_{s_2} \right]\\
					& = & \mathbb{E}_G\left[\mathbb{P}_G\left[X_{s_2} \enskip s_1 \le s_2 \left\| X_{s_1}=j \right.\right] \mathbb{P}_G\left[X_{s_1} = j \enskip s_1 \le t \left\|X_0=i \right.\right]  \left\| \right. \mathscr{F}_{s_2} \right]
		\end{IEEEeqnarray*}
		\item It follows that
		\begin{IEEEeqnarray*}{+rCl+x*}
			\mathbb{P}_G\left[T_j \le t \left\| X_0=i \right.\right]
				& = & \mathbb{P}_G\left[X_s: \exists u \le t \text{ where } \forall v \ge u \enskip X_v = j \left\| X_0=i \right. \right]\\
				& = & \mathbb{P}_{\left(I-P_j\right)G}\left[X_s: \exists u \le t \text{ where } \forall v \ge u \enskip X_v = j \left\| X_0=i \right. \right]\\
				& = & \mathbb{P}_{\left(I-P_j\right)G}\left[X_t = j \left\| X_0=i \right. \right]\\
				& = & \hat{e}_i \exp\left(t\left(I - P_j\right) G\right) \hat{e}_j & \IEEEQEDhere
		\end{IEEEeqnarray*}
	\end{enumerate}
\end{IEEEproof}

This result generalizes in the obvious manner; where if we have a set of first hitting 
states $J$ then the transition probabilities of the process $\left(I - P_J\right) G$ gives
the cumulative distributions of the hitting times; where $P_J = P_{j_1} + P_{j_2} + \cdots$.
This implies that if we can design our experiment to observe as many of the first hitting 
times as possible we will greatly simplify our statistical estimators.

In light of this, we can reformulate the standard textbook result, for example in Buchholz 
et. al \cite{buchholz_input_2014}, of first hitting time statistics in the context of the
stochastic contraction Lie algebra $\mathfrak{st}^{+}(\hat{\mathbbm{1}})$. As is standard we 
start with an the experiment designed to observe the first exit time $T_{i \rightarrow j}$ 
from $\hat{e_i}$ to every other state $\hat{e}_j$, where $i \ne j$. Assuming the process is 
generated by $G = \sum_{ij}x_{ij}C_{ij}$, and keeping $i \ne j$ fixed, the density of the 
conditional distribution of $T_{i \rightarrow j}$ is
\begin{IEEEeqnarray*}{rCl}
	p\left(X_t=j \left\| X_0=i \right.\right)
		& = & \frac{d}{dt} \mathbb{P}\left[X_T=j, T\le t \left\| X_0=i \right.\right]\\
		& = & \frac{d}{dt} \hat{e_i} \exp\left(tP_iG\right) \hat{e}_j\\
		& = & \hat{e_i} P_iG \exp\left(tP_iG\right) \hat{e}_j\\
		& = & \hat{e_i} \left(\sum_{l \ne i} x_{il} C_{il}\right) \exp\left(t\sum_{l \ne i} x_{il} C_{il}\right) \hat{e}_j\\
		& = & \left(\sum_{l \ne i} x_{il}\left(\hat{e}_l  - \hat{e}_i\right) \right) \left(- e^{-tx_{ij}}\right) \hat{e}_i\\
		& = & x_{ij} e^{-tx_{ij}}
\end{IEEEeqnarray*}

Intuitively if we design our experiment to observer the durations $t_n = T_{i \rightarrow j}$
between $N_{ij}$ replicated transitions $i \rightarrow j$, the maximum likelihood estimate of
each rate $x_{ij}$ is then the simple average
\begin{IEEEeqnarray*}{rCl}
	\tilde{x}_{ij}
		& = & \frac{N_{ij}}{\sum_{n=1}^{N_{ij}} t_n}
\end{IEEEeqnarray*}

However for experiments that involve opportunistic sampling, surveys, or population
monitoring it is generally not possible to observe every distinct transition. Typically the
initial state of the transition is known or can be inferred, but only a subset of exit
states are observed. In this situation the projection operator $P_i$ onto a single
dimensional subspace is replaced with a projection $I - P_A = P_{i_1} + P_{i_2} + \cdots$ 
onto a multidimensional subspace; where $P_A$ is the projection onto the observed absorbing 
states in set $A$.



%With total knowledge of all first hits p_ij=N_ij/T_ij
%Prove its a stopped process where we take the product with the projection onto 
%all the other transitative states. Essentially zero out the rows of the states
%we observe first hitting on, turn them into absorbing states
% Introduce simple linear parameterization of the vector space of the algebra
\section{The Likelihood and Its Maximization}
%Long formula basically
\section{Newton-Raphson Maximization}
\section{Figures and Illustrations}