\chapter{Maximum Likelihood Estimation from First Hitting Times}
\section{Distribution of First Hitting Times}
Contemporary methods for fitting time homogeneous Markov processes on a finite 
state space require directly parameterizing the transition probability matrix $\mathbb{P}\left[X_n = j \left\|X_0 = i \right.\right] = \left\langle \hat{e}_i, P^n \hat{e}_j \right\rangle$,
as they depend on realizing the process through discrete time steps $n$. While this 
formulation has many powerful applications, there are analyses where the parameterization of 
the generator of the time homogeneous Markov process, $\mathbb{P}\left[X_t = j \left\|X_0 = i \right.\right] = \left\langle \hat{e}_i, \exp\left({tG}\right) \hat{e}_j \right\rangle$, 
is of greater meaning, or importance. In particular when the observed process is, at least
in principle continuous, or when the desired parameterization is in units of rates per time
parameterization of the generator is the more natural choice.

The natural experimental design for continuous time homogeneous Markov process on a finite
state space is the observation of a stopped process, such as the statistic of first hitting 
time of a state, or the first exit time from a state statistic. Surprisingly it is possible
to explicitly formulate the distribution of these two statistics in terms of the generator
of the process.

To start, recall the definition of the projection operator on a finite dimensional vector
space $P_i = \hat{e}_i \otimes \hat{e}_i$; which projects each vector in the space onto a
fixed unit vector $\hat{e}_i$. Projection operators hold a special purpose in analyzing
continuous time homogeneous Markov processes on a finite state spaces. The projection
operator $I-P_k$, when left multiplied to the generator $G = \sum_{ij}x_{ij}C_{ij}$, yields 
a new process $\left(I-P_k\right)G$, where state $k$ is an absorbing state.

The project operators $P_i$ are useful in formulating the distribution of first hitting 
times of a process generated by $G$ in terms of the transition probabilities of a process
generated by $\left(I-P_i\right)G$. We can apply this to restate the first hitting time 
results in the exercises of Rogers and Williams \cite{rogers_diffusions_2000}.
\begin{theorem}
	If $T_j = \inf\left\lbrace t: X_t=j\right\rbrace$ is the first 
	hitting time statistic of the transition to $j$ of a process generated by $G = \sum_{ij}x_{ij}C_{ij}$
	then
	\begin{IEEEeqnarray*}{rCl}
		\mathbb{P}_G\left[T_j \le t \left\| X_0=i \right.\right]
			& = & 1 - \left\langle \hat{e}_i, \exp\left(t\left(I - P_j\right) G\right) \hat{e}_j \right\rangle
	\end{IEEEeqnarray*}
\end{theorem}
\begin{IEEEproof}
	The proof hinges on formalizing the intuition that once we know a continuous time 
	homogeneous Markov process on a finite state space $X_t$ has first touched the state $j$ 
	then we need no further information, and so we can work with the simpler process where $j$
	is an absorbing state.
	\begin{IEEEeqnarray*}{+rCl+x*}
		\mathbb{P}_G\left[T_j \le t \left\| X_0=i \right.\right]
			& = & 1 - \mathbb{P}_G\left[T_j > t \left\| X_0=i \right.\right]\\
			& = & 1 - \mathbb{P}_G\left[\forall s \le t \enskip X_s \ne j, \enskip \exists u > t \enskip X_u=j \left\| X_0=i \right.\right]\\
			& = & 1 - \mathbb{P}_G\left[\forall s \le t \enskip X_s \ne j \left\| X_0=i \right.\right]\mathbb{P}_G\left[\exists u > t \enskip X_u=j \left\| \forall s \le t \enskip X_s \ne j \right.\right]\\
			& = & 1 - \mathbb{P}_G\left[\forall s \le t \enskip X_s \ne j \left\| X_0=i \right.\right]\mathbb{P}_G\left[X_u = j, u > t \left\| X_t \ne j \right.\right]\\
			& = & 1 - \mathbb{P}_{\left(I - P_j\right)G}\left[\forall s \le t \enskip X_s \ne j \left\| X_0=i \right.\right]\mathbb{P}_{\left(I - P_j\right)G}\left[X_u = j, \enskip u > t \left\| X_t \ne j \right.\right]\\
			& = & 1 - \mathbb{P}_{\left(I - P_j\right)G}\left[\forall s \le t \enskip X_s \ne j, \enskip X_u = j, \enskip u > t  \left\| X_0=i \right.\right]\\
			& = & 1 - \mathbb{P}_{\left(I - P_j\right)G}\left[X_t = j \left\| X_0=i \right.\right]\\
			& = & 1 - \left\langle \hat{e}_i, \exp\left(t\left(I - P_j\right) G\right) \hat{e}_j \right\rangle & \IEEEQEDhere
	\end{IEEEeqnarray*}
\end{IEEEproof}
This result generalizes in the obvious manner; where if we have a set of first hitting 
states $J$ then the transition probabilities of the process $\left(I - P_J\right) G$ gives
the cumulative distributions of the hitting times; where $P_J = P_{j_1} + P_{j_2} + \cdots$.
This implies that if we can design our experiment to observe as many of the first hitting 
times as possible we will greatly simplify our statistical estimators.

In light of this, we can reformulate the standard textbook result, for example in Buchholz 
et. al \cite{buchholz_input_2014}, of first hitting time statistics in the context of the
stochastic contraction Lie algebra $\mathfrak{st}^{+}(\hat{\mathbbm{1}})$. As is standard we 
start with an the experiment designed to observe the first exit time $T_{i \rightarrow j}$ 
from $\hat{e_i}$ to every other state $\hat{e}_j$, where $i \ne j$. Assuming the process is 
generated by $G = \sum_{ij}x_{ij}C_{ij}$, and keeping $i \ne j$ fixed, the density of the 
distribution of $T_{i \rightarrow j}$ is
\begin{IEEEeqnarray*}{rCl}
	p_G\left(T_{i \rightarrow j}=t \left\| X_0=i \right.\right)
		& = & \frac{d}{dt} \mathbb{P}_G\left[ T_j\le t \left\| X_0=i \right.\right]\\
		& = & \frac{d}{dt} \mathbb{P}_{P_i G}\left[ T_j\le t \left\| X_0=i \right.\right]\\
		& = & \frac{d}{dt} \left(1 - \left\langle \hat{e}_i, \exp\left(tP_iG\right) \hat{e}_j\right) \right\rangle\\
		& = & \frac{d}{dt} \left(1 - \left\langle \hat{e}_i, \exp\left(t\sum_{l \ne i} x_{il} C_{il}\right) \hat{e}_j\right) \right\rangle\\
		& = & \frac{d}{dt} \left(1 - \left\langle \hat{e}_i, e^{-tx_{ij}} \hat{e}_i\right) \right\rangle\\
		& = & \frac{d}{dt} \left(1 - e^{-tx_{ij}}\right)\\
		& = & x_{ij} e^{-tx_{ij}}
\end{IEEEeqnarray*}
Intuitively if we design our experiment to observer the durations $t_n = T_{i \rightarrow j}$
between $N_{ij}$ replicated transitions $i \rightarrow j$, the maximum likelihood estimate of
each rate $x_{ij}$ is then the simple average
\begin{IEEEeqnarray*}{rCl}
	\tilde{x}_{ij}
		& = & \frac{N_{ij}}{\sum_{n=1}^{N_{ij}} t_n}
\end{IEEEeqnarray*}
However for experiments that involve opportunistic sampling, surveys, or population
monitoring it is generally not possible to observe every distinct transition. Typically the
initial state of the transition is known or can be inferred, but only a subset of exit
states are observed. In this situation the projection operator $P_i$ onto a single
dimensional subspace is replaced with a projection $I - P_A = P_{i_1} + P_{i_2} + \cdots$ 
onto a multidimensional subspace; where $P_A$ is the projection onto the observed absorbing 
states in set $A$.
\section{The Likelihood and Its Maximization}
With a method to derive the density in hand we can proceed to formulate the log-likelihood
of the first hitting times. To do so we must carefully formulate the experimental design to 
which the log-likelihood will apply. Rather than attempt to formulate the most general
likelihood model possible, which would be notationally laborious given the infinite
permutations and combinations of models available, we will illustrate the formulation of
the likelihood through a specific application to an aging process.

An aging process is a continuous time homogeneous finite birth death-process, where all the
sequential transitions between states are reversible except for transitions to the final
state, which is an absorbing state representing death. In the context first hitting time
statistics, a finite subset of the states act as sentinel states, where the first hitting
time statistic for the transition between any pair of, possible non-adjacent, sentinel 
states is observed. An example of this process is illustrated in figure \ref{fig:agingprocess},
which displays a seven state aging process, with three sentinel states. The transitions
between states that are not sentinel are not directly observed; but rather acts as a type
of memory register that broadens the centrality of the distribution of first hitting times.

Given an $U$ state aging process, the generator takes on the simple sequential form:
\begin{IEEEeqnarray*}{rCl}
	G 
		& = & \sum_{i=1}^{U-1} x_{i\left(i+1\right)}C_{i\left(i+1\right)} 
			+ \sum_{i=2}^{U-1} x_{i\left(i-1\right)}C_{i\left(i-1\right)} \\
		& = & \sum_{i=1}^{U-1} x_{i\left(i+1\right)}\left(\hat{e}_{i} \otimes \hat{e}_{i+1} - \hat{e}_i \otimes \hat{e}_i\right)
			+ \sum_{i=2}^{U-1} x_{i\left(i-1\right)}\left(\hat{e}_{i} \otimes \hat{e}_{i-1} - \hat{e}_i \otimes \hat{e}_i\right)
\end{IEEEeqnarray*}
The $U$ state aging process as $2U-3$ unknown parameters, $x_{i\left( i+1\right)}$ for $1 \le i \le U-1$
and $x_{i\left( i-1\right)}$ for $2 \le i \le U-1$, to be fitted by likelihood maximization.
Of the $U$ states, a subset of $V \le U$ states are sentinel states, $1 \le i_1 < \cdots i_V \le U$,
for which we observe the first hitting time statistics for the transitions between the
sentinel states.

Generalizing the theorem in the previous section the generators $G_v^\pm$ for the observed first
hitting time statistics, $T_{i_v \rightarrow i_{v \pm 1}}$, are given by
\begin{IEEEeqnarray*}{rCl}
	G_v^\pm
		& = & \sum_{j=i_{v}}^{i_{v \pm 1} \mp 1} P_j G\\
		& = & \sum_{j=i_{v}}^{i_{v \pm 1} \mp 1} x_{j \left(j-1\right)}C_{j \left(j-1\right)} + x_{j \left(j+1\right)}C_{j \left(j+1\right)}
\end{IEEEeqnarray*}
The distribution density of $T_{i_v \rightarrow i_{v \pm 1}}$ can be concisely stated as
\begin{IEEEeqnarray*}{rCl}
	p_G\left(T_{i_v \rightarrow i_{v \pm 1}} = t\right)
		& = & \left\langle \hat{e}_{i_v}, G_v^\pm \exp\left(t G_v^\pm \right) \hat{e}_{i_{v \pm 1}} \right\rangle
\end{IEEEeqnarray*}
The next step is to formulate the log-likelihood. This requires establishing the observed
data. For each of the first hitting time statistics $T_{i_v \rightarrow i_{v \pm 1}}$ we 
observe $N_v^\pm$ replications of durations $t_v^{\pm\left(n\right)}$, enumerated by $1 \le n \le N_v^\pm$.
\begin{IEEEeqnarray*}{rCl}
	\Lambda
		& = & \sum_{v=1}^V \sum_{n=1}^{N_v^{+}} \ln p_G\left(T_{i_v \rightarrow i_{v + 1}} = t_v^{+\left(n\right)}\right)
			\:+ \sum_{v=1}^V \sum_{n=1}^{N_v^{-}} \ln p_G\left(T_{i_v \rightarrow i_{v - 1}} = t_v^{-\left(n\right)}\right)\\
		& = & \sum_{v=1}^V \sum_{n=1}^{N_v^{+}} \ln \left\langle \hat{e}_{i_v}, G_v^{+} \exp\left(t_v^{+\left(n\right)} G_v^{+} \right) \hat{e}_{i_{v + 1}} \right\rangle
			\:+ \sum_{v=1}^V \sum_{n=1}^{N_v^{-}} \ln \left\langle \hat{e}_{i_v}, G_v^{-} \exp\left(t_v^{-\left(n\right)} G_v^{-} \right) \hat{e}_{i_{v - 1}} \right\rangle\\
		& = & \sum_{v=1}^V \sum_{n=1}^{N_v^{+}} \ln \left\langle x_{i_v\left(i_v + 1\right)} \hat{e}_{i_v + 1} + x_{i_v\left(i_v - 1\right)} \hat{e}_{i_v - 1} - \left(x_{i_v\left(i_v + 1\right)} + x_{i_v\left(i_v - 1\right)}\right) \hat{e}_{i_v}, \exp\left(t_v^{+\left(n\right)} G_v^{+} \right) \hat{e}_{i_{v + 1}} \right\rangle\\
		&   & \:+ \sum_{v=1}^V \sum_{n=1}^{N_v^{+}} \ln \left\langle x_{i_v\left(i_v + 1\right)} \hat{e}_{i_v + 1} + x_{i_v\left(i_v - 1\right)} \hat{e}_{i_v - 1} - \left(x_{i_v\left(i_v + 1\right)} + x_{i_v\left(i_v - 1\right)}\right) \hat{e}_{i_v}, \exp\left(t_v^{-\left(n\right)} G_v^{-} \right) \hat{e}_{i_{v - 1}} \right\rangle
\end{IEEEeqnarray*}
Maximization requires differentiation by the $2U-3$ parameters $x_{k \Delta_k}$, with $\Delta _k = k \pm 1$.
By the linearity of the generator this will result in algebraically replacing the $x_{k \Delta_k}$
with indicator functions of the form $\mathbb{I}\left[i_v = k \right]$, and $\mathbb{I}\left[k \in i_v, \dots, i_{v \pm 1} \mp 1 \right]$, 
where $k$ is the index of partial differentiation. To simplify the notation of the indicator
functions let $\mathbb{I}_v^{+}\left[k, \Delta_k \right] = \mathbb{I}\left[i_v = k \right]\mathbb{I}\left[k < \Delta_k \right]$,
$\mathbb{I}_v^{-}\left[k, \Delta_k \right] = \mathbb{I}\left[i_v = k \right]\mathbb{I}\left[k > \Delta_k \right]$,
$\mathbb{I}_v^{+}\left[k\right] = \mathbb{I}\left[i_v \le k \le i_{v + 1} - 1 \right]$, 
and $\mathbb{I}_v^{-}\left[k\right] = \mathbb{I}\left[i_{v - 1} + 1 \le k \le i_v \right]$
\begin{IEEEeqnarray*}{rCl}
	\frac{\partial \Lambda}{\partial x_{k \Delta_k}}
		& = & \mathbb{I}_v^\pm \left[k, \Delta_k \right] \sum_{n=1}^{N_v^\pm} \frac{\left\langle \hat{e}_{\Delta_k} - \hat{e}_k , \exp\left(t_v^{\pm\left(n\right)} G_v^\pm \right) \right\rangle}
			{\left\langle \hat{e}_{i_v}, G_v^\pm \exp\left(t_v^{\pm\left(n\right)} G_v^\pm \right) \hat{e}_{i_{v + 1}} \right\rangle}\\[2ex]
		&   & \:+ \mathbb{I}_v^\pm \left[k\right] \sum_{n=1}^{N_v^\pm} \frac{\left\langle \hat{e}_{i_v}, G_v^\pm \frac{\partial \exp\left(t_v^{+\left(n\right)} G_v^{+} \right)}{\partial x_{k \Delta_k}} \hat{e}_{i_{v + 1}} \right\rangle}
			{\left\langle \hat{e}_{i_v}, G_v^\pm \exp\left(t_v^{\pm\left(n\right)} G_v^\pm \right) \hat{e}_{i_{v + 1}} \right\rangle}\\[2ex]
		& = & \mathbb{I}_v^\pm \left[k, \Delta_k \right] \sum_{n=1}^{N_v^\pm} \frac{\left\langle \hat{e}_{\Delta_k} - \hat{e}_k , \exp\left(t_v^{\pm\left(n\right)} G_v^\pm \right) \right\rangle}
			{\left\langle \hat{e}_{i_v}, G_v^\pm \exp\left(t_v^{\pm\left(n\right)} G_v^\pm \right) \hat{e}_{i_{v + 1}} \right\rangle}\\[2ex]
		&   & \:+ \mathbb{I}_v^\pm \left[k\right] \sum_{n=1}^{N_v^\pm}  t_v^{\pm\left(n\right)} \frac{\left\langle \hat{e}_{i_v}, G_v^\pm \operatorname{GEX}\left(t_v^{+\left(n\right)} G_v^{+}, C_{k \Delta_k} \right) \hat{e}_{i_{v + 1}} \right\rangle}
			{\left\langle \hat{e}_{i_v}, G_v^\pm \exp\left(t_v^{\pm\left(n\right)} G_v^\pm \right) \hat{e}_{i_{v + 1}} \right\rangle}
\end{IEEEeqnarray*}
Factoring the summands by both the transition between sentinel states and the observations
makes the algorithm to calculate the terms of the partial differential equation more
transparent \ref{alg:singlesummand}. For a given generator $G$ and differential parameter $x_{k \Delta_k}$
we first determine the canonical generator $C_{k \Delta_k}$ and sub-generator $G_v$.
After that one time computation, the data $t_v^{\pm \left(n\right)}$ is looped through to
produce the full partial derivative of the log-likelihood \ref{alg:completesum}. In the case
of calculating all the partial derivatives of the log-likelihood for large data sets it
is more efficient to loop through the data once, calculating all the partial derivatives at
each data point, and then producing the sum of the derivatives \ref{alg:allpartials}. 

While the gradient alone is sufficient for gradient decent searches for local maximum, 
maximization of the log-likelihood by the Newton-Raphson method requires calculation of the
Hessian of the log-likelihood. By necessity the Hessian of the log-likelihood will be 
complicated. However in this example we have two simplifications at our disposal. First, the
generator $G$ is linear in $x_{ij}$ and so the second derivatives vanish. Second, the partial
derivative by the transition rates $x_{k \Delta_k }$ and $x_{l \Delta _l }$ will only be 
non-trivial when the states fall between the same sentinel states.
\begin{IEEEeqnarray*}{rCl}
	\frac{\partial^2 \Lambda}{\partial x_{k \Delta_k} \partial x_{l \Delta_l}}
		& = & \mathbb{I}_v^\pm \left[k, \Delta_k \right] \mathbb{I}_v^\pm \left[l \right] \sum_{n=1}^{N_v^\pm} \frac{\left\langle \hat{e}_{\Delta_k} - \hat{e}_k , \frac{\partial \exp\left(t_v^{\pm\left(n\right)} G_v^\pm \right)}{\partial x_{l \Delta_l}} \right\rangle}
			{\left\langle \hat{e}_{i_v}, G_v^\pm \exp\left(t_v^{\pm\left(n\right)} G_v^\pm \right) \hat{e}_{i_{v + 1}} \right\rangle}\\[2ex]
		&   & \:+ \mathbb{I}_v^\pm \left[k\right] \mathbb{I}_v^\pm \left[l, \Delta_l \right] \sum_{n=1}^{N_v^\pm} \frac{\left\langle \hat{e}_{\Delta_l} - \hat{e}_l , \frac{\partial \exp\left(t_v^{\pm\left(n\right)} G_v^\pm \right)}{\partial x_{k \Delta_k}} \right\rangle}
			{\left\langle \hat{e}_{i_v}, G_v^\pm \exp\left(t_v^{\pm\left(n\right)} G_v^\pm \right) \hat{e}_{i_{v + 1}} \right\rangle}\\[2ex]
		&   & \:+ \mathbb{I}_v^\pm \left[k\right] \mathbb{I}_v^\pm \left[l\right] \sum_{n=1}^{N_v^\pm} \frac{\left\langle \hat{e}_{i_v}, G_v^\pm \frac{\partial^2 \exp\left(t_v^{+\left(n\right)} G_v^{+} \right)}{\partial x_{k \Delta_k} \partial x_{l \Delta_l}} \hat{e}_{i_{v + 1}} \right\rangle}
			{\left\langle \hat{e}_{i_v}, G_v^\pm \exp\left(t_v^{\pm\left(n\right)} G_v^\pm \right) \hat{e}_{i_{v + 1}} \right\rangle}\\[2ex]
		&   & \:- \mathbb{I}_v^\pm \left[k, \Delta_k \right] \mathbb{I}_v^\pm \left[l, \Delta_l \right] \sum_{n=1}^{N_v^\pm} \frac{\left\langle \hat{e}_{\Delta_k} - \hat{e}_k , \exp\left(t_v^{\pm\left(n\right)} G_v^\pm \right) \right\rangle \left\langle \hat{e}_{\Delta_l} - \hat{e}_l , \exp\left(t_v^{\pm\left(n\right)} G_v^\pm \right) \right\rangle}
			{\left\langle \hat{e}_{i_v}, G_v^\pm \exp\left(t_v^{\pm\left(n\right)} G_v^\pm \right) \hat{e}_{i_{v + 1}} \right\rangle^2}\\[2ex]
		&   & \:- \mathbb{I}_v^\pm \left[k, \Delta_k \right] \mathbb{I}_v^\pm \left[l\right] \sum_{n=1}^{N_v^\pm} \frac{\left\langle \hat{e}_{\Delta_k} - \hat{e}_k , \exp\left(t_v^{\pm\left(n\right)} G_v^\pm \right) \right\rangle \left\langle \hat{e}_{i_v}, G_v^\pm \frac{\partial \exp\left(t_v^{+\left(n\right)} G_v^{+} \right)}{\partial x_{l \Delta_l}} \hat{e}_{i_{v + 1}} \right\rangle}
			{\left\langle \hat{e}_{i_v}, G_v^\pm \exp\left(t_v^{\pm\left(n\right)} G_v^\pm \right) \hat{e}_{i_{v + 1}} \right\rangle^2}\\[2ex]
		&   & \:- \mathbb{I}_v^\pm \left[k\right] \mathbb{I}_v^\pm \left[l, \Delta_l \right] \sum_{n=1}^{N_v^\pm} \frac{\left\langle \hat{e}_{i_v}, G_v^\pm \frac{\partial \exp\left(t_v^{+\left(n\right)} G_v^{+} \right)}{\partial x_{k \Delta_k}} \hat{e}_{i_{v + 1}} \right\rangle \left\langle \hat{e}_{\Delta_l} - \hat{e}_l , \exp\left(t_v^{\pm\left(n\right)} G_v^\pm \right) \right\rangle}
			{\left\langle \hat{e}_{i_v}, G_v^\pm \exp\left(t_v^{\pm\left(n\right)} G_v^\pm \right) \hat{e}_{i_{v + 1}} \right\rangle^2}\\[2ex]
		&   & \:- \mathbb{I}_v^\pm \left[k\right] \mathbb{I}_v^\pm \left[l\right] \sum_{n=1}^{N_v^\pm} \frac{\left\langle \hat{e}_{i_v}, G_v^\pm \frac{\partial \exp\left(t_v^{+\left(n\right)} G_v^{+} \right)}{\partial x_{k \Delta_k}} \hat{e}_{i_{v + 1}} \right\rangle \left\langle \hat{e}_{i_v}, G_v^\pm \frac{\partial \exp\left(t_v^{+\left(n\right)} G_v^{+} \right)}{\partial x_{l \Delta_l}} \hat{e}_{i_{v + 1}} \right\rangle}
			{\left\langle \hat{e}_{i_v}, G_v^\pm \exp\left(t_v^{\pm\left(n\right)} G_v^\pm \right) \hat{e}_{i_{v + 1}} \right\rangle^2}
\end{IEEEeqnarray*}
A full implementation of both the Hessian of the log-likelihood, and the Newton-Raphson 
method for likelihood maximization is complex undertaking, but fundamentally is not
intractable. For the most part, successful implementation requires patience and diligence on
the part of the developer.
\clearpage
\section{Figures and Illustrations}
\begin{figure}[!ht]
	\centering
	\begin{tikzpicture}[->,thick,node distance=2cm]
		\node[state, very thick, font=\bf] (healthy)                                        {$1$};
		\node[state, draw=gray, text=gray] (healthy-improving) [right of=healthy]           {$2$};
		\node[state, draw=gray, text=gray] (healthy-worsening) [right of=healthy-improving] {$3$};
		\node[state, very thick, font=\bf] (treated)           [right of=healthy-worsening] {$4$};
		\node[state, draw=gray, text=gray] (treated-improving) [right of=treated]           {$5$};
		\node[state, draw=gray, text=gray] (treated-worsening) [right of=treated-improving] {$6$};
		\node[state, very thick, font=\bf] (death)             [right of=treated-worsening] {$7$};
		\path
			(healthy)           edge [loop left, very thick, font=\bf]  node         {$-x_{12}$}                     (healthy)
			(healthy)           edge [bend left, very thick, font=\bf]  node [above] {$x_{12}$}                      (healthy-improving)
			(healthy-improving) edge [bend left, very thick, font=\bf]  node [below] {$x_{21}$}                      (healthy)
			(healthy-improving) edge [loop below, draw=gray, text=gray] node         {$-\left(x_{21}+x_{23}\right)$} (healthy-improving)
			(healthy-improving) edge [bend left, draw=gray, text=gray]  node [above] {$x_{23}$}                      (healthy-worsening)
			(healthy-worsening) edge [bend left, draw=gray, text=gray]  node [below] {$x_{32}$}                      (healthy-improving)
			(healthy-worsening) edge [loop above, draw=gray, text=gray] node         {$-\left(x_{32}+x_{34}\right)$} (healthy-worsening)
			(healthy-worsening) edge [bend left, very thick, font=\bf]  node [above] {$x_{34}$}                      (treated)
			(treated)           edge [bend left, very thick, font=\bf]  node [below] {$x_{43}$}                      (healthy-worsening)
			(treated)           edge [loop below, very thick, font=\bf] node         {$-\left(x_{43}+x_{45}\right)$} (treated)
			(treated)           edge [bend left, very thick, font=\bf]  node [above] {$x_{45}$}                      (treated-improving)
			(treated-improving) edge [bend left, very thick, font=\bf]  node [below] {$x_{54}$}                      (treated)
			(treated-improving) edge [loop above, draw=gray, text=gray] node         {$-\left(x_{54}+x_{56}\right)$} (treated-improving)
			(treated-improving) edge [bend left, draw=gray, text=gray]  node [above] {$x_{56}$}                      (treated-worsening)
			(treated-worsening) edge [bend left, draw=gray, text=gray]  node [below] {$x_{65}$}                      (treated-improving)
			(treated-worsening) edge [loop below, draw=gray, text=gray] node         {$-\left(x_{65}+x_{67}\right)$} (treated-worsening)
			(treated-worsening) edge [very thick, font=\bf]             node [above] {$x_{67}$} (death);
	\end{tikzpicture}
	\caption[$7$ State Aging Process]{A representation of an aging process by a reversible $7$ state birth-death process, with $3$ sentinel states: healthy ($1$), care placement ($4$), and death ($7$). Each pair of intermediate states represents either a state of improving ($2$, $5$) or worsening ($3$, $6$) health.}
	\label{fig:agingprocess}
\end{figure}
\begin{algorithm}[!ht]
	\caption[Single summand of $\frac{\partial \Lambda}{\partial x_{k\left(k \pm 1\right)}}$]{Numerical calculation of a single summand in the gradient of the log-likelihood $\frac{\partial \Lambda}{\partial x_{k\left(k \pm 1\right)}}$ of the first hitting time statistic of an aging process. The inner products do not need full evaluation, rather by definition of the basis $\hat{e}_i$ the inner products select entries from the matrix by index.}
	\label{alg:singlesummand}
	\begin{algorithmic}[1]
		\Function{PLL}{$G$, $t$, $i$, $j$, $k$, $l$}
			\If{$\left|k - l\right| \ne 1$}
				\State \Return 0
			\EndIf
			\State $E \gets \operatorname{EXP}\left(t G\right)$\Comment{Call to matrix exponential}
			\State $C \gets \hat{e}_k \otimes \hat{e}_l - \hat{e}_k \otimes \hat{e}_k$\Comment{Canonical generator}
			\State $D \gets \left(C + \operatorname{PER}\left(t G, C\right)\right)E$\Comment{Call to gradient perturbation}
			\State $x_{+} \gets \left\langle \hat{e}_i , G \hat{e}_{i+1} \right\rangle$\Comment{Right transition rate}
			\State $x_{-} \gets \left\langle \hat{e}_i , G \hat{e}_{i-1} \right\rangle$\Comment{Left transition rate}
			\State $x_0 \gets -\left(x_{+} + x_{-}\right)$\Comment{Central transition rate}
			\If{$k = i$}
				\State $e_{+} \gets \left\langle \hat{e}_{i+1}, E \hat{e}_j \right\rangle$\Comment{Right probability}
				\State $e_0 \gets \left\langle \hat{e}_i, E \hat{e}_j \right\rangle$\Comment{Central probability}
				\State $e_{-} \gets \left\langle \hat{e}_{i-1}, E \hat{e}_j \right\rangle$\Comment{Left probability}
				\State \Return $\frac{e_\pm - e_0 + t \left(x_{+} \left\langle \hat{e}_{i+1}, D \hat{e}_j \right\rangle + x_{-} \left\langle \hat{e}_{i-1}, D \hat{e}_j \right\rangle + x_0 \left\langle \hat{e}_i, D \hat{e}_j \right\rangle\right)}{x_{+} e_{+} + x_{-}e_{-} + x_0 e_0}$
			\EndIf
			\State \Return $t \frac{ x_{+} \left\langle \hat{e}_{i+1}, D \hat{e}_j \right\rangle + x_{-} \left\langle \hat{e}_{i-1}, D \hat{e}_j \right\rangle + x_0 \left\langle \hat{e}_i, D \hat{e}_j \right\rangle}{x_{+} \left\langle \hat{e}_{i+1}, E \hat{e}_j \right\rangle + x_{-} \left\langle \hat{e}_{i-1}, E \hat{e}_j \right\rangle + x_0 \left\langle \hat{e}_i, E \hat{e}_j \right\rangle}$
		\EndFunction
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}[!ht]
	\caption[Complete sum of $\frac{\partial \Lambda}{\partial x_{kl}}$]{Numerical calculation of the gradient of the complete sum log-likelihood $\frac{\partial \Lambda}{\partial x_{kl}}$ of first hitting time statistics, for any  generator $G$. Implemented as a straight forward single instruction, multiple data loop (SIMD). The Kronecker product does not need to be computed as it represents matrix element assignment by index.}
	\label{alg:completesum}
	\begin{algorithmic}[1]
		\Function{CLL}{$G$, $\left\lbrace \left(t, i, j \right)_1 , \dots , \left(t, i, j \right)_n \right\rbrace$, $k$, $l$}
			\State $r \gets 0$\Comment{Loop initialization}
			\ForAll{$\left(t,i,j\right) \in \left\lbrace \left(t , i, j \right)_1 , \dots , \left(t, i, j \right)_n \right\rbrace$}
				\If{$i \le k \le  j - 1$}
					\State $G_v \gets \left(\sum_{n= i}^{j-1} \hat{e}_n \otimes \hat{e}_n \right) G$\Comment{First hitting time generator}
					\State $r \gets r + \operatorname{PLL}\left(G_v, t, i, j, k, l\right)$ \Comment{SIMD computation}
					\State \textbf{continue}
				\EndIf
				\If{$j+1 \le k \le i$}
					\State $G_v \gets \left(\sum_{n= j+1}^i \hat{e}_n \otimes \hat{e}_n \right) G$\Comment{First hitting time generator}
					\State $r \gets r + \operatorname{PLL}\left(G_v, t, i, j, k, l\right)$ \Comment{SIMD computation}
					\State \textbf{continue}
				\EndIf
			\EndFor
			\State \Return $r$
		\EndFunction
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}[!ht]
	\caption[All partial derivatives of $\frac{\partial \Lambda}{\partial x_{kl}}$]{Numerical calculation of the every partial derivative of the  gradient of the complete sum log-likelihood $\frac{\partial \Lambda}{\partial x_{kl}}$ of first hitting time statistics, for any  generator $G$. Implemented as an iteration over the partial derivatives within a SIMD loop over the data. The Kronecker product does not need to be computed as it represents matrix element assignment by index.}
	\label{alg:allpartials}
	\begin{algorithmic}[1]
		\Function{GLL}{$G$, $\left\lbrace \left(t, i, j \right)_1 , \dots , \left(t, i, j \right)_n \right\rbrace$}
			\State $R \gets 0$\Comment{Loop initialization}
			\ForAll{$\left(t,i,j\right) \in \left\lbrace \left(t , i, j \right)_1 , \dots , \left(t, i, j \right)_n \right\rbrace$}
				\State $G_v \gets \left(\sum_{n= i}^{j-1} \hat{e}_n \otimes \hat{e}_n \right) G$\Comment{First hitting time generator}
				\ForAll{$ k \in \left\lbrace i \wedge j+1, \dots , i \vee j-1 \right\rbrace $}
					\ForAll{$l \in \left\lbrace 1, \dots, N \right\rbrace$}
						\State $R \gets R + \operatorname{PLL}\left(G_v, t, i, j, k, l\right) \left(\hat{e}_k \otimes \hat{e}_l\right)$\Comment{SIMD computation}
					\EndFor
				\EndFor
			\EndFor
			\State \Return $R$
		\EndFunction
	\end{algorithmic}
\end{algorithm}