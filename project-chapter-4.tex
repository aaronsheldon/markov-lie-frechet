\chapter{Maximum Likelihood Estimation from First Hitting Times}
\section{Distribution of First Hitting Times}
Contemporary methods for fitting time homogeneous Markov processes on a finite 
state space require directly parameterizing the transition probability matrix $\mathbb{P}\left[X_n = j \left\|X_0 = i \right.\right] = \left\langle \hat{e}_i, P^n \hat{e}_j \right\rangle$,
as they depend on realizing the process through discrete time steps $n$. While this 
formulation has many powerful applications, there are analyses where the parameterization of 
the generator of the time homogeneous Markov process, $\mathbb{P}\left[X_t = j \left\|X_0 = i \right.\right] = \left\langle \hat{e}_i, \exp\left({tG}\right) \hat{e}_j \right\rangle$, 
is of greater meaning, or importance. In particular when the observed process is, at least
in principle continuous, or when the desired parameterization is in units of rates per time
the parameterization of the generator is the more natural choice.

The natural experimental design for continuous time homogeneous Markov process on a finite
state space is the observation of a stopped process, such as the statistic of first hitting 
time of a state, or the first exit time from a state. Surprisingly it is possible to
explicitly formulate the distribution of these two statistics in terms of the generator of
the process.

To start, recall the definition of the projection operator on a finite dimensional vector
space $P_i = \hat{e}_i \otimes \hat{e}_i$; which projects each vector in the space onto a
fixed unit vector $\hat{e}_i$. Projection operators hold a special purpose in analyzing
continuous time homogeneous Markov processes on a finite state spaces. The projection
operator $I-P_k$, when left multiplied to the generator $G = \sum_{ij}x_{ij}C_{ij}$, yields 
a new process $\left(I-P_k\right)G$, where state $k$ is an absorbing state.

The project operators $P_i$ are useful in formulating the distribution of first hitting 
times of a process generated by $G$ in terms of the transition probabilities of a process
generated by $\left(I-P_i\right)G$. We can apply this to restate the first hitting time 
results in the exercises of Rogers and Williams \cite{rogers_diffusions_2000}.
\begin{theorem}
	If $T_j = \inf\left\lbrace t: X_t=j\right\rbrace$ is the first 
	hitting time statistic of the transition to state $j$ of a process generated by $G = \sum_{ij}x_{ij}C_{ij}$
	then
	\begin{IEEEeqnarray*}{rCl}
		\mathbb{P}_G\left[T_j \le t \left\| X_0=i \right.\right]
			& = & 1 - \left\langle \hat{e}_i, \exp\left(t\left(I - P_j\right) G\right) \hat{e}_j \right\rangle
	\end{IEEEeqnarray*}
\end{theorem}
\begin{IEEEproof}
	The proof hinges on formalizing the intuition that once we know a continuous time 
	homogeneous Markov process on a finite state space $X_t$ has first touched the state $j$ 
	then we need no further information, and so we can work with the simpler process where $j$
	is an absorbing state.
	\begin{IEEEeqnarray*}{+rCl+x*}
		\mathbb{P}_G\left[T_j \le t \left\| X_0=i \right.\right]
			& = & 1 - \mathbb{P}_G\left[T_j > t \left\| X_0=i \right.\right]\\
			& = & 1 - \mathbb{P}_G\left[\forall s \le t \enskip X_s \ne j, \enskip \exists u > t \enskip X_u=j \left\| X_0=i \right.\right]\\
			& = & 1 - \mathbb{P}_G\left[\forall s \le t \enskip X_s \ne j \left\| X_0=i \right.\right]\mathbb{P}_G\left[\exists u > t \enskip X_u=j \left\| \forall s \le t \enskip X_s \ne j \right.\right]\\
			& = & 1 - \mathbb{P}_G\left[\forall s \le t \enskip X_s \ne j \left\| X_0=i \right.\right]\mathbb{P}_G\left[X_u = j, u > t \left\| X_t \ne j \right.\right]\\
			& = & 1 - \mathbb{P}_{\left(I - P_j\right)G}\left[\forall s \le t \enskip X_s \ne j \left\| X_0=i \right.\right]\mathbb{P}_{\left(I - P_j\right)G}\left[X_u = j, \enskip u > t \left\| X_t \ne j \right.\right]\\
			& = & 1 - \mathbb{P}_{\left(I - P_j\right)G}\left[\forall s \le t \enskip X_s \ne j, \enskip X_u = j, \enskip u > t  \left\| X_0=i \right.\right]\\
			& = & 1 - \mathbb{P}_{\left(I - P_j\right)G}\left[X_t = j \left\| X_0=i \right.\right]\\
			& = & 1 - \left\langle \hat{e}_i, \exp\left(t\left(I - P_j\right) G\right) \hat{e}_j \right\rangle & \IEEEQEDhere
	\end{IEEEeqnarray*}
\end{IEEEproof}
This result generalizes in a natural manner. To explain we will change notation slightly;
eliding the explicit enumeration of states. Let $\hat{u} \ne \hat{v} \in \left\lbrace \hat{e}_1, \dots , \hat{e}_N \right\rbrace$
be the state representing unit vectors, where $\hat{u}$ is the initial state, and $\hat{v}$ 
the final state of the first hitting time statistic $T_{\hat{v}}$. Further, suppose we have additional
data that the transition could only occur through any of the the allowed paths through 
states $\hat{u}_1, \dots \hat{u}_M \in \left\lbrace \hat{e}_1, \dots , \hat{e}_N \right\rbrace$
where $\hat{u}_k \ne \hat{u}, \hat{v}, \hat{u}_l$ with $k \ne l$. The cumulative
distribution of $T_{\hat{v}}$ is given by
\begin{IEEEeqnarray*}{rCl}
	\mathbb{P}_G\left[T_{\hat{v}} \le t \left\| X_0 = \hat{u} \right.\right]
		& = & 1 - \left\langle \hat{u}, \exp\left(t P G\right) \hat{v} \right\rangle
\end{IEEEeqnarray*}
where the projection operator $P$ is over $\hat{u}$ and the allowed intermediate states $\hat{u}_m$
\begin{IEEEeqnarray*}{rCl}
	P
		& = & \hat{u} \otimes \hat{u}  + \sum_{m = 1}^M \hat{u}_m \otimes \hat{u}_m
\end{IEEEeqnarray*}
This implies that if we can design our experiment to observe as many of the first hitting 
times as possible, and to minimize the number of unknown paths between intermediate states,
we will greatly simplify our statistical estimators.

In light of this, we can reformulate the standard textbook result, for example in Buchholz 
et. al \cite{buchholz_input_2014}, of first hitting time statistics in the context of the
stochastic contraction Lie algebra $\mathfrak{st}^{+}(\hat{\mathbbm{1}})$. As is standard we 
start with an the experiment designed to observe the first exit time $T_j$ 
from $\hat{e_i}$ to any other state $\hat{e}_j$, where $i \ne j$. Assuming the process is 
generated by $G = \sum_{ij}x_{ij}C_{ij}$, and keeping $i \ne j$ fixed, the density of the 
distribution of $T_j$ is
\begin{IEEEeqnarray*}{rCl}
	p_G\left[T_j=t \left\| X_0=i \right.\right]
		& = & \frac{d}{dt} \mathbb{P}_G\left[ T_j\le t \left\| X_0=i \right.\right]\\
		& = & \frac{d}{dt} \mathbb{P}_{P_i G}\left[ T_j\le t \left\| X_0=i \right.\right]\\
		& = & \frac{d}{dt} \left(1 - \left\langle \hat{e}_i, \exp\left(tP_iG\right) \hat{e}_j\right) \right\rangle\\
		& = & \frac{d}{dt} \left(1 - \left\langle \hat{e}_i, \exp\left(t\sum_{l \ne i} x_{il} C_{il}\right) \hat{e}_j\right) \right\rangle\\
		& = & \frac{d}{dt} \left(1 - \left\langle \hat{e}_i, e^{-tx_{ij}} \hat{e}_i\right) \right\rangle\\
		& = & \frac{d}{dt} \left(1 - e^{-tx_{ij}}\right)\\
		& = & x_{ij} e^{-tx_{ij}}
\end{IEEEeqnarray*}
Intuitively if we design our experiment to observer the $N_{ij}$ replicated durations $t^{\left(n\right)} = T_j^{\left(n\right)}$
of every transition from $i$ to $j$, the maximum likelihood estimate of each rate $x_{ij}$ 
is then the simple average
\begin{IEEEeqnarray*}{rCl}
	\tilde{x}_{ij}
		& = & \frac{N_{ij}}{\sum_{n=1}^{N_{ij}} t^{\left(n\right)}}
\end{IEEEeqnarray*}
However for experiments that involve opportunistic sampling, surveys, or population
monitoring it is generally not possible to observe every distinct transition. Typically the
initial state of the transition is known or can be inferred, but only a subset of states are
observed as absorbing or exit states. In this situation the projection operator $P_i$ onto a single
dimensional subspace is replaced with a projection $I - P_A = P_{i_1} + P_{i_2} + \cdots$ 
onto a multidimensional subspace; where $P_A$ is the projection onto the observed absorbing 
states in set $A$. In the next section we will denote that states for which we can observe
absorption, or exit as the sentinel states of the process.
\section{The Likelihood and Its Maximization}
With a method to derive the density in hand we can proceed to formulate the log-likelihood
of the first hitting times. To do so we must carefully formulate the experimental design to 
which the log-likelihood will apply. Rather than attempt to formulate the most general
likelihood model possible, which would be notationally laborious given the infinite
permutations and combinations of models available, we will illustrate the formulation of
the likelihood through a specific application to an aging process.

An aging process is a continuous time homogeneous finite birth death-process, where all the
sequential transitions between states are reversible except for transitions to the final
state, which is an absorbing state representing death. In the context first hitting time
statistics, a finite subset of the states act as sentinel states, where the first hitting
time statistic for the transition between any pair of, possible non-adjacent, sentinel 
states is observed. An example of this process is illustrated in figure \ref{fig:agingprocess},
which displays a seven state aging process, with three sentinel states. The transitions
between states that are not sentinel are not directly observed; but rather acts as a type
of memory register that broadens the centrality of the distribution of first hitting times.

Given an $U$ state aging process, the generator takes on the simple sequential form:
\begin{IEEEeqnarray*}{rCl}
	G 
		& = & \sum_{u=1}^{U-1} x_{u\left(u+1\right)}C_{u\left(u+1\right)} 
			+ \sum_{u=2}^{U-1} x_{u\left(u-1\right)}C_{i\left(u-1\right)} \\
		& = & \sum_{u=1}^{U-1} x_{u\left(u+1\right)}\left(\hat{e}_u \otimes \hat{e}_{u+1} - \hat{e}_u \otimes \hat{e}_u\right)
			+ \sum_{u=2}^{U-1} x_{u\left(u-1\right)}\left(\hat{e}_u \otimes \hat{e}_{u-1} - \hat{e}_u \otimes \hat{e}_u\right)
\end{IEEEeqnarray*}
The $U$ state aging process has $2U-3$ unknown parameters, $x_{u\left(u+1\right)}$ for $1 \le u \le U-1$
and $x_{u\left(u-1\right)}$ for $2 \le u \le U-1$, that require estimation by likelihood
maximization. Of the $U$ states, a subset of $V \le U$ states are sentinel states, $1 \le u_1 < \cdots u_V \le U$,
for which we observe the first hitting time statistics for the transitions to the sentinel 
states.

Using the generalization of the theorem in the previous section the generators $G_v^\pm$ for
the observed first hitting time statistics, $T_{u_{v \pm 1}}$, are given by (noting the careful
application of signs)
\begin{IEEEeqnarray*}{rCl}
	G_v^\pm
		& = & \sum_{u=u_{v}}^{u_{v \pm 1} \mp 1} P_u G\\
		& = & \sum_{u=u_{v}}^{u_{v \pm 1} \mp 1} \left(\hat{e}_u \otimes \hat{e}_u\right) G\\
		& = & \sum_{u=u_{v}}^{u_{v \pm 1} \mp 1} x_{u \left(u-1\right)}C_{u \left(u-1\right)} + x_{u \left(u+1\right)}C_{u \left(u+1\right)}
\end{IEEEeqnarray*}
The distribution density of $T_{u_{v \pm 1}}$ can be concisely stated as
\begin{IEEEeqnarray*}{rCl}
	p_G\left[T_{u_{v \pm 1}} = t \left\| X_0 = u_v \right.\right]
		& = & \left\langle \hat{e}_{u_v}, G_v^\pm \exp\left(t G_v^\pm \right) \hat{e}_{u_{v \pm 1}} \right\rangle
\end{IEEEeqnarray*}
The next step is to formulate the log-likelihood. This requires establishing the observed
data, to do so we will again elide the explicit enumeration of the states so that we can
emphasize the enumeration of the observations. We will start be considering the general
first hitting time likelihood, and then specify the formulation for the aging process.

Consider $N$ observations of first hitting time statistics $T_{\hat{v}}^{\left(n\right)} = t^{\left(n\right)}$
of transitions from state $\hat{u}^{\left(n\right)}$ to state $\hat{v}^{\left(n\right)}$,
the allowed states indicated by the projection $P^{\left(n\right)}$. The log-likelihood is
then the sum over the observations
\begin{IEEEeqnarray*}{rCl}
	\Lambda
		& = & \sum_{n=1}^N \ln p_G\left[ T_{\hat{v}}^{\left(n\right)} = t^{\left(n\right)} \left\| X_0 = \hat{u}^{\left(n\right)} \right.\right]\\
		& = & \sum_{n=1}^N \ln \left\langle \hat{u}^{\left(n\right)}, P^{\left(n\right)} G \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle\\
		& = & \sum_{n=1}^N \ln \left\langle \hat{u}^{\left(n\right)}, G \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle
\end{IEEEeqnarray*}
For an aging process, if $\hat{u}^{\left(n\right)} = \hat{e}_u$, we denote $\hat{u}_{+}^{\left(n\right)} = \hat{e}_{u+1}$,
$\hat{u}_{-}^{\left(n\right)} = \hat{e}_{u-1}$, $x_{+}^{\left(n\right)} = x_{u\left(u+1\right)}$,
$x_{-}^{\left(n\right)} = x_{u\left(u-1\right)}$, and $x^{\left(n\right)} = x_{+}^{\left(n\right)} + x_{-}^{\left(n\right)}$ the log-likelihood simplifies to
\begin{IEEEeqnarray*}{rCl}
	\Lambda
		& = & \sum_{n=1}^N \ln \left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle
\end{IEEEeqnarray*}
Maximization of the general model requires differentiation by each of the parameters $x_{ij}$.
To simplify we elide the index and denote the a general choice of parameter $x$; which could
be any one of the parameters $x_{ij}$, we further elide the indexes of the canonical
generator so that $C_x = C_{ij}$. Partial differentiation of the log-likelihood is then
\begin{IEEEeqnarray*}{rCl}
	\frac{\partial \Lambda}{\partial x}
		& = & \sum_{n=1}^N \frac{\left\langle \hat{u}^{\left(n\right)}, \left(C_x \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) + G \frac{\partial}{\partial x} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right)\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle \hat{u}^{\left(n\right)}, G \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
\end{IEEEeqnarray*}
In the aging process $x$ can be one of $2U-3$ parameters $x_{k \Delta_k}$, with $\Delta _k = k \pm 1$.
Using the same notation as for the log-likelihood the partial derivative is (noting the 
signs carefully)
\begin{IEEEeqnarray*}{rCl}
	\frac{\partial \Lambda}{\partial x}
		& = & \sum_{n=1}^N  \frac{\mathbb{I}\left[x = x^{\left(n\right)}_\pm \right] \left\langle \hat{u}_{\pm}^{\left(n\right)} - \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\\
		&   & \:+ \frac{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\frac{\partial}{\partial x} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
\end{IEEEeqnarray*}
Factoring the summands by both the transition between sentinel states and the observations
makes the algorithm to calculate the terms of the partial differential equation more
transparent \ref{alg:singlesummand}. For a given generator $G$ and differential parameter $x_{k \Delta_k}$
we first determine the canonical generator $C_{k \Delta_k}$ and sub-generator $G_v$.
After that one time computation, the data $t_v^{\pm \left(n\right)}$ is looped through to
produce the full partial derivative of the log-likelihood \ref{alg:completesum}. In the case
of calculating all the partial derivatives of the log-likelihood for large data sets it
is more efficient to loop through the data once, calculating all the partial derivatives at
each data point, and then producing the sum of the derivatives \ref{alg:allpartials}. 

While the gradient alone is sufficient for gradient decent searches for local maximum, 
maximization of the log-likelihood by the Newton-Raphson method requires calculation of the
Hessian of the log-likelihood. By necessity the Hessian of the log-likelihood will be 
complicated. For the second partial derivative let $y$ be any single parameter $x_{ij}$, 
even possibly $x = y$, with $C_y$ elided equivalently; the second partial derivative is then
\begin{IEEEeqnarray*}{rCl}
	\frac{\partial^2 \Lambda}{\partial x \partial y}
		& = & \sum_{n=1}^N \frac{\left\langle \hat{u}^{\left(n\right)}, \left(C_x \frac{\partial}{\partial y} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) + C_y \frac{\partial}{\partial x} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) + G \frac{\partial^2}{\partial x \partial y} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right)\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle \hat{u}^{\left(n\right)}, G \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\\
		&   & \:- \left(\frac{\left\langle \hat{u}^{\left(n\right)}, \left(C_x \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) + G \frac{\partial}{\partial x} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right)\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle \hat{u}^{\left(n\right)}, G \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\right.\\
		&   & \qquad  \left. \cdot \frac{\left\langle \hat{u}^{\left(n\right)}, \left(C_y \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) + G \frac{\partial}{\partial y} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right)\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle \hat{u}^{\left(n\right)}, G \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\right)
\end{IEEEeqnarray*}
In our running example of the aging process we have two simplifications at our disposal.
First, the generator $G$ is linear in $x_{ij}$ and so the second derivatives vanish. Second,
the partial derivative by the transition rates $x_{k \Delta_k }$ and $x_{l \Delta _l }$ will
only be non-trivial when the states fall between the same sentinel states (carefully noting
that the signs in the indicator functions only apply to the fraction on which the indicator
function resides)
\begin{IEEEeqnarray*}{rCl}
	\frac{\partial^2 \Lambda}{\partial x \partial y}
		& = & \sum_{n=1}^N \frac{\mathbb{I}\left[x = x^{\left(n\right)}_\pm \right] \left\langle \hat{u}_{\pm}^{\left(n\right)} - \hat{u}^{\left(n\right)}, \frac{\partial}{\partial y} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\\[2ex]
		&   & \:+ \frac{\mathbb{I}\left[y = x^{\left(n\right)}_\pm \right] \left\langle \hat{u}_{\pm}^{\left(n\right)} - \hat{u}^{\left(n\right)}, \frac{\partial}{\partial x} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\\[2ex]
		&   & \:+ \frac{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\frac{\partial^2}{\partial x \partial y} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\\[2ex]
		&   & \:- \left(\frac{\mathbb{I}\left[x = x^{\left(n\right)}_\pm \right] \left\langle \hat{u}_{\pm}^{\left(n\right)} - \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\right.\\[2ex]
		&   & \:+ \left. \frac{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\frac{\partial}{\partial x} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\right)\\[2ex]
		&   & \cdot \left(\frac{\mathbb{I}\left[y = x^{\left(n\right)}_\pm \right] \left\langle \hat{u}_{\pm}^{\left(n\right)} - \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\right.\\[2ex]
		&   & \:+ \left. \frac{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\frac{\partial}{\partial y} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\right)
\end{IEEEeqnarray*}
A full implementation of both the Hessian of the log-likelihood, and the Newton-Raphson 
method for likelihood maximization is complex undertaking \ref{alg:secondpartial}, but 
fundamentally is not intractable. For the most part, successful implementation requires 
patience and diligence on the part of the developer, along with rigorous unit testing 
against known closed form solutions for the gradient and Hessian of the matrix exponential.
\clearpage
\section{Figures and Illustrations}
\begin{figure}[!ht]
	\centering
	\begin{tikzpicture}[->,thick,node distance=2cm]
		\node[state, very thick, font=\bf] (healthy)                                        {$1$};
		\node[state, draw=gray, text=gray] (healthy-improving) [right of=healthy]           {$2$};
		\node[state, draw=gray, text=gray] (healthy-worsening) [right of=healthy-improving] {$3$};
		\node[state, very thick, font=\bf] (treated)           [right of=healthy-worsening] {$4$};
		\node[state, draw=gray, text=gray] (treated-improving) [right of=treated]           {$5$};
		\node[state, draw=gray, text=gray] (treated-worsening) [right of=treated-improving] {$6$};
		\node[state, very thick, font=\bf] (death)             [right of=treated-worsening] {$7$};
		\path
			(healthy)           edge [loop left, very thick, font=\bf]  node         {$-x_{12}$}                     (healthy)
			(healthy)           edge [bend left, very thick, font=\bf]  node [above] {$x_{12}$}                      (healthy-improving)
			(healthy-improving) edge [bend left, very thick, font=\bf]  node [below] {$x_{21}$}                      (healthy)
			(healthy-improving) edge [loop below, draw=gray, text=gray] node         {$-\left(x_{21}+x_{23}\right)$} (healthy-improving)
			(healthy-improving) edge [bend left, draw=gray, text=gray]  node [above] {$x_{23}$}                      (healthy-worsening)
			(healthy-worsening) edge [bend left, draw=gray, text=gray]  node [below] {$x_{32}$}                      (healthy-improving)
			(healthy-worsening) edge [loop above, draw=gray, text=gray] node         {$-\left(x_{32}+x_{34}\right)$} (healthy-worsening)
			(healthy-worsening) edge [bend left, very thick, font=\bf]  node [above] {$x_{34}$}                      (treated)
			(treated)           edge [bend left, very thick, font=\bf]  node [below] {$x_{43}$}                      (healthy-worsening)
			(treated)           edge [loop below, very thick, font=\bf] node         {$-\left(x_{43}+x_{45}\right)$} (treated)
			(treated)           edge [bend left, very thick, font=\bf]  node [above] {$x_{45}$}                      (treated-improving)
			(treated-improving) edge [bend left, very thick, font=\bf]  node [below] {$x_{54}$}                      (treated)
			(treated-improving) edge [loop above, draw=gray, text=gray] node         {$-\left(x_{54}+x_{56}\right)$} (treated-improving)
			(treated-improving) edge [bend left, draw=gray, text=gray]  node [above] {$x_{56}$}                      (treated-worsening)
			(treated-worsening) edge [bend left, draw=gray, text=gray]  node [below] {$x_{65}$}                      (treated-improving)
			(treated-worsening) edge [loop below, draw=gray, text=gray] node         {$-\left(x_{65}+x_{67}\right)$} (treated-worsening)
			(treated-worsening) edge [very thick, font=\bf]             node [above] {$x_{67}$} (death);
	\end{tikzpicture}
	\caption[$7$ State Aging Process]{A representation of an aging process by a reversible $7$ state birth-death process, with $3$ sentinel states: healthy ($1$), care placement ($4$), and death ($7$). Each pair of intermediate states represents either a state of improving ($2$, $5$) or worsening ($3$, $6$) health.}
	\label{fig:agingprocess}
\end{figure}
\begin{algorithm}[!ht]
	\caption[Single summand of $\frac{\partial \Lambda}{\partial x_{k\left(k \pm 1\right)}}$]{Numerical calculation of a single summand in the gradient of the log-likelihood $\frac{\partial \Lambda}{\partial x_{k\left(k \pm 1\right)}}$ of the first hitting time statistic of an aging process. The inner products do not need full evaluation, rather by definition of the basis $\hat{e}_i$ the inner products select entries from the matrix by index.}
	\label{alg:singlesummand}
	\begin{algorithmic}[1]
		\Function{PLL}{$G$, $t$, $i$, $j$, $k$, $l$}
			\If{$\left|k - l\right| \ne 1$}
				\State \Return 0
			\EndIf
			\State $E \gets \operatorname{EXP}\left(t G\right)$\Comment{Call to matrix exponential}
			\State $C \gets \hat{e}_k \otimes \hat{e}_l - \hat{e}_k \otimes \hat{e}_k$\Comment{Canonical generator}
			\State $D \gets \left(C + \operatorname{PEX}\left(G, C\right)\right)E$\Comment{Call to gradient perturbation}
			\State $x_{+} \gets \left\langle \hat{e}_i , G \hat{e}_{i+1} \right\rangle$\Comment{Right transition rate}
			\State $x_{-} \gets \left\langle \hat{e}_i , G \hat{e}_{i-1} \right\rangle$\Comment{Left transition rate}
			\State $x_0 \gets -\left(x_{+} + x_{-}\right)$\Comment{Central transition rate}
			\If{$k = i$}
				\State $e_{+} \gets \left\langle \hat{e}_{i+1}, E \hat{e}_j \right\rangle$\Comment{Right probability}
				\State $e_0 \gets \left\langle \hat{e}_i, E \hat{e}_j \right\rangle$\Comment{Central probability}
				\State $e_{-} \gets \left\langle \hat{e}_{i-1}, E \hat{e}_j \right\rangle$\Comment{Left probability}
				\State \Return $\frac{e_{l-k} - e_0 + t \left(x_{+} \left\langle \hat{e}_{i+1}, D \hat{e}_j \right\rangle + x_{-} \left\langle \hat{e}_{i-1}, D \hat{e}_j \right\rangle + x_0 \left\langle \hat{e}_i, D \hat{e}_j \right\rangle\right)}{x_{+} e_{+} + x_{-}e_{-} + x_0 e_0}$
			\EndIf
			\State \Return $t \frac{ x_{+} \left\langle \hat{e}_{i+1}, D \hat{e}_j \right\rangle + x_{-} \left\langle \hat{e}_{i-1}, D \hat{e}_j \right\rangle + x_0 \left\langle \hat{e}_i, D \hat{e}_j \right\rangle}{x_{+} \left\langle \hat{e}_{i+1}, E \hat{e}_j \right\rangle + x_{-} \left\langle \hat{e}_{i-1}, E \hat{e}_j \right\rangle + x_0 \left\langle \hat{e}_i, E \hat{e}_j \right\rangle}$
		\EndFunction
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}[!ht]
	\caption[Complete sum of $\frac{\partial \Lambda}{\partial x_{kl}}$]{Numerical calculation of the gradient of the complete sum log-likelihood $\frac{\partial \Lambda}{\partial x_{kl}}$ of $M$ first hitting time statistics, for any  generator $G$. Implemented as a straight forward single instruction, multiple data loop (SIMD). The Kronecker product does not need to be computed as it represents matrix element assignment by index.  The supplied data includes the projection matrix $P$ that defines the states allowed along the path of transition. In the context of an aging process the projection is given by $P = \sum_{n=i \wedge j+1}^{i \vee j-1} \hat{e}_n \otimes \hat{e}_n$.}
	\label{alg:completesum}
	\begin{algorithmic}[1]
		\Function{CLL}{$G$, $\left\lbrace \left(t, i, j, P \right)_1 , \dots , \left(t, i, j, P \right)_M \right\rbrace$, $k$, $l$}
			\State $r \gets 0$\Comment{Loop initialization}
			\ForAll{$\left(t,i,j\right) \in \left\lbrace \left(t , i, j, P \right)_1 , \dots , \left(t, i, j, P \right)_M \right\rbrace$}
				\State $r \gets r + \operatorname{PLL}\left(PG, t, i, j, k, l\right)$ \Comment{SIMD computation}
			\EndFor
			\State \Return $r$
		\EndFunction
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}[!ht]
	\caption[All partial derivatives of $\frac{\partial \Lambda}{\partial x_{kl}}$]{Numerical calculation of the every partial derivative of the  gradient of the complete sum log-likelihood $\frac{\partial \Lambda}{\partial x_{kl}}$ of $M$ first hitting time statistics, for any $N \times N$ generator $G$. Implemented as an iteration over the partial derivatives within a SIMD loop over the data. The Kronecker product does not need to be computed as it represents matrix element assignment by index. The supplied data includes the projection matrix $P$ that defines the states allowed along the path of transition. In the context of an aging process the projection is given by $P = \sum_{n=i \wedge j+1}^{i \vee j-1} \hat{e}_n \otimes \hat{e}_n$.}
	\label{alg:allpartials}
	\begin{algorithmic}[1]
		\Function{GLL}{$G$, $\left\lbrace \left(t, i, j, P \right)_1 , \dots , \left(t, i, j, P \right)_M \right\rbrace$}
			\State $R \gets 0$\Comment{Loop initialization}
			\ForAll{$\left(t,i,j\right) \in \left\lbrace \left(t , i, j, P \right)_1 , \dots , \left(t, i, j, P \right)_M \right\rbrace$}
				\State $G_v \gets PG$\Comment{First hitting time generator}
				\ForAll{$ k \in \left\lbrace i \wedge j+1, \dots , i \vee j-1 \right\rbrace $}
					\ForAll{$l \in \left\lbrace 1, \dots, N \right\rbrace$}
						\State $R \gets R + \operatorname{PLL}\left(G_v, t, i, j, k, l\right) \left(\hat{e}_k \otimes \hat{e}_l\right)$\Comment{SIMD computation}
					\EndFor
				\EndFor
			\EndFor
			\State \Return $R$
		\EndFunction
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}[!ht]
	\caption[Single summand in $\frac{\partial^2 \Lambda}{\partial x_{k \Delta_k} \partial x_{l \Delta_l}}$]{Numerical calculation of a single summand in the Hessian of the log-likelihood $\frac{\partial^2 \Lambda}{\partial x_{k \Delta_k} \partial x_{l \Delta_l}}$ of a first hitting time statistics. The inner products do not need full evaluation, rather by definition of the basis $\hat{e}_i$ the inner products select entries from the matrix by index.}
	\label{alg:secondpartial}
	\begin{algorithmic}[1]
		\Function{PPL}{$G$, $t$, $i$, $j$, $k$, $\Delta_k$, $l$, $\Delta_l$}
			\State $E \gets \operatorname{EXP}\left(tG\right)$\Comment{Call to matrix exponential}
			\State $C_k \gets \hat{e}_k \otimes \hat{e}_{\Delta_k} - \hat{e}_k \otimes \hat{e}_k$\Comment{Canonical generator}
			\State $C_k \gets \hat{e}_l \otimes \hat{e}_{\Delta_l} - \hat{e}_l \otimes \hat{e}_l$\Comment{Canonical generator}
			\State $P_k \gets \left(C_k + \operatorname{PEX}\left(G,C_k\right)\right)$\Comment{Call to gradient perturbation}
			\State $P_l \gets \left(C_l + \operatorname{PEX}\left(G,C_l\right)\right)$\Comment{Call gradient perturbation}
			\State $x_{+} \gets \left\langle \hat{e}_i , G \hat{e}_{i+1} \right\rangle$\Comment{Right transition rate}
			\State $x_{-} \gets \left\langle \hat{e}_i , G \hat{e}_{i-1} \right\rangle$\Comment{Left transition rate}
			\State $x_0 \gets -\left(x_{+} + x_{-}\right)$\Comment{Central transition rate}
			\If{$k=i$}
				\State $e_{+} \gets \left\langle \hat{e}_{i+1}, t P_k E \hat{e}_j \right\rangle$\Comment{Right probability}
				\State $e_0 \gets \left\langle \hat{e}_i, t P_k E \hat{e}_j \right\rangle$\Comment{Central probability}
				\State $e_{-} \gets \left\langle \hat{e}_{i-1}, t P_k E \hat{e}_j \right\rangle$\Comment{Left probability}
				\State $e_k \gets e_{\Delta_k - k} - e_0 $
			\Else
				\State $e_k \gets 0$
			\EndIf
			\If{$l=i$}
				\State $e_{+} \gets \left\langle \hat{e}_{i+1}, t P_l E \hat{e}_j \right\rangle$\Comment{Right probability}
				\State $e_0 \gets \left\langle \hat{e}_i, t P_l E \hat{e}_j \right\rangle$\Comment{Central probability}
				\State $e_{-} \gets \left\langle \hat{e}_{i-1}, t P_l E \hat{e}_j \right\rangle$\Comment{Left probability}
				\State $e_l \gets e_{\Delta_l - l} - e_0$
			\Else
				\State $e_l \gets 0$
			\EndIf
			\State $p_{kl} \gets \left\langle \hat{e}_i, \frac{1}{2}\left(t \operatorname{BEX}\left(G,C_k,C_l\right) +  t^2 \left(P_k P_l + P_l P_k\right)\right) E \hat{e}_j \right\rangle$
			\State $p_k \gets PLL\left(G,t,i,j,k,\Delta_k\right)$\Comment{Call to single summand partial}
			\State $p_l \gets PLL\left(G,t,i,j,l,\Delta_l\right)$\Comment{Call to single summand partial}
			\State \Return $\frac{e_k+e_l+p_{kl}}{x_{+} \left\langle \hat{e}_{i+1}, E \hat{e}_j \right\rangle + x_{-} \left\langle \hat{e}_{i-1}, E \hat{e}_j \right\rangle + x_0 \left\langle \hat{e}_i, E \hat{e}_j \right\rangle}-p_k p_l$
		\EndFunction
	\end{algorithmic}
\end{algorithm}