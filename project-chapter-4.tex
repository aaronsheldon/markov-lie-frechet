\chapter{Maximum Likelihood Estimation from First Hitting Times}
\section{Distribution of First Hitting Times}
Contemporary methods for fitting time homogeneous Markov processes on a finite-state space
require directly parameterizing the transition probability matrix $\mathbb{P}\left[X_n = j \left\|X_0 = i \right.\right] = \left\langle \hat{e}_i, P^n \hat{e}_j \right\rangle$,
as the methods depend on realizing the process through discrete time steps $n$. While this 
formulation has many powerful applications, there are analyses where the parameterization of 
the generator of the time homogeneous Markov process, $\mathbb{P}\left[X_t = j \left\|X_0 = i \right.\right] = \left\langle \hat{e}_i, \exp\left({tG}\right) \hat{e}_j \right\rangle$, 
is of greater meaning, or importance. In particular when the observed process is, at least
in principle, continuous or when the desired estimator is in units of rates per time, the 
parameterization of the generator is the more natural choice.

The natural experimental design for continuous time homogeneous Markov process on a
finite-state space is the observation of a stopped process, such as the statistic of the 
first hitting time to a state, or the first exit time from a state. Surprisingly, it is 
possible to explicitly formulate the distribution of these two statistics in terms of the 
generator of the process.

To start, recall the definition of the projection operator on a finite dimensional vector
space $P_i = \hat{e}_i \otimes \hat{e}_i$ which projects each vector in the space onto a
fixed unit vector $\hat{e}_i$. Projection operators hold a special purpose in analyzing
continuous time homogeneous Markov processes on finite-state spaces. The projection operator
$I-P_k$, when left multiplied to the generator $G = \sum_{ij}x_{ij}C_{ij}$, yields a new 
process $\left(I-P_k\right)G$, where state $k$ is an absorbing state.

The projection operators $P_i$ are useful in formulating the distribution of first hitting 
times of a process generated by $G$ in terms of the transition probabilities of a process
generated by $\left(I-P_i\right)G$. We can apply this to restate the first hitting time 
results in the exercises of Rogers and Williams \cite{rogers_diffusions_2000}.
\begin{theorem}
	If the statistic $T_k = \inf\left\lbrace t: X_t=k\right\rbrace$ is the first hitting time
	of the transition to state $k$ of a process generated by $G = \sum_{ij}x_{ij}C_{ij}$, then
	\begin{IEEEeqnarray*}{rCl}
		\mathbb{P}_G\left[T_k \le t \left\| X_0=l \right.\right]
			& = & 1 - \left\langle \hat{e}_l, \exp\left(t\left(I - P_k\right) G\right) \hat{e}_k \right\rangle.
	\end{IEEEeqnarray*}
\end{theorem}
\begin{IEEEproof}
	The proof hinges on formalizing the intuition that once we know a continuous time 
	homogeneous Markov process on a finite-state space $X_t$ has first touched the state $k$ 
	then we need no further information, and so we can work with the simpler process where $k$
	is an absorbing state. Being mindful to assure that every set is $\mathscr{F}_t$ 
	measurable, we calculate the cumulative distribution:
	\begin{IEEEeqnarray*}{+rCl+x*}
		\mathbb{P}_G\left[T_k \le t \left\| X_0=l \right.\right]
			& = & 1 - \mathbb{P}_G\left[T_k > t \left\| X_0=l \right.\right]\\
			& = & 1 - \mathbb{P}_G\left[\forall s \le t \enskip X_s \ne k, \enskip \exists u > t \enskip X_u=j \left\| X_0=l \right.\right]\\
			& = & 1 - \mathbb{P}_G\left[\forall s \le t \enskip X_s \ne k \left\| X_0=l \right.\right]\mathbb{P}_G\left[\exists u > t \enskip X_u=k \left\| \forall s \le t \enskip X_s \ne k \right.\right]\\
			& = & 1 - \mathbb{P}_G\left[\forall s \le t \enskip X_s \ne k \left\| X_0=l \right.\right]\mathbb{P}_G\left[X_u = k, u > t \left\| X_t \ne k \right.\right]\\
			& = & 1 - \mathbb{P}_{\left(I - P_k\right)G}\left[\forall s \le t \enskip X_s \ne k \left\| X_0=l \right.\right]\mathbb{P}_{\left(I - P_k\right)G}\left[X_u = k, \enskip u > t \left\| X_t \ne k \right.\right]\\
			& = & 1 - \mathbb{P}_{\left(I - P_k\right)G}\left[\forall s \le t \enskip X_s \ne k, \enskip X_u = k, \enskip u > t  \left\| X_0=l \right.\right]\\
			& = & 1 - \mathbb{P}_{\left(I - P_k\right)G}\left[X_t = k \left\| X_0=l \right.\right]\\
			& = & 1 - \left\langle \hat{e}_l, \exp\left(t\left(I - P_k\right) G\right) \hat{e}_k \right\rangle. & \IEEEQEDhere
	\end{IEEEeqnarray*}
\end{IEEEproof}
This result generalizes in a natural manner. To explain the generalization we will change 
notation slightly, eliding the explicit enumeration of states. Let $\hat{u} \ne \hat{v} \in \left\lbrace \hat{e}_1, \dots , \hat{e}_N \right\rbrace$
be a pair of distinct unit vectors representing states of the process, where $\hat{u}$ is 
the initial state, and $\hat{v}$ the final state with first hitting time statistic $T_{\hat{v}}$. 
Further, suppose we have additional data that the transition could only occur through any of
the allowed paths through states $\hat{u}_1, \dots \hat{u}_M \in \left\lbrace \hat{e}_1, \dots , \hat{e}_N \right\rbrace$
where all the unit vectors are pairwise unequal, i.e. $\hat{u}_k \ne \hat{u}, \hat{v}, \hat{u}_l$
for any $l \ne k$. The cumulative distribution of $T_{\hat{v}}$ is given by
\begin{IEEEeqnarray*}{rCl}
	\mathbb{P}_G\left[T_{\hat{v}} \le t \left\| X_0 = \hat{u} \right.\right]
		& = & 1 - \left\langle \hat{u}, \exp\left(t P G\right) \hat{v} \right\rangle.
\end{IEEEeqnarray*}
where the projection operator $P$ is over $\hat{u}$ and the allowed intermediate states
$\hat{u}_m$'s, i.e.
\begin{IEEEeqnarray*}{rCl}
	P
		& = & \hat{u} \otimes \hat{u}  + \sum_{m = 1}^M \hat{u}_m \otimes \hat{u}_m.
\end{IEEEeqnarray*}
This implies that if we can design our experiment to observe as many of the first hitting 
times as possible, and to minimize the number of unknown paths through intermediate states,
then we will greatly simplify our statistical estimators. In light of this, we can 
reformulate the standard textbook result, for example in Buchholz et. al \cite{buchholz_input_2014}, 
of the first hitting time statistic in the context of the stochastic contraction Lie algebra 
$\mathfrak{st}^{+}(\hat{\mathbbm{1}})$. As is standard we start with an experiment designed 
to observe the first hitting time statistic $T_j$ to state $\hat{e}_j$, from any other state 
$\hat{e_i}$ where $i \ne j$. Assuming the process is generated by $G = \sum_{ij}x_{ij}C_{ij}$ 
and keeping $i \ne j$ fixed, the density of $T_j$ is
\begin{IEEEeqnarray*}{rCl}
	p_G\left[T_j=t \left\| X_0=i \right.\right]
		& = & \frac{d}{dt} \mathbb{P}_G\left[ T_j\le t \left\| X_0=i \right.\right]\\
		& = & \frac{d}{dt} \mathbb{P}_{P_i G}\left[ T_j\le t \left\| X_0=i \right.\right]\\
		& = & \frac{d}{dt} \left(1 - \left\langle \hat{e}_i, \exp\left(tP_iG\right) \hat{e}_j\right) \right\rangle\\
		& = & \frac{d}{dt} \left(1 - \left\langle \hat{e}_i, \exp\left(t\sum_{l \ne i} x_{il} C_{il}\right) \hat{e}_j\right) \right\rangle\\
		& = & \frac{d}{dt} \left(1 - \left\langle \hat{e}_i, e^{-tx_{ij}} \hat{e}_i\right) \right\rangle\\
		& = & \frac{d}{dt} \left(1 - e^{-tx_{ij}}\right)\\
		& = & x_{ij} e^{-tx_{ij}}
\end{IEEEeqnarray*}
Intuitively if we design our experiment to observe the $N_{ij}$ replicated durations $t^{\left(n\right)} = T_j^{\left(n\right)}$
of every transition from $i$ to $j$, the maximum likelihood estimate of each rate, $x_{ij}$, 
is then the simple average
\begin{IEEEeqnarray*}{rCl}
	\tilde{x}_{ij}
		& = & \frac{N_{ij}}{\sum_{n=1}^{N_{ij}} t^{\left(n\right)}}
\end{IEEEeqnarray*}
However for experiments that involve an opportunistic sample, a survey, or a monitored 
population it is generally not possible to observe every distinct transition. Typically, the
initial state of the transition is known or can be inferred, but only a subset of states are
observed as absorbing, or exit states. In this situation the projection operator $P_i$ onto 
a single dimensional subspace is replaced with a projection $I - P_A = P_{i_1} + P_{i_2} + \cdots$ 
onto a multidimensional subspace, where $P_A$ is the projection onto the observed absorbing 
states in set $A$. In the next section we will name those states for which we can observe
absorption or exit as the sentinel states of the process.
\section{Likelihood and Its Maximization}
With a method to derive the density in hand we can proceed to formulate the log-likelihood
of first hitting time statistics. To do so we must carefully formulate the experimental 
design to which the log-likelihood will apply. Rather than attempting to formulate the 
possibly most general likelihood model possible, which would be notationally laborious given 
the infinite number of permutations and combinations of models available, we will illustrate 
the formulation of a log-likelihood through a specific application to an aging process.

An aging process is a continuous time homogeneous finite-state birth-death process where all 
the sequential transitions between states are reversible except for transitions to the final 
state, an absorbing state representing death. In the context of first hitting times, a 
finite subset of states act as sentinel states, where the first hitting time statistic for 
the transition between any pair of, possibly non-adjacent, sentinel states is observed. An 
example of this process is illustrated in figure \ref{fig:agingprocess}, which displays a 
seven-state aging process with three sentinel states. The transitions between states that 
are not sentinel are not directly observed but rather act as a type of memory register 
that broadens the centrality of the distribution of first hitting times.

Given an $U$-state aging process, the generator $G$ takes on the simple sequential form
\begin{IEEEeqnarray*}{rCl}
	G 
		& = & \sum_{u=1}^{U-1} x_{u\left(u+1\right)}C_{u\left(u+1\right)} 
			+ \sum_{u=2}^{U-1} x_{u\left(u-1\right)}C_{i\left(u-1\right)} \\
		& = & \sum_{u=1}^{U-1} x_{u\left(u+1\right)}\left(\hat{e}_u \otimes \hat{e}_{u+1} - \hat{e}_u \otimes \hat{e}_u\right)
			+ \sum_{u=2}^{U-1} x_{u\left(u-1\right)}\left(\hat{e}_u \otimes \hat{e}_{u-1} - \hat{e}_u \otimes \hat{e}_u\right)
\end{IEEEeqnarray*}
The $U$-state aging process has $2U-3$ unknown parameters, $x_{u\left(u+1\right)}$ for $1 \le u \le U-1$,
and $x_{u\left(u-1\right)}$ for $2 \le u \le U-1$, that are required to be estimated, for 
example by likelihood maximization. Of the $U$ states, a subset of $V \le U$ states are 
sentinel states denoted by $1 \le u_1 < \cdots u_V \le U$. We observe the first hitting time 
statistics for the transitions between the $V$ sentinel states.

Using the generalization of the theorem in previous section, the generators $G_v^\pm$ for
the observed first hitting time statistics $T_{u_{v \pm 1}}$ are given by 
\footnote{Note the careful application of signs $\pm$ whose order must correspond on both sides of the equation.}
\begin{IEEEeqnarray*}{rCl}
	G_v^\pm
		& = & \sum_{u=u_{v}}^{u_{v \pm 1} \mp 1} P_u G\\
		& = & \sum_{u=u_{v}}^{u_{v \pm 1} \mp 1} \left(\hat{e}_u \otimes \hat{e}_u\right) G\\
		& = & \sum_{u=u_{v}}^{u_{v \pm 1} \mp 1} x_{u \left(u-1\right)}C_{u \left(u-1\right)} + x_{u \left(u+1\right)}C_{u \left(u+1\right)}.
\end{IEEEeqnarray*}
The density function of $T_{u_{v \pm 1}}$ can be concisely stated as
\begin{IEEEeqnarray*}{rCl}
	p_G\left[T_{u_{v \pm 1}} = t \left\| X_0 = u_v \right.\right]
		& = & \left\langle \hat{e}_{u_v}, G_v^\pm \exp\left(t G_v^\pm \right) \hat{e}_{u_{v \pm 1}} \right\rangle\\
		& = & \left\langle x_{u_v\left(u_v+1\right)}\hat{e}_{u_v+1} + x_{u_v\left(u_v-1\right)}\hat{e}_{{u_v}-1}, \exp\left(t G_v^\pm \right) \hat{e}_{u_{v \pm 1}} \right\rangle\\
		&   & \:- \left\langle \left(x_{u_v\left(u_v+1\right)}+x_{u_v\left(u_v-1\right)}\right)\hat{e}_{u_v}, \exp\left(t G_v^\pm \right) \hat{e}_{u_{v \pm 1}} \right\rangle
\end{IEEEeqnarray*}
The next step is to formulate the log-likelihood. This requires recognizing the observed
data. To do so we will again elide the explicit enumeration of the states so that we can
emphasize the enumeration of the observations. We will start with the general first hitting 
time log-likelihood, and then specify the formulation for the aging process.

Consider $N$ observations of first hitting time statistics $T_{\hat{v}}^{\left(n\right)} = t^{\left(n\right)}$
of transitions from state $\hat{u}^{\left(n\right)}$ to state $\hat{v}^{\left(n\right)}$.
Where the transition may be through any one of infinite number of obscured paths through the
allowed states indicated by the projection $P^{\left(n\right)}$. The log-likelihood is then 
the sum over the observations:
\begin{IEEEeqnarray*}{rCl}
	\Lambda_G
		& = & \sum_{n=1}^N \ln p_G\left[ T_{\hat{v}}^{\left(n\right)} = t^{\left(n\right)} \left\| X_0 = \hat{u}^{\left(n\right)} \right.\right]\\
		& = & \sum_{n=1}^N \ln \left\langle \hat{u}^{\left(n\right)}, P^{\left(n\right)} G \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle\\
		& = & \sum_{n=1}^N \ln \left\langle \hat{u}^{\left(n\right)}, G \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle
\end{IEEEeqnarray*}
For an aging process with $\hat{u}^{\left(n\right)} = \hat{e}_u$, we denote $\hat{u}_{+}^{\left(n\right)} = \hat{e}_{u+1}$,
$\hat{u}_{-}^{\left(n\right)} = \hat{e}_{u-1}$, $x_{+}^{\left(n\right)} = x_{u\left(u+1\right)}$,
$x_{-}^{\left(n\right)} = x_{u\left(u-1\right)}$, and $x^{\left(n\right)} = x_{+}^{\left(n\right)} + x_{-}^{\left(n\right)}$
so that then the log-likelihood simplifies to
\begin{IEEEeqnarray*}{rCl}
	\Lambda_G
		& = & \sum_{n=1}^N \ln \left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle
\end{IEEEeqnarray*}
Maximization of the general model requires differentiation with respect to each parameter $x_{ij}$.
For simplicity we elide the index and denote the parameter as $x$ which could be any one of 
the parameters $x_{ij}$. We further elide the indexes of the canonical generator so that 
$C_x = C_{ij}$. The partial derivative of the log-likelihood is then
\begin{IEEEeqnarray*}{rCl}
	\frac{\partial \Lambda_G}{\partial x}
		& = & \sum_{n=1}^N \frac{\left\langle \hat{u}^{\left(n\right)}, \left(C_x \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) + G \frac{\partial}{\partial x} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right)\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle \hat{u}^{\left(n\right)}, G \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
\end{IEEEeqnarray*}
In the aging process $x$ can be any one of the $2U-3$ parameters $x_{k \Delta_k}$ with $\Delta _k = k \pm 1$.
Using the same notation as for the log-likelihood, the partial derivative is
\footnote{Note the careful application of signs $\pm$ whose order must correspond on both sides of the equation.}
\footnote{Note that the indicator function $\mathbb{I}$ is with respect to the equivalence $\equiv$ and not equality $=$. The equivalence relation checks that the variable $x_\pm^{\left(n\right)}$ is the same one as the partial differential variable $x$. It can be informally read as ``is the same parameter as''.}
\begin{IEEEeqnarray*}{rCl}
	\frac{\partial \Lambda_G}{\partial x}
		& = & \sum_{n=1}^N  \frac{\mathbb{I}\left[x \equiv x^{\left(n\right)}_\pm \right] \left\langle \hat{u}_{\pm}^{\left(n\right)} - \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\\
		&   & \:+ \frac{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\frac{\partial}{\partial x} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
\end{IEEEeqnarray*}
Eliding the state enumerating indexes $i,j$ into the data enumerating index $n$ clarifies
the distinction between the provided data and the parameters requiring estimation. Each 
individual summand in the first partial derivative of the log-likelihood requires the data
for each observation $n$:
\begin{enumerate}
	\item the observed time $t^{\left(n\right)}$
	\item the observed initial state $\hat{u}^{\left(n\right)}$
	\item the observed final state $\hat{v}^{\left(n\right)}$
	\item and the allowed intermediate states $P^{\left(n\right)}$ including $\hat{u}^{\left(n\right)}$
\end{enumerate}
A single summand in the partial derivative requires, $G$, the generator of the process as a 
parameter \ref{alg:singlesummand}.

For the aging process, given a single differential parameter $x_{k \Delta_k}$, we determine 
the canonical generator $C_{k \Delta_k}$ and then loop through the data computing the 
sub-generator $G_v^\pm$, which is supplied to the algorithm to compute the partial 
derivative of the matrix exponential. The terms are summed to produce the full partial 
derivative of the log-likelihood with respect to $x_{k \Delta_k}$ \ref{alg:completesum}. 
When calculating all the partial derivatives of the log-likelihood for a large data set, it 
is more efficient to loop through the data once calculate all the partial derivatives at 
each observation $n$, and then produce the sum of the derivatives \ref{alg:allpartials}. 

While the gradient alone is sufficient for a gradient decent search for local maximum, the 
maximization of the log-likelihood by the Newton-Raphson method requires calculation of the
Hessian of the log-likelihood. By necessity the Hessian of the log-likelihood will be 
complicated. For the second partial derivative, let $y$ be any single parameter $x_{ij}$, 
even possibly $x = y$, with $C_y$ elided equivalently. The second partial derivative is then
\begin{IEEEeqnarray*}{rCl}
	\frac{\partial^2 \Lambda_G}{\partial x \partial y}
		& = & \sum_{n=1}^N \frac{\left\langle \hat{u}^{\left(n\right)}, \left(C_x \frac{\partial}{\partial y} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) + C_y \frac{\partial}{\partial x} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right)\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle \hat{u}^{\left(n\right)}, G \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\\
		&   & \:+ \frac{\left\langle \hat{u}^{\left(n\right)}, G \frac{\partial^2}{\partial x \partial y} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle \hat{u}^{\left(n\right)}, G \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\\
		&   & \:- \frac{\left\langle \hat{u}^{\left(n\right)}, \left(C_x \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) + G \frac{\partial}{\partial x} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right)\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle \hat{u}^{\left(n\right)}, G \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\\
		&   & \cdot \frac{\left\langle \hat{u}^{\left(n\right)}, \left(C_y \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) + G \frac{\partial}{\partial y} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right)\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle \hat{u}^{\left(n\right)}, G \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
\end{IEEEeqnarray*}
In our running example of the aging process, we have two simplifications at our disposal.
First, the generator $G$ is linear in $x_{ij}$ and so the second derivatives vanish. Second,
the partial derivative with respect to the transition rate parameters $x_{k \Delta_k }$ and 
$x_{l \Delta _l }$ will be non-trivial only when the states fall between the same sentinel 
states
\footnote{Note the careful application of signs $\pm$ whose order must correspond on both sides of the equation.}
\footnote{Note that the indicator function $\mathbb{I}$ is with respect to the equivalence $\equiv$ and not equality $=$. The equivalence relation checks that the variable $x_\pm^{\left(n\right)}$ is the same one as the partial differential variable $x$. It can be informally read as ``is the same parameter as''.}
\begin{IEEEeqnarray*}{rCl}
	\frac{\partial^2 \Lambda_G}{\partial x \partial y}
		& = & \sum_{n=1}^N \frac{\mathbb{I}\left[x \equiv x^{\left(n\right)}_\pm \right] \left\langle \hat{u}_{\pm}^{\left(n\right)} - \hat{u}^{\left(n\right)}, \frac{\partial}{\partial y} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\\[2ex]
		&   & \:+ \frac{\mathbb{I}\left[y \equiv x^{\left(n\right)}_\pm \right] \left\langle \hat{u}_{\pm}^{\left(n\right)} - \hat{u}^{\left(n\right)}, \frac{\partial}{\partial x} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\\[2ex]
		&   & \:+ \frac{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\frac{\partial^2}{\partial x \partial y} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\\[2ex]
		&   & \:- \left(\frac{\mathbb{I}\left[x \equiv x^{\left(n\right)}_\pm \right] \left\langle \hat{u}_{\pm}^{\left(n\right)} - \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\right.\\[2ex]
		&   & \:+ \left. \frac{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\frac{\partial}{\partial x} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\right)\\[2ex]
		&   & \cdot \left(\frac{\mathbb{I}\left[y \equiv x^{\left(n\right)}_\pm \right] \left\langle \hat{u}_{\pm}^{\left(n\right)} - \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\right.\\[2ex]
		&   & \:+ \left. \frac{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\frac{\partial}{\partial y} \exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}
			{\left\langle x_{+}^{\left(n\right)} \hat{u}_{+}^{\left(n\right)} + x_{-}^{\left(n\right)} \hat{u}_{-}^{\left(n\right)} - x^{\left(n\right)} \hat{u}^{\left(n\right)},\exp\left(t^{\left(n\right)} P^{\left(n\right)} G\right) \hat{v}^{\left(n\right)} \right\rangle}\right)
\end{IEEEeqnarray*}
A full implementation of both the Hessian of the log-likelihood, and the Newton-Raphson 
method for likelihood maximization is complex undertaking \ref{alg:secondpartial}, but 
fundamentally is not intractable. The Newton-Raphson method calculates the $\left(m+1\right)^{th}$ 
estimation $\tilde{G}^{\left(m+1\right)}$ as
\begin{IEEEeqnarray*}{rCl}
	\tilde{G}^{\left(m+1\right)}
		& = & \tilde{G}^{\left(m\right)} - \left[ \vec{\frac{\partial^2 \Lambda_G}{\partial G^2}} \right]_{G=\tilde{G}^{\left(m\right)}}^{-1} \left[ \vec{\frac{\partial \Lambda_G}{\partial G}} \right]_{G=\tilde{G}^{\left(m\right)}}
\end{IEEEeqnarray*}
where $\vec{\frac{\partial \Lambda_G}{\partial G}}$ is the Fr\'{e}chet gradient with respect to the generator $G$ and 
$\vec{\frac{\partial^2 \Lambda_G}{\partial G^2}}$ is the Fr\'{e}chet Hessian.

For the most part, successful implementation requires patience and diligence on the part of 
the developer, along with rigorous unit testing against known closed-form solutions for the 
gradient and Hessian of the matrix exponential.
\clearpage
\section{Figures and Illustrations}
\begin{figure}[!ht]
	\centering
	\begin{tikzpicture}[->,thick,node distance=2cm]
		\node[state, very thick, font=\bf] (healthy)                                        {$1$};
		\node[state, draw=gray, text=gray] (healthy-improving) [right of=healthy]           {$2$};
		\node[state, draw=gray, text=gray] (healthy-worsening) [right of=healthy-improving] {$3$};
		\node[state, very thick, font=\bf] (treated)           [right of=healthy-worsening] {$4$};
		\node[state, draw=gray, text=gray] (treated-improving) [right of=treated]           {$5$};
		\node[state, draw=gray, text=gray] (treated-worsening) [right of=treated-improving] {$6$};
		\node[state, very thick, font=\bf] (death)             [right of=treated-worsening] {$7$};
		\path
			(healthy)           edge [loop left, very thick, font=\bf]  node         {$-x_{12}$}                     (healthy)
			(healthy)           edge [bend left, very thick, font=\bf]  node [above] {$x_{12}$}                      (healthy-improving)
			(healthy-improving) edge [bend left, very thick, font=\bf]  node [below] {$x_{21}$}                      (healthy)
			(healthy-improving) edge [loop below, draw=gray, text=gray] node         {$-\left(x_{21}+x_{23}\right)$} (healthy-improving)
			(healthy-improving) edge [bend left, draw=gray, text=gray]  node [above] {$x_{23}$}                      (healthy-worsening)
			(healthy-worsening) edge [bend left, draw=gray, text=gray]  node [below] {$x_{32}$}                      (healthy-improving)
			(healthy-worsening) edge [loop above, draw=gray, text=gray] node         {$-\left(x_{32}+x_{34}\right)$} (healthy-worsening)
			(healthy-worsening) edge [bend left, very thick, font=\bf]  node [above] {$x_{34}$}                      (treated)
			(treated)           edge [bend left, very thick, font=\bf]  node [below] {$x_{43}$}                      (healthy-worsening)
			(treated)           edge [loop below, very thick, font=\bf] node         {$-\left(x_{43}+x_{45}\right)$} (treated)
			(treated)           edge [bend left, very thick, font=\bf]  node [above] {$x_{45}$}                      (treated-improving)
			(treated-improving) edge [bend left, very thick, font=\bf]  node [below] {$x_{54}$}                      (treated)
			(treated-improving) edge [loop above, draw=gray, text=gray] node         {$-\left(x_{54}+x_{56}\right)$} (treated-improving)
			(treated-improving) edge [bend left, draw=gray, text=gray]  node [above] {$x_{56}$}                      (treated-worsening)
			(treated-worsening) edge [bend left, draw=gray, text=gray]  node [below] {$x_{65}$}                      (treated-improving)
			(treated-worsening) edge [loop below, draw=gray, text=gray] node         {$-\left(x_{65}+x_{67}\right)$} (treated-worsening)
			(treated-worsening) edge [very thick, font=\bf]             node [above] {$x_{67}$} (death);
	\end{tikzpicture}
	\caption[$7$ State Aging Process]{A representation of an aging process by a reversible $7$-state birth-death process, with $3$ sentinel states: healthy ($1$), care placement ($4$), and death ($7$). Each pair of intermediate states represents either a state of improving ($2$, $5$) or worsening ($3$, $6$) health.}
	\label{fig:agingprocess}
\end{figure}
\begin{algorithm}[!ht]
	\caption[Single summand of $\frac{\partial \Lambda}{\partial x_{k\left(k \pm 1\right)}}$]{Numerical calculation of a single summand in the gradient of the log-likelihood $\frac{\partial \Lambda}{\partial x_{k\left(k \pm 1\right)}}$ of the first hitting time statistic of an aging process. The inner products do not need full evaluation, rather by definition of the basis $\hat{e}_i$ the inner products select entries from the matrix by index.}
	\label{alg:singlesummand}
	\begin{algorithmic}[1]
		\Function{PLL}{$G$, $t$, $i$, $j$, $k$, $l$}
			\If{$\left|k - l\right| \ne 1$}
				\State \Return 0
			\EndIf
			\State $E \gets \operatorname{EXP}\left(t G\right)$\Comment{Call to matrix exponential}
			\State $C \gets \hat{e}_k \otimes \hat{e}_l - \hat{e}_k \otimes \hat{e}_k$\Comment{Canonical generator}
			\State $D \gets \left(C + \operatorname{PEX}\left(G, C\right)\right)E$\Comment{Call to gradient perturbation}
			\State $x_{+} \gets \left\langle \hat{e}_i , G \hat{e}_{i+1} \right\rangle$\Comment{Right transition rate}
			\State $x_{-} \gets \left\langle \hat{e}_i , G \hat{e}_{i-1} \right\rangle$\Comment{Left transition rate}
			\State $x_0 \gets -\left(x_{+} + x_{-}\right)$\Comment{Central transition rate}
			\If{$k = i$}
				\State $e_{+} \gets \left\langle \hat{e}_{i+1}, E \hat{e}_j \right\rangle$\Comment{Right probability}
				\State $e_0 \gets \left\langle \hat{e}_i, E \hat{e}_j \right\rangle$\Comment{Central probability}
				\State $e_{-} \gets \left\langle \hat{e}_{i-1}, E \hat{e}_j \right\rangle$\Comment{Left probability}
				\State \Return $\frac{e_{l-k} - e_0 + t \left(x_{+} \left\langle \hat{e}_{i+1}, D \hat{e}_j \right\rangle + x_{-} \left\langle \hat{e}_{i-1}, D \hat{e}_j \right\rangle + x_0 \left\langle \hat{e}_i, D \hat{e}_j \right\rangle\right)}{x_{+} e_{+} + x_{-}e_{-} + x_0 e_0}$
			\EndIf
			\State \Return $t \frac{ x_{+} \left\langle \hat{e}_{i+1}, D \hat{e}_j \right\rangle + x_{-} \left\langle \hat{e}_{i-1}, D \hat{e}_j \right\rangle + x_0 \left\langle \hat{e}_i, D \hat{e}_j \right\rangle}{x_{+} \left\langle \hat{e}_{i+1}, E \hat{e}_j \right\rangle + x_{-} \left\langle \hat{e}_{i-1}, E \hat{e}_j \right\rangle + x_0 \left\langle \hat{e}_i, E \hat{e}_j \right\rangle}$
		\EndFunction
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}[!ht]
	\caption[Complete sum of $\frac{\partial \Lambda}{\partial x_{kl}}$]{Numerical calculation of the gradient of the complete sum log-likelihood $\frac{\partial \Lambda}{\partial x_{kl}}$ of $M$ first hitting time statistics, for any  generator $G$. Implemented as a straight forward single instruction, multiple data loop (SIMD). The Kronecker product does not need to be computed as it represents matrix element assignment by index.  The supplied data includes the projection matrix $P$ that defines the states allowed along the path of transition. In the context of an aging process the projection is given by $P = \sum_{n=i \wedge j+1}^{i \vee j-1} \hat{e}_n \otimes \hat{e}_n$.}
	\label{alg:completesum}
	\begin{algorithmic}[1]
		\Function{CLL}{$G$, $\left\lbrace \left(t, i, j, P \right)_1 , \dots , \left(t, i, j, P \right)_M \right\rbrace$, $k$, $l$}
			\State $r \gets 0$\Comment{Loop initialization}
			\ForAll{$\left(t,i,j\right) \in \left\lbrace \left(t , i, j, P \right)_1 , \dots , \left(t, i, j, P \right)_M \right\rbrace$}
				\State $r \gets r + \operatorname{PLL}\left(PG, t, i, j, k, l\right)$ \Comment{SIMD computation}
			\EndFor
			\State \Return $r$
		\EndFunction
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}[!ht]
	\caption[All partial derivatives of $\frac{\partial \Lambda}{\partial x_{kl}}$]{Numerical calculation of the every partial derivative of the  gradient of the complete sum log-likelihood $\frac{\partial \Lambda}{\partial x_{kl}}$ of $M$ first hitting time statistics, for any $N \times N$ generator $G$. Implemented as an iteration over the partial derivatives within a SIMD loop over the data. The Kronecker product does not need to be computed as it represents matrix element assignment by index. The supplied data includes the projection matrix $P$ that defines the states allowed along the path of transition. In the context of an aging process the projection is given by $P = \sum_{n=i \wedge j+1}^{i \vee j-1} \hat{e}_n \otimes \hat{e}_n$.}
	\label{alg:allpartials}
	\begin{algorithmic}[1]
		\Function{GLL}{$G$, $\left\lbrace \left(t, i, j, P \right)_1 , \dots , \left(t, i, j, P \right)_M \right\rbrace$}
			\State $R \gets 0$\Comment{Loop initialization}
			\ForAll{$\left(t,i,j\right) \in \left\lbrace \left(t , i, j, P \right)_1 , \dots , \left(t, i, j, P \right)_M \right\rbrace$}
				\State $G_v \gets PG$\Comment{First hitting time generator}
				\ForAll{$ k \in \left\lbrace i \wedge j+1, \dots , i \vee j-1 \right\rbrace $}
					\ForAll{$l \in \left\lbrace 1, \dots, N \right\rbrace$}
						\State $R \gets R + \operatorname{PLL}\left(G_v, t, i, j, k, l\right) \left(\hat{e}_k \otimes \hat{e}_l\right)$\Comment{SIMD computation}
					\EndFor
				\EndFor
			\EndFor
			\State \Return $R$
		\EndFunction
	\end{algorithmic}
\end{algorithm}
\begin{algorithm}[!ht]
	\caption[Single summand in $\frac{\partial^2 \Lambda}{\partial x_{k \Delta_k} \partial x_{l \Delta_l}}$]{Numerical calculation of a single summand in the Hessian of the log-likelihood $\frac{\partial^2 \Lambda}{\partial x_{k \Delta_k} \partial x_{l \Delta_l}}$ of a first hitting time statistics. The inner products do not need full evaluation, rather by definition of the basis $\hat{e}_i$ the inner products select entries from the matrix by index.}
	\label{alg:secondpartial}
	\begin{algorithmic}[1]
		\Function{PPL}{$G$, $t$, $i$, $j$, $k$, $\Delta_k$, $l$, $\Delta_l$}
			\State $E \gets \operatorname{EXP}\left(tG\right)$\Comment{Call to matrix exponential}
			\State $C_k \gets \hat{e}_k \otimes \hat{e}_{\Delta_k} - \hat{e}_k \otimes \hat{e}_k$\Comment{Canonical generator}
			\State $C_k \gets \hat{e}_l \otimes \hat{e}_{\Delta_l} - \hat{e}_l \otimes \hat{e}_l$\Comment{Canonical generator}
			\State $P_k \gets \left(C_k + \operatorname{PEX}\left(G,C_k\right)\right)$\Comment{Call to gradient perturbation}
			\State $P_l \gets \left(C_l + \operatorname{PEX}\left(G,C_l\right)\right)$\Comment{Call gradient perturbation}
			\State $x_{+} \gets \left\langle \hat{e}_i , G \hat{e}_{i+1} \right\rangle$\Comment{Right transition rate}
			\State $x_{-} \gets \left\langle \hat{e}_i , G \hat{e}_{i-1} \right\rangle$\Comment{Left transition rate}
			\State $x_0 \gets -\left(x_{+} + x_{-}\right)$\Comment{Central transition rate}
			\If{$k=i$}
				\State $e_{+} \gets \left\langle \hat{e}_{i+1}, t P_k E \hat{e}_j \right\rangle$\Comment{Right probability}
				\State $e_0 \gets \left\langle \hat{e}_i, t P_k E \hat{e}_j \right\rangle$\Comment{Central probability}
				\State $e_{-} \gets \left\langle \hat{e}_{i-1}, t P_k E \hat{e}_j \right\rangle$\Comment{Left probability}
				\State $e_k \gets e_{\Delta_k - k} - e_0 $
			\Else
				\State $e_k \gets 0$
			\EndIf
			\If{$l=i$}
				\State $e_{+} \gets \left\langle \hat{e}_{i+1}, t P_l E \hat{e}_j \right\rangle$\Comment{Right probability}
				\State $e_0 \gets \left\langle \hat{e}_i, t P_l E \hat{e}_j \right\rangle$\Comment{Central probability}
				\State $e_{-} \gets \left\langle \hat{e}_{i-1}, t P_l E \hat{e}_j \right\rangle$\Comment{Left probability}
				\State $e_l \gets e_{\Delta_l - l} - e_0$
			\Else
				\State $e_l \gets 0$
			\EndIf
			\State $p_{kl} \gets \left\langle \hat{e}_i, \frac{1}{2}\left(t \operatorname{BEX}\left(G,C_k,C_l\right) +  t^2 \left(P_k P_l + P_l P_k\right)\right) E \hat{e}_j \right\rangle$
			\State $p_k \gets PLL\left(G,t,i,j,k,\Delta_k\right)$\Comment{Call to single summand partial}
			\State $p_l \gets PLL\left(G,t,i,j,l,\Delta_l\right)$\Comment{Call to single summand partial}
			\State \Return $\frac{e_k+e_l+p_{kl}}{x_{+} \left\langle \hat{e}_{i+1}, E \hat{e}_j \right\rangle + x_{-} \left\langle \hat{e}_{i-1}, E \hat{e}_j \right\rangle + x_0 \left\langle \hat{e}_i, E \hat{e}_j \right\rangle}-p_k p_l$
		\EndFunction
	\end{algorithmic}
\end{algorithm}