\chapter{Maximum Likelihood Estimation from First Hitting Times}
\section{Distribution of First Hitting Times}
Contemporary methods for fitting time homogeneous Markov processes on a finite 
state space require directly parameterizing the transition probability matrix $\mathbb{P}\left[X_n = j \left\|X_0 = i \right.\right] = \hat{e}_i P^n \hat{e}_j$,
as they depend on realizing the process through discrete time steps $n$. While this 
formulation has many powerful applications, there are analyses where the parameterization of 
the generator of the time homogeneous Markov process, $\mathbb{P}\left[X_t = j \left\|X_0 = i \right.\right] = \hat{e}_i \exp\left({tG}\right) \hat{e}_j$, 
is of greater meaning, or importance. In particular when the observed process is, at least,
in principle continuous, or when the desired parameterization is in units of rates per time
parameterization of the generator is the more natural choice.

The natural experimental design for continuous time homogeneous Markov process on a finite
state space is the observation of a stopped process, such as the first hitting time to a
state statistic, or the first exit time from a state statistic. Surprisingly it is possible
to explicitly formulate the distribution of these two statistics in terms of the generator
of the process.

To start, recall the definition of the projection operator on a finite dimensional vector
space $P_i = \hat{e}_i \otimes \hat{e}_i$; which projects each vector in the space onto a
fixed unit vector $\hat{e}_i$. Projection operators hold a special purpose in analyzing
continuous time homogeneous Markov processes on a finite state spaces. The projection
operator $I-P_k$, when left multiplied to the generator $G = \sum_{ij}x_{ij}C_{ij}$, yields 
a new process $\left(I-P_k\right)G$, where state $k$ is an absorbing state.

The project operators $P_i$ are useful in formulating the distribution of first hitting 
times of a process generated by $G$ in terms of the transition probabilities of a process
generated by $\left(I-P_i\right)G$. We can apply this to restate the first hitting time 
results in the exercises of Rogers and Williams \cite{rogers_diffusions_2000}.
\begin{theorem}
	If $T_j = \inf\left\lbrace t: X_t=j\right\rbrace$ is the first 
	hitting time statistic of the transition to $j$ of a process generated by $G = \sum_{ij}x_{ij}C_{ij}$
	then
	\begin{IEEEeqnarray*}{rCl}
		\mathbb{P}_G\left[T_j \le t \left\| X_0=i \right.\right]
			& = & 1 - \hat{e}_i \exp\left(t\left(I - P_j\right) G\right) \hat{e}_j
	\end{IEEEeqnarray*}
\end{theorem}

\begin{IEEEproof}
	The proof hinges on formalizing the intuition that once we know a continuous time 
	homogeneous Markov process on a finite state space $X_t$ has first touched the state $j$ 
	then we need no further information, and so we can work with the simpler process where $j$
	is an absorbing state.
	\begin{IEEEeqnarray*}{+rCl+x*}
		\mathbb{P}_G\left[T_j \le t \left\| X_0=i \right.\right]
			& = & 1 - \mathbb{P}_G\left[T_j > t \left\| X_0=i \right.\right]\\
			& = & 1 - \mathbb{P}_G\left[\forall s \le t \enskip X_s \ne j, \enskip \exists u > t \enskip X_u=j \left\| X_0=i \right.\right]\\
			& = & 1 - \mathbb{P}_G\left[\forall s \le t \enskip X_s \ne j \left\| X_0=i \right.\right]\mathbb{P}_G\left[\exists u > t \enskip X_u=j \left\| \forall s \le t \enskip X_s \ne j \right.\right]\\
			& = & 1 - \mathbb{P}_G\left[\forall s \le t \enskip X_s \ne j \left\| X_0=i \right.\right]\mathbb{P}_G\left[X_u = j, u > t \left\| X_t \ne j \right.\right]\\
			& = & 1 - \mathbb{P}_{\left(I - P_j\right)G}\left[\forall s \le t \enskip X_s \ne j \left\| X_0=i \right.\right]\mathbb{P}_{\left(I - P_j\right)G}\left[X_u = j, \enskip u > t \left\| X_t \ne j \right.\right]\\
			& = & 1 - \mathbb{P}_{\left(I - P_j\right)G}\left[\forall s \le t \enskip X_s \ne j, \enskip X_u = j, \enskip u > t  \left\| X_0=i \right.\right]\\
			& = & 1 - \mathbb{P}_{\left(I - P_j\right)G}\left[X_t = j \left\| X_0=i \right.\right]\\
			& = & 1 - \hat{e}_i \exp\left(t\left(I - P_j\right) G\right) \hat{e}_j & \IEEEQEDhere
	\end{IEEEeqnarray*}
\end{IEEEproof}

This result generalizes in the obvious manner; where if we have a set of first hitting 
states $J$ then the transition probabilities of the process $\left(I - P_J\right) G$ gives
the cumulative distributions of the hitting times; where $P_J = P_{j_1} + P_{j_2} + \cdots$.
This implies that if we can design our experiment to observe as many of the first hitting 
times as possible we will greatly simplify our statistical estimators.

In light of this, we can reformulate the standard textbook result, for example in Buchholz 
et. al \cite{buchholz_input_2014}, of first hitting time statistics in the context of the
stochastic contraction Lie algebra $\mathfrak{st}^{+}(\hat{\mathbbm{1}})$. As is standard we 
start with an the experiment designed to observe the first exit time $T_{i \rightarrow j}$ 
from $\hat{e_i}$ to every other state $\hat{e}_j$, where $i \ne j$. Assuming the process is 
generated by $G = \sum_{ij}x_{ij}C_{ij}$, and keeping $i \ne j$ fixed, the density of the 
conditional distribution of $T_{i \rightarrow j}$ is
\begin{IEEEeqnarray*}{rCl}
	p\left(X_t=j \left\| X_0=i \right.\right)
		& = & \frac{d}{dt} \mathbb{P}\left[X_T=j, T\le t \left\| X_0=i \right.\right]\\
		& = & \frac{d}{dt} \left(1 - \hat{e_i} \exp\left(tP_iG\right) \hat{e}_j\right)\\
		& = & \hat{e_i} P_iG \exp\left(tP_iG\right) \hat{e}_j\\
		& = & \hat{e_i} \left(\sum_{l \ne i} x_{il} C_{il}\right) \exp\left(t\sum_{l \ne i} x_{il} C_{il}\right) \hat{e}_j\\
		& = & \left(\sum_{l \ne i} x_{il}\left(\hat{e}_l  - \hat{e}_i\right) \right) \left(- e^{-tx_{ij}}\right) \hat{e}_i\\
		& = & x_{ij} e^{-tx_{ij}}
\end{IEEEeqnarray*}

Intuitively if we design our experiment to observer the durations $t_n = T_{i \rightarrow j}$
between $N_{ij}$ replicated transitions $i \rightarrow j$, the maximum likelihood estimate of
each rate $x_{ij}$ is then the simple average
\begin{IEEEeqnarray*}{rCl}
	\tilde{x}_{ij}
		& = & \frac{N_{ij}}{\sum_{n=1}^{N_{ij}} t_n}
\end{IEEEeqnarray*}

However for experiments that involve opportunistic sampling, surveys, or population
monitoring it is generally not possible to observe every distinct transition. Typically the
initial state of the transition is known or can be inferred, but only a subset of exit
states are observed. In this situation the projection operator $P_i$ onto a single
dimensional subspace is replaced with a projection $I - P_A = P_{i_1} + P_{i_2} + \cdots$ 
onto a multidimensional subspace; where $P_A$ is the projection onto the observed absorbing 
states in set $A$. Distribution density of the first hitting time statistic $T_A = \inf\left\lbrace t: X_t \in A \right\rbrace$
is then $p\left(T_A \le t\right) = \left(1-P_A\right)\exp\left(t \left(1-P_A\right) G\right)$.

\section{The Likelihood and Its Maximization}
With a density in hand we can proceed to formulate the likelihood of the for the first 
hitting times. To do so we must carefully formulate the experimental design to which the
likelihood will apply.

\section{Newton-Raphson Maximization}
\section{Figures and Illustrations}